{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.impute import SimpleImputer\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow import nn ## de goi cac active function\n",
    "from sklearn import tree\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input,Activation, Dropout, Flatten, Dense\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from collections import namedtuple\n",
    "import matplotlib\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "from IPython.display import clear_output\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import np_utils ## dung de categorical cac label\n",
    "from sklearn.model_selection import train_test_split  ## dung de tach bo test ra\n",
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "import datetime\n",
    "from math import pow\n",
    "import random\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from collections import deque\n",
    "import csv\n",
    "from keras.layers import PReLU\n",
    "import math\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import h5py\n",
    "import simplejson as json\n",
    "from tqdm import tqdm\n",
    "from keras_radam import RAdam\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import clear_output\n",
    "from keras.utils import  to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import math\n",
    "from keras.callbacks import History \n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3  \\\n",
      "0   1.5250  6.267768e-04  7.008009e-05  7.357284e-03  6.423477e-04   \n",
      "1   1.5275  2.194808e-03  8.639115e-05  9.189811e-03  2.403074e-04   \n",
      "2   1.5300  1.671665e-02  2.634570e-04  1.545621e-02  7.445869e-05   \n",
      "3   1.5325  3.662386e-01  4.872822e-03  3.790922e-02  9.149724e-03   \n",
      "4   1.5350  6.132423e-01  8.438783e-03  1.631000e-02  1.562700e-02   \n",
      "5   1.5375  6.612227e-01  8.108768e-03  4.204107e-02  1.048023e-02   \n",
      "6   1.5400  4.991382e-01  4.970275e-03  5.351833e-02  5.230132e-03   \n",
      "7   1.5425  4.043050e-01  3.501639e-03  4.201422e-02  1.452211e-02   \n",
      "8   1.5450  6.379861e-01  6.492284e-03  2.736030e-02  3.639567e-02   \n",
      "9   1.5475  5.319590e-01  8.605414e-03  1.105764e-02  1.193523e-02   \n",
      "10  1.5500  5.689370e-01  1.504422e-02  4.147845e-02  3.621517e-03   \n",
      "11  1.5525  5.712135e-01  2.167229e-02  3.957852e-02  1.048726e-02   \n",
      "12  1.5550  4.536188e-01  2.354945e-02  3.139316e-03  3.126116e-03   \n",
      "13  1.5575  1.564698e-01  1.129499e-02  5.973522e-02  8.397369e-03   \n",
      "14  1.5600  5.181086e-04  3.851388e-05  7.266459e-04  2.377631e-04   \n",
      "15  1.5625  3.196805e-04  1.955239e-05  1.055047e-04  4.376495e-05   \n",
      "16  1.5650  2.018542e-04  1.175204e-05  4.070887e-05  1.368702e-05   \n",
      "17  1.5675  1.369221e-04  7.380850e-06  2.688887e-05  5.952853e-06   \n",
      "18  1.5700  1.010686e-04  4.843824e-06  2.182827e-05  3.265751e-06   \n",
      "19  1.5725  8.079313e-05  3.275742e-06  1.928429e-05  2.096339e-06   \n",
      "20  1.5750  6.984510e-05  2.268738e-06  1.794198e-05  1.480648e-06   \n",
      "21  1.5775  6.565906e-05  1.548661e-06  1.789426e-05  1.133546e-06   \n",
      "22  1.5800  6.865344e-05  1.056877e-06  1.941261e-05  9.361029e-07   \n",
      "23  1.5825  8.418182e-05  7.722900e-07  2.474754e-05  4.591283e-07   \n",
      "24  1.5850  1.404230e-04  1.305274e-06  4.355419e-05  1.226920e-06   \n",
      "25  1.5875  8.325868e-04  1.971355e-05  2.637415e-04  1.874987e-05   \n",
      "26  1.5900  5.833713e-04  5.002408e-05  4.763878e-04  6.293087e-05   \n",
      "27  1.5925  4.797351e-05  7.790284e-06  4.845242e-06  6.921793e-06   \n",
      "28  1.5950  8.670987e-06  3.267251e-06  4.751514e-07  3.573952e-06   \n",
      "29  1.5975  2.320437e-06  2.038487e-06  2.121258e-08  2.671734e-06   \n",
      "30  1.6000  9.018211e-07  1.460907e-06  1.640141e-08  2.827580e-06   \n",
      "31  1.6025  7.123654e-07  1.101834e-06  8.552133e-08  2.566945e-05   \n",
      "32  1.6050  1.304982e-06  7.304065e-07  9.891818e-07  3.667371e-06   \n",
      "33  1.6075  2.708918e-06  1.704552e-06  9.448553e-05  8.512909e-06   \n",
      "34  1.6100  4.893705e-07  1.086110e-06  1.090949e-06  1.125744e-06   \n",
      "35  1.6125  3.855736e-07  8.342046e-07  6.413904e-07  1.035044e-06   \n",
      "36  1.6150  4.342195e-07  7.247505e-07  5.881899e-07  9.609059e-07   \n",
      "37  1.6175  4.789411e-07  6.419506e-07  6.432722e-07  9.072635e-07   \n",
      "38  1.6200  5.099075e-07  5.890021e-07  7.972293e-07  8.664508e-07   \n",
      "39  1.6225  5.293102e-07  5.483521e-07  1.341902e-06  8.384885e-07   \n",
      "40  1.6250  4.711664e-07  5.245234e-07  8.839217e-06  8.902332e-07   \n",
      "\n",
      "           Loss4  \n",
      "0   1.905137e-05  \n",
      "1   3.727799e-05  \n",
      "2   1.899631e-04  \n",
      "3   3.558606e-03  \n",
      "4   6.862756e-03  \n",
      "5   1.181117e-02  \n",
      "6   1.716079e-02  \n",
      "7   2.058878e-02  \n",
      "8   3.306899e-02  \n",
      "9   2.946688e-02  \n",
      "10  4.324636e-02  \n",
      "11  3.234890e-02  \n",
      "12  6.780882e-03  \n",
      "13  5.574741e-04  \n",
      "14  4.383446e-06  \n",
      "15  2.709510e-06  \n",
      "16  1.636726e-06  \n",
      "17  1.102634e-06  \n",
      "18  8.090117e-07  \n",
      "19  6.315467e-07  \n",
      "20  5.108062e-07  \n",
      "21  4.327158e-07  \n",
      "22  3.728905e-07  \n",
      "23  3.496818e-07  \n",
      "24  3.110605e-07  \n",
      "25  2.159018e-07  \n",
      "26  8.670110e-08  \n",
      "27  2.263835e-07  \n",
      "28  2.211941e-07  \n",
      "29  2.411727e-07  \n",
      "30  2.186782e-07  \n",
      "31  2.224140e-07  \n",
      "32  1.814627e-07  \n",
      "33  2.510174e-07  \n",
      "34  1.837723e-07  \n",
      "35  1.804133e-07  \n",
      "36  1.777462e-07  \n",
      "37  1.766439e-07  \n",
      "38  1.762250e-07  \n",
      "39  1.759063e-07  \n",
      "40  1.762138e-07  \n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.294852  0.002311  2.635875e-03\n",
      "1   1.5275  0.309081  0.001490  1.795729e-03\n",
      "2   1.5300  0.334105  0.001282  1.623990e-03\n",
      "3   1.5325  0.369770  0.001444  1.996089e-03\n",
      "4   1.5350  0.417315  0.001999  3.634116e-03\n",
      "5   1.5375  0.478206  0.002728  1.177032e-02\n",
      "6   1.5400  0.545807  0.003619  4.206769e-02\n",
      "7   1.5425  0.645566  0.005418  2.027403e-02\n",
      "8   1.5450  0.753076  0.012537  1.662217e-02\n",
      "9   1.5475  0.858329  0.013565  3.596371e-02\n",
      "10  1.5500  0.947207  0.007951  4.717223e-02\n",
      "11  1.5525  1.035638  0.009383  1.425992e-02\n",
      "12  1.5550  0.981476  0.039715  1.605307e-02\n",
      "13  1.5575  0.938121  0.002418  5.003545e-03\n",
      "14  1.5600  0.790970  0.000272  2.867008e-04\n",
      "15  1.5625  0.632660  0.000056  4.374967e-05\n",
      "16  1.5650  0.492222  0.000016  1.067989e-05\n",
      "17  1.5675  0.377301  0.000006  3.440465e-06\n",
      "18  1.5700  0.286430  0.000003  1.312012e-06\n",
      "19  1.5725  0.215189  0.000002  5.399587e-07\n",
      "20  1.5750  0.159029  0.000002  2.009906e-07\n",
      "21  1.5775  0.114237  0.000002  4.520259e-08\n",
      "22  1.5800  0.078122  0.000002  4.338622e-08\n",
      "23  1.5825  0.050120  0.000003  7.267866e-08\n",
      "24  1.5850  0.026873  0.000003  1.029577e-07\n",
      "25  1.5875  0.010089  0.000004  1.191867e-07\n",
      "26  1.5900  0.001381  0.000005  1.264474e-07\n",
      "27  1.5925  0.004232  0.000007  1.133919e-07\n",
      "28  1.5950  0.024663  0.000011  4.305505e-08\n",
      "29  1.5975  0.070906  0.000023  1.041764e-07\n",
      "30  1.6000  0.152485  0.000088  6.961811e-07\n",
      "31  1.6025  0.276925  0.000783  5.479279e-06\n",
      "32  1.6050  0.439360  0.000505  1.410392e-05\n",
      "33  1.6075  0.606305  0.000225  4.841430e-06\n",
      "34  1.6100  0.749445  0.000144  1.865210e-07\n",
      "35  1.6125  0.855375  0.000017  2.176731e-08\n",
      "36  1.6150  0.921200  0.000010  1.161037e-07\n",
      "37  1.6175  0.956710  0.000007  1.541489e-07\n",
      "38  1.6200  0.972260  0.000005  1.800037e-07\n",
      "39  1.6225  0.975305  0.000004  1.985664e-07\n",
      "40  1.6250  0.970082  0.000004  2.117713e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.001691  5.408815e-04  3.213630e-03\n",
      "1   1.5275  0.003403  2.252752e-04  4.400668e-03\n",
      "2   1.5300  0.012617  7.455876e-06  9.245909e-03\n",
      "3   1.5325  0.154955  4.608904e-03  4.642866e-02\n",
      "4   1.5350  0.664325  3.988920e-02  3.896226e-02\n",
      "5   1.5375  0.842653  3.642013e-02  1.449024e-02\n",
      "6   1.5400  0.614232  7.385617e-03  3.864665e-02\n",
      "7   1.5425  0.617709  7.272574e-03  4.674179e-02\n",
      "8   1.5450  0.870990  2.447510e-02  3.771547e-02\n",
      "9   1.5475  0.959850  2.372102e-02  1.785169e-02\n",
      "10  1.5500  0.662230  8.030389e-03  2.120213e-02\n",
      "11  1.5525  0.921199  3.092431e-02  4.620466e-02\n",
      "12  1.5550  0.828882  4.400454e-02  2.500839e-02\n",
      "13  1.5575  0.455419  1.881693e-02  1.720995e-02\n",
      "14  1.5600  0.001033  1.593887e-03  6.961007e-04\n",
      "15  1.5625  0.000265  2.042644e-04  5.915334e-05\n",
      "16  1.5650  0.000168  5.718142e-05  1.418678e-05\n",
      "17  1.5675  0.000112  2.358230e-05  5.510658e-06\n",
      "18  1.5700  0.000080  1.248622e-05  2.760080e-06\n",
      "19  1.5725  0.000060  7.899860e-06  1.579207e-06\n",
      "20  1.5750  0.000048  5.732213e-06  9.488055e-07\n",
      "21  1.5775  0.000039  4.679222e-06  5.625864e-07\n",
      "22  1.5800  0.000032  4.317070e-06  2.820897e-07\n",
      "23  1.5825  0.000028  4.872743e-06  7.650059e-08\n",
      "24  1.5850  0.000024  6.976767e-06  3.269183e-09\n",
      "25  1.5875  0.000021  2.864441e-05  4.032823e-06\n",
      "26  1.5900  0.000018  2.708856e-05  2.608703e-05\n",
      "27  1.5925  0.000018  1.121585e-06  2.333378e-06\n",
      "28  1.5950  0.000017  4.177431e-07  1.190624e-06\n",
      "29  1.5975  0.000017  4.030539e-07  7.372328e-07\n",
      "30  1.6000  0.000018  5.331796e-07  4.338941e-07\n",
      "31  1.6025  0.000024  1.017740e-06  9.119234e-08\n",
      "32  1.6050  0.000104  2.344605e-05  1.666409e-05\n",
      "33  1.6075  0.000051  9.154693e-07  1.186269e-05\n",
      "34  1.6100  0.000028  1.948099e-06  1.219098e-05\n",
      "35  1.6125  0.000009  3.114765e-07  1.148367e-06\n",
      "36  1.6150  0.000007  2.286389e-07  6.143595e-07\n",
      "37  1.6175  0.000006  2.179007e-07  4.518979e-07\n",
      "38  1.6200  0.000006  2.275329e-07  3.670865e-07\n",
      "39  1.6225  0.000005  2.461583e-07  3.095064e-07\n",
      "40  1.6250  0.000005  3.871730e-07  2.811765e-07\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  3.249131e-03  1.157054e-03\n",
      "1   1.5275  0.118812  2.613183e-03  6.583992e-04\n",
      "2   1.5300  0.183227  2.781244e-03  4.358437e-04\n",
      "3   1.5325  0.311414  3.799868e-03  5.476985e-04\n",
      "4   1.5350  0.536881  6.214607e-03  1.940065e-03\n",
      "5   1.5375  0.785143  1.063555e-02  6.469603e-03\n",
      "6   1.5400  0.886749  1.868861e-02  1.096699e-02\n",
      "7   1.5425  0.882151  3.886312e-02  1.137066e-02\n",
      "8   1.5450  0.888211  4.058553e-02  1.062819e-02\n",
      "9   1.5475  0.898351  1.882241e-02  1.231554e-02\n",
      "10  1.5500  0.884318  1.285530e-02  2.202517e-02\n",
      "11  1.5525  0.891510  1.641046e-02  3.164553e-02\n",
      "12  1.5550  0.750956  2.978085e-02  8.950454e-03\n",
      "13  1.5575  0.311554  4.397421e-03  1.516899e-03\n",
      "14  1.5600  0.093184  2.851123e-04  2.572778e-04\n",
      "15  1.5625  0.033225  3.847958e-05  6.024352e-05\n",
      "16  1.5650  0.014730  9.810420e-06  1.877581e-05\n",
      "17  1.5675  0.007910  4.079919e-06  7.290227e-06\n",
      "18  1.5700  0.005029  2.359308e-06  3.380012e-06\n",
      "19  1.5725  0.003729  1.656281e-06  1.829884e-06\n",
      "20  1.5750  0.003201  1.292808e-06  1.161332e-06\n",
      "21  1.5775  0.003186  1.061193e-06  8.638020e-07\n",
      "22  1.5800  0.003736  8.840757e-07  8.006064e-07\n",
      "23  1.5825  0.004868  6.678824e-07  8.602698e-07\n",
      "24  1.5850  0.009585  4.383928e-07  1.716265e-06\n",
      "25  1.5875  0.045032  1.942413e-07  9.298206e-06\n",
      "26  1.5900  0.190316  1.750018e-05  7.138411e-05\n",
      "27  1.5925  0.009399  2.096493e-06  4.743000e-06\n",
      "28  1.5950  0.002505  1.572777e-06  1.970329e-06\n",
      "29  1.5975  0.001030  1.383101e-06  1.623976e-06\n",
      "30  1.6000  0.000513  1.639650e-06  2.830783e-06\n",
      "31  1.6025  0.000302  4.824041e-06  3.687513e-05\n",
      "32  1.6050  0.000174  1.069142e-06  5.351556e-08\n",
      "33  1.6075  0.000114  6.800334e-06  1.233166e-06\n",
      "34  1.6100  0.000081  3.778238e-07  5.756362e-08\n",
      "35  1.6125  0.000068  3.178216e-07  1.064093e-08\n",
      "36  1.6150  0.000070  3.811052e-07  1.364581e-08\n",
      "37  1.6175  0.000089  4.076630e-07  1.865470e-08\n",
      "38  1.6200  0.000143  4.278335e-07  2.225208e-08\n",
      "39  1.6225  0.000324  4.344700e-07  2.490008e-08\n",
      "40  1.6250  0.002578  4.196965e-07  4.331406e-08\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  9.275143e-06  3.249131e-03\n",
      "1   1.5275  0.118812  1.240541e-05  2.613183e-03\n",
      "2   1.5300  0.183227  2.183270e-05  2.781244e-03\n",
      "3   1.5325  0.311414  4.718525e-05  3.799868e-03\n",
      "4   1.5350  0.536881  9.373105e-05  6.214607e-03\n",
      "5   1.5375  0.785143  1.233090e-04  1.063555e-02\n",
      "6   1.5400  0.886749  1.488600e-04  1.868861e-02\n",
      "7   1.5425  0.882151  1.721407e-04  3.886312e-02\n",
      "8   1.5450  0.888211  1.664701e-04  4.058553e-02\n",
      "9   1.5475  0.898351  1.511181e-04  1.882241e-02\n",
      "10  1.5500  0.884318  1.197090e-04  1.285530e-02\n",
      "11  1.5525  0.891510  5.678842e-05  1.641046e-02\n",
      "12  1.5550  0.750956  2.636283e-05  2.978085e-02\n",
      "13  1.5575  0.311554  8.209738e-06  4.397421e-03\n",
      "14  1.5600  0.093184  5.872857e-07  2.851123e-04\n",
      "15  1.5625  0.033225  1.921785e-07  3.847958e-05\n",
      "16  1.5650  0.014730  1.224903e-07  9.810420e-06\n",
      "17  1.5675  0.007910  9.910043e-08  4.079919e-06\n",
      "18  1.5700  0.005029  8.774090e-08  2.359308e-06\n",
      "19  1.5725  0.003729  8.086510e-08  1.656281e-06\n",
      "20  1.5750  0.003201  7.532593e-08  1.292808e-06\n",
      "21  1.5775  0.003186  7.206798e-08  1.061193e-06\n",
      "22  1.5800  0.003736  6.910069e-08  8.840757e-07\n",
      "23  1.5825  0.004868  6.383778e-08  6.678824e-07\n",
      "24  1.5850  0.009585  6.207522e-08  4.383928e-07\n",
      "25  1.5875  0.045032  6.199881e-08  1.942413e-07\n",
      "26  1.5900  0.190316  1.598424e-07  1.750018e-05\n",
      "27  1.5925  0.009399  3.989894e-08  2.096493e-06\n",
      "28  1.5950  0.002505  4.372294e-08  1.572777e-06\n",
      "29  1.5975  0.001030  4.314594e-08  1.383101e-06\n",
      "30  1.6000  0.000513  4.067785e-08  1.639650e-06\n",
      "31  1.6025  0.000302  7.136202e-08  4.824041e-06\n",
      "32  1.6050  0.000174  3.886280e-08  1.069142e-06\n",
      "33  1.6075  0.000114  1.053652e-07  6.800334e-06\n",
      "34  1.6100  0.000081  6.681929e-09  3.778238e-07\n",
      "35  1.6125  0.000068  2.825183e-08  3.178216e-07\n",
      "36  1.6150  0.000070  2.910617e-08  3.811052e-07\n",
      "37  1.6175  0.000089  2.790654e-08  4.076630e-07\n",
      "38  1.6200  0.000143  2.688429e-08  4.278335e-07\n",
      "39  1.6225  0.000324  2.600963e-08  4.344700e-07\n",
      "40  1.6250  0.002578  2.723457e-08  4.196965e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  8.678195e-03  4.354697e-03  7.979320e-04\n",
      "1   1.5275  1.464066e-02  4.603720e-03  3.862844e-04\n",
      "2   1.5300  3.787371e-02  6.737941e-03  8.956786e-05\n",
      "3   1.5325  1.789523e-01  1.542104e-02  1.004522e-03\n",
      "4   1.5350  8.346425e-01  2.707670e-02  1.730301e-02\n",
      "5   1.5375  7.881981e-01  7.361873e-03  2.512415e-02\n",
      "6   1.5400  9.421682e-01  7.993247e-03  2.622900e-02\n",
      "7   1.5425  7.225043e-01  1.494140e-02  9.523406e-03\n",
      "8   1.5450  6.045495e-01  2.154035e-02  4.897305e-03\n",
      "9   1.5475  8.158972e-01  3.391100e-02  1.155260e-02\n",
      "10  1.5500  9.152217e-01  2.872994e-02  1.409190e-02\n",
      "11  1.5525  1.031628e+00  1.594717e-02  9.329139e-03\n",
      "12  1.5550  7.263572e-01  1.031064e-02  2.668847e-03\n",
      "13  1.5575  4.447288e-01  1.393132e-02  5.157502e-03\n",
      "14  1.5600  6.175406e-03  3.591040e-04  3.403970e-04\n",
      "15  1.5625  3.572644e-04  2.709877e-05  6.092533e-05\n",
      "16  1.5650  7.057531e-05  3.265422e-06  1.778475e-05\n",
      "17  1.5675  3.676279e-05  5.636786e-07  6.868278e-06\n",
      "18  1.5700  2.596438e-05  2.223149e-07  3.247247e-06\n",
      "19  1.5725  1.936498e-05  1.984777e-07  1.809216e-06\n",
      "20  1.5750  1.448678e-05  2.304673e-07  1.172802e-06\n",
      "21  1.5775  1.054124e-05  2.870431e-07  8.914730e-07\n",
      "22  1.5800  7.097104e-06  4.136866e-07  8.517066e-07\n",
      "23  1.5825  2.872110e-06  6.842304e-07  9.936282e-07\n",
      "24  1.5850  6.000600e-07  1.808587e-06  2.678692e-06\n",
      "25  1.5875  5.398022e-05  1.556182e-05  2.662113e-05\n",
      "26  1.5900  3.096854e-04  4.860001e-05  5.544758e-05\n",
      "27  1.5925  5.218712e-05  1.745059e-06  5.215496e-06\n",
      "28  1.5950  3.553839e-05  5.937621e-07  2.377305e-06\n",
      "29  1.5975  3.225042e-05  3.267548e-07  1.717714e-06\n",
      "30  1.6000  3.636656e-05  6.286471e-07  2.129434e-06\n",
      "31  1.6025  5.353359e-05  1.724389e-05  2.574305e-05\n",
      "32  1.6050  1.455642e-04  8.067656e-06  4.426458e-06\n",
      "33  1.6075  7.479076e-04  2.178997e-05  7.372365e-06\n",
      "34  1.6100  4.359472e-05  7.170879e-07  5.648023e-07\n",
      "35  1.6125  6.184811e-06  5.420901e-07  4.704096e-07\n",
      "36  1.6150  1.240907e-06  4.512076e-07  4.072215e-07\n",
      "37  1.6175  2.004300e-07  3.991376e-07  3.627241e-07\n",
      "38  1.6200  3.276010e-08  3.611923e-07  3.290254e-07\n",
      "39  1.6225  4.536845e-08  3.499531e-07  3.040243e-07\n",
      "40  1.6250  5.310729e-08  4.279843e-07  3.075118e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  8.678195e-03  4.354697e-03  7.979320e-04\n",
      "1   1.5275  1.464066e-02  4.603720e-03  3.862844e-04\n",
      "2   1.5300  3.787371e-02  6.737941e-03  8.956786e-05\n",
      "3   1.5325  1.789523e-01  1.542104e-02  1.004522e-03\n",
      "4   1.5350  8.346425e-01  2.707670e-02  1.730301e-02\n",
      "5   1.5375  7.881981e-01  7.361873e-03  2.512415e-02\n",
      "6   1.5400  9.421682e-01  7.993247e-03  2.622900e-02\n",
      "7   1.5425  7.225043e-01  1.494140e-02  9.523406e-03\n",
      "8   1.5450  6.045495e-01  2.154035e-02  4.897305e-03\n",
      "9   1.5475  8.158972e-01  3.391100e-02  1.155260e-02\n",
      "10  1.5500  9.152217e-01  2.872994e-02  1.409190e-02\n",
      "11  1.5525  1.031628e+00  1.594717e-02  9.329139e-03\n",
      "12  1.5550  7.263572e-01  1.031064e-02  2.668847e-03\n",
      "13  1.5575  4.447288e-01  1.393132e-02  5.157502e-03\n",
      "14  1.5600  6.175406e-03  3.591040e-04  3.403970e-04\n",
      "15  1.5625  3.572644e-04  2.709877e-05  6.092533e-05\n",
      "16  1.5650  7.057531e-05  3.265422e-06  1.778475e-05\n",
      "17  1.5675  3.676279e-05  5.636786e-07  6.868278e-06\n",
      "18  1.5700  2.596438e-05  2.223149e-07  3.247247e-06\n",
      "19  1.5725  1.936498e-05  1.984777e-07  1.809216e-06\n",
      "20  1.5750  1.448678e-05  2.304673e-07  1.172802e-06\n",
      "21  1.5775  1.054124e-05  2.870431e-07  8.914730e-07\n",
      "22  1.5800  7.097104e-06  4.136866e-07  8.517066e-07\n",
      "23  1.5825  2.872110e-06  6.842304e-07  9.936282e-07\n",
      "24  1.5850  6.000600e-07  1.808587e-06  2.678692e-06\n",
      "25  1.5875  5.398022e-05  1.556182e-05  2.662113e-05\n",
      "26  1.5900  3.096854e-04  4.860001e-05  5.544758e-05\n",
      "27  1.5925  5.218712e-05  1.745059e-06  5.215496e-06\n",
      "28  1.5950  3.553839e-05  5.937621e-07  2.377305e-06\n",
      "29  1.5975  3.225042e-05  3.267548e-07  1.717714e-06\n",
      "30  1.6000  3.636656e-05  6.286471e-07  2.129434e-06\n",
      "31  1.6025  5.353359e-05  1.724389e-05  2.574305e-05\n",
      "32  1.6050  1.455642e-04  8.067656e-06  4.426458e-06\n",
      "33  1.6075  7.479076e-04  2.178997e-05  7.372365e-06\n",
      "34  1.6100  4.359472e-05  7.170879e-07  5.648023e-07\n",
      "35  1.6125  6.184811e-06  5.420901e-07  4.704096e-07\n",
      "36  1.6150  1.240907e-06  4.512076e-07  4.072215e-07\n",
      "37  1.6175  2.004300e-07  3.991376e-07  3.627241e-07\n",
      "38  1.6200  3.276010e-08  3.611923e-07  3.290254e-07\n",
      "39  1.6225  4.536845e-08  3.499531e-07  3.040243e-07\n",
      "40  1.6250  5.310729e-08  4.279843e-07  3.075118e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  3.249131e-03  1.157054e-03\n",
      "1   1.5275  0.118812  2.613183e-03  6.583992e-04\n",
      "2   1.5300  0.183227  2.781244e-03  4.358437e-04\n",
      "3   1.5325  0.311414  3.799868e-03  5.476985e-04\n",
      "4   1.5350  0.536881  6.214607e-03  1.940065e-03\n",
      "5   1.5375  0.785143  1.063555e-02  6.469603e-03\n",
      "6   1.5400  0.886749  1.868861e-02  1.096699e-02\n",
      "7   1.5425  0.882151  3.886312e-02  1.137066e-02\n",
      "8   1.5450  0.888211  4.058553e-02  1.062819e-02\n",
      "9   1.5475  0.898351  1.882241e-02  1.231554e-02\n",
      "10  1.5500  0.884318  1.285530e-02  2.202517e-02\n",
      "11  1.5525  0.891510  1.641046e-02  3.164553e-02\n",
      "12  1.5550  0.750956  2.978085e-02  8.950454e-03\n",
      "13  1.5575  0.311554  4.397421e-03  1.516899e-03\n",
      "14  1.5600  0.093184  2.851123e-04  2.572778e-04\n",
      "15  1.5625  0.033225  3.847958e-05  6.024352e-05\n",
      "16  1.5650  0.014730  9.810420e-06  1.877581e-05\n",
      "17  1.5675  0.007910  4.079919e-06  7.290227e-06\n",
      "18  1.5700  0.005029  2.359308e-06  3.380012e-06\n",
      "19  1.5725  0.003729  1.656281e-06  1.829884e-06\n",
      "20  1.5750  0.003201  1.292808e-06  1.161332e-06\n",
      "21  1.5775  0.003186  1.061193e-06  8.638020e-07\n",
      "22  1.5800  0.003736  8.840757e-07  8.006064e-07\n",
      "23  1.5825  0.004868  6.678824e-07  8.602698e-07\n",
      "24  1.5850  0.009585  4.383928e-07  1.716265e-06\n",
      "25  1.5875  0.045032  1.942413e-07  9.298206e-06\n",
      "26  1.5900  0.190316  1.750018e-05  7.138411e-05\n",
      "27  1.5925  0.009399  2.096493e-06  4.743000e-06\n",
      "28  1.5950  0.002505  1.572777e-06  1.970329e-06\n",
      "29  1.5975  0.001030  1.383101e-06  1.623976e-06\n",
      "30  1.6000  0.000513  1.639650e-06  2.830783e-06\n",
      "31  1.6025  0.000302  4.824041e-06  3.687513e-05\n",
      "32  1.6050  0.000174  1.069142e-06  5.351556e-08\n",
      "33  1.6075  0.000114  6.800334e-06  1.233166e-06\n",
      "34  1.6100  0.000081  3.778238e-07  5.756362e-08\n",
      "35  1.6125  0.000068  3.178216e-07  1.064093e-08\n",
      "36  1.6150  0.000070  3.811052e-07  1.364581e-08\n",
      "37  1.6175  0.000089  4.076630e-07  1.865470e-08\n",
      "38  1.6200  0.000143  4.278335e-07  2.225208e-08\n",
      "39  1.6225  0.000324  4.344700e-07  2.490008e-08\n",
      "40  1.6250  0.002578  4.196965e-07  4.331406e-08\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  9.275143e-06  3.249131e-03\n",
      "1   1.5275  0.118812  1.240541e-05  2.613183e-03\n",
      "2   1.5300  0.183227  2.183270e-05  2.781244e-03\n",
      "3   1.5325  0.311414  4.718525e-05  3.799868e-03\n",
      "4   1.5350  0.536881  9.373105e-05  6.214607e-03\n",
      "5   1.5375  0.785143  1.233090e-04  1.063555e-02\n",
      "6   1.5400  0.886749  1.488600e-04  1.868861e-02\n",
      "7   1.5425  0.882151  1.721407e-04  3.886312e-02\n",
      "8   1.5450  0.888211  1.664701e-04  4.058553e-02\n",
      "9   1.5475  0.898351  1.511181e-04  1.882241e-02\n",
      "10  1.5500  0.884318  1.197090e-04  1.285530e-02\n",
      "11  1.5525  0.891510  5.678842e-05  1.641046e-02\n",
      "12  1.5550  0.750956  2.636283e-05  2.978085e-02\n",
      "13  1.5575  0.311554  8.209738e-06  4.397421e-03\n",
      "14  1.5600  0.093184  5.872857e-07  2.851123e-04\n",
      "15  1.5625  0.033225  1.921785e-07  3.847958e-05\n",
      "16  1.5650  0.014730  1.224903e-07  9.810420e-06\n",
      "17  1.5675  0.007910  9.910043e-08  4.079919e-06\n",
      "18  1.5700  0.005029  8.774090e-08  2.359308e-06\n",
      "19  1.5725  0.003729  8.086510e-08  1.656281e-06\n",
      "20  1.5750  0.003201  7.532593e-08  1.292808e-06\n",
      "21  1.5775  0.003186  7.206798e-08  1.061193e-06\n",
      "22  1.5800  0.003736  6.910069e-08  8.840757e-07\n",
      "23  1.5825  0.004868  6.383778e-08  6.678824e-07\n",
      "24  1.5850  0.009585  6.207522e-08  4.383928e-07\n",
      "25  1.5875  0.045032  6.199881e-08  1.942413e-07\n",
      "26  1.5900  0.190316  1.598424e-07  1.750018e-05\n",
      "27  1.5925  0.009399  3.989894e-08  2.096493e-06\n",
      "28  1.5950  0.002505  4.372294e-08  1.572777e-06\n",
      "29  1.5975  0.001030  4.314594e-08  1.383101e-06\n",
      "30  1.6000  0.000513  4.067785e-08  1.639650e-06\n",
      "31  1.6025  0.000302  7.136202e-08  4.824041e-06\n",
      "32  1.6050  0.000174  3.886280e-08  1.069142e-06\n",
      "33  1.6075  0.000114  1.053652e-07  6.800334e-06\n",
      "34  1.6100  0.000081  6.681929e-09  3.778238e-07\n",
      "35  1.6125  0.000068  2.825183e-08  3.178216e-07\n",
      "36  1.6150  0.000070  2.910617e-08  3.811052e-07\n",
      "37  1.6175  0.000089  2.790654e-08  4.076630e-07\n",
      "38  1.6200  0.000143  2.688429e-08  4.278335e-07\n",
      "39  1.6225  0.000324  2.600963e-08  4.344700e-07\n",
      "40  1.6250  0.002578  2.723457e-08  4.196965e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda        Output         Loss1         Loss2         Loss3  \\\n",
      "0   1.5250  6.267768e-04  7.008009e-05  7.357284e-03  6.423477e-04   \n",
      "1   1.5275  2.194808e-03  8.639115e-05  9.189811e-03  2.403074e-04   \n",
      "2   1.5300  1.671665e-02  2.634570e-04  1.545621e-02  7.445869e-05   \n",
      "3   1.5325  3.662386e-01  4.872822e-03  3.790922e-02  9.149724e-03   \n",
      "4   1.5350  6.132423e-01  8.438783e-03  1.631000e-02  1.562700e-02   \n",
      "5   1.5375  6.612227e-01  8.108768e-03  4.204107e-02  1.048023e-02   \n",
      "6   1.5400  4.991382e-01  4.970275e-03  5.351833e-02  5.230132e-03   \n",
      "7   1.5425  4.043050e-01  3.501639e-03  4.201422e-02  1.452211e-02   \n",
      "8   1.5450  6.379861e-01  6.492284e-03  2.736030e-02  3.639567e-02   \n",
      "9   1.5475  5.319590e-01  8.605414e-03  1.105764e-02  1.193523e-02   \n",
      "10  1.5500  5.689370e-01  1.504422e-02  4.147845e-02  3.621517e-03   \n",
      "11  1.5525  5.712135e-01  2.167229e-02  3.957852e-02  1.048726e-02   \n",
      "12  1.5550  4.536188e-01  2.354945e-02  3.139316e-03  3.126116e-03   \n",
      "13  1.5575  1.564698e-01  1.129499e-02  5.973522e-02  8.397369e-03   \n",
      "14  1.5600  5.181086e-04  3.851388e-05  7.266459e-04  2.377631e-04   \n",
      "15  1.5625  3.196805e-04  1.955239e-05  1.055047e-04  4.376495e-05   \n",
      "16  1.5650  2.018542e-04  1.175204e-05  4.070887e-05  1.368702e-05   \n",
      "17  1.5675  1.369221e-04  7.380850e-06  2.688887e-05  5.952853e-06   \n",
      "18  1.5700  1.010686e-04  4.843824e-06  2.182827e-05  3.265751e-06   \n",
      "19  1.5725  8.079313e-05  3.275742e-06  1.928429e-05  2.096339e-06   \n",
      "20  1.5750  6.984510e-05  2.268738e-06  1.794198e-05  1.480648e-06   \n",
      "21  1.5775  6.565906e-05  1.548661e-06  1.789426e-05  1.133546e-06   \n",
      "22  1.5800  6.865344e-05  1.056877e-06  1.941261e-05  9.361029e-07   \n",
      "23  1.5825  8.418182e-05  7.722900e-07  2.474754e-05  4.591283e-07   \n",
      "24  1.5850  1.404230e-04  1.305274e-06  4.355419e-05  1.226920e-06   \n",
      "25  1.5875  8.325868e-04  1.971355e-05  2.637415e-04  1.874987e-05   \n",
      "26  1.5900  5.833713e-04  5.002408e-05  4.763878e-04  6.293087e-05   \n",
      "27  1.5925  4.797351e-05  7.790284e-06  4.845242e-06  6.921793e-06   \n",
      "28  1.5950  8.670987e-06  3.267251e-06  4.751514e-07  3.573952e-06   \n",
      "29  1.5975  2.320437e-06  2.038487e-06  2.121258e-08  2.671734e-06   \n",
      "30  1.6000  9.018211e-07  1.460907e-06  1.640141e-08  2.827580e-06   \n",
      "31  1.6025  7.123654e-07  1.101834e-06  8.552133e-08  2.566945e-05   \n",
      "32  1.6050  1.304982e-06  7.304065e-07  9.891818e-07  3.667371e-06   \n",
      "33  1.6075  2.708918e-06  1.704552e-06  9.448553e-05  8.512909e-06   \n",
      "34  1.6100  4.893705e-07  1.086110e-06  1.090949e-06  1.125744e-06   \n",
      "35  1.6125  3.855736e-07  8.342046e-07  6.413904e-07  1.035044e-06   \n",
      "36  1.6150  4.342195e-07  7.247505e-07  5.881899e-07  9.609059e-07   \n",
      "37  1.6175  4.789411e-07  6.419506e-07  6.432722e-07  9.072635e-07   \n",
      "38  1.6200  5.099075e-07  5.890021e-07  7.972293e-07  8.664508e-07   \n",
      "39  1.6225  5.293102e-07  5.483521e-07  1.341902e-06  8.384885e-07   \n",
      "40  1.6250  4.711664e-07  5.245234e-07  8.839217e-06  8.902332e-07   \n",
      "\n",
      "           Loss4  \n",
      "0   1.905137e-05  \n",
      "1   3.727799e-05  \n",
      "2   1.899631e-04  \n",
      "3   3.558606e-03  \n",
      "4   6.862756e-03  \n",
      "5   1.181117e-02  \n",
      "6   1.716079e-02  \n",
      "7   2.058878e-02  \n",
      "8   3.306899e-02  \n",
      "9   2.946688e-02  \n",
      "10  4.324636e-02  \n",
      "11  3.234890e-02  \n",
      "12  6.780882e-03  \n",
      "13  5.574741e-04  \n",
      "14  4.383446e-06  \n",
      "15  2.709510e-06  \n",
      "16  1.636726e-06  \n",
      "17  1.102634e-06  \n",
      "18  8.090117e-07  \n",
      "19  6.315467e-07  \n",
      "20  5.108062e-07  \n",
      "21  4.327158e-07  \n",
      "22  3.728905e-07  \n",
      "23  3.496818e-07  \n",
      "24  3.110605e-07  \n",
      "25  2.159018e-07  \n",
      "26  8.670110e-08  \n",
      "27  2.263835e-07  \n",
      "28  2.211941e-07  \n",
      "29  2.411727e-07  \n",
      "30  2.186782e-07  \n",
      "31  2.224140e-07  \n",
      "32  1.814627e-07  \n",
      "33  2.510174e-07  \n",
      "34  1.837723e-07  \n",
      "35  1.804133e-07  \n",
      "36  1.777462e-07  \n",
      "37  1.766439e-07  \n",
      "38  1.762250e-07  \n",
      "39  1.759063e-07  \n",
      "40  1.762138e-07  \n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.001691  5.408815e-04  3.213630e-03\n",
      "1   1.5275  0.003403  2.252752e-04  4.400668e-03\n",
      "2   1.5300  0.012617  7.455876e-06  9.245909e-03\n",
      "3   1.5325  0.154955  4.608904e-03  4.642866e-02\n",
      "4   1.5350  0.664325  3.988920e-02  3.896226e-02\n",
      "5   1.5375  0.842653  3.642013e-02  1.449024e-02\n",
      "6   1.5400  0.614232  7.385617e-03  3.864665e-02\n",
      "7   1.5425  0.617709  7.272574e-03  4.674179e-02\n",
      "8   1.5450  0.870990  2.447510e-02  3.771547e-02\n",
      "9   1.5475  0.959850  2.372102e-02  1.785169e-02\n",
      "10  1.5500  0.662230  8.030389e-03  2.120213e-02\n",
      "11  1.5525  0.921199  3.092431e-02  4.620466e-02\n",
      "12  1.5550  0.828882  4.400454e-02  2.500839e-02\n",
      "13  1.5575  0.455419  1.881693e-02  1.720995e-02\n",
      "14  1.5600  0.001033  1.593887e-03  6.961007e-04\n",
      "15  1.5625  0.000265  2.042644e-04  5.915334e-05\n",
      "16  1.5650  0.000168  5.718142e-05  1.418678e-05\n",
      "17  1.5675  0.000112  2.358230e-05  5.510658e-06\n",
      "18  1.5700  0.000080  1.248622e-05  2.760080e-06\n",
      "19  1.5725  0.000060  7.899860e-06  1.579207e-06\n",
      "20  1.5750  0.000048  5.732213e-06  9.488055e-07\n",
      "21  1.5775  0.000039  4.679222e-06  5.625864e-07\n",
      "22  1.5800  0.000032  4.317070e-06  2.820897e-07\n",
      "23  1.5825  0.000028  4.872743e-06  7.650059e-08\n",
      "24  1.5850  0.000024  6.976767e-06  3.269183e-09\n",
      "25  1.5875  0.000021  2.864441e-05  4.032823e-06\n",
      "26  1.5900  0.000018  2.708856e-05  2.608703e-05\n",
      "27  1.5925  0.000018  1.121585e-06  2.333378e-06\n",
      "28  1.5950  0.000017  4.177431e-07  1.190624e-06\n",
      "29  1.5975  0.000017  4.030539e-07  7.372328e-07\n",
      "30  1.6000  0.000018  5.331796e-07  4.338941e-07\n",
      "31  1.6025  0.000024  1.017740e-06  9.119234e-08\n",
      "32  1.6050  0.000104  2.344605e-05  1.666409e-05\n",
      "33  1.6075  0.000051  9.154693e-07  1.186269e-05\n",
      "34  1.6100  0.000028  1.948099e-06  1.219098e-05\n",
      "35  1.6125  0.000009  3.114765e-07  1.148367e-06\n",
      "36  1.6150  0.000007  2.286389e-07  6.143595e-07\n",
      "37  1.6175  0.000006  2.179007e-07  4.518979e-07\n",
      "38  1.6200  0.000006  2.275329e-07  3.670865e-07\n",
      "39  1.6225  0.000005  2.461583e-07  3.095064e-07\n",
      "40  1.6250  0.000005  3.871730e-07  2.811765e-07\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.294852  0.002311  2.635875e-03\n",
      "1   1.5275  0.309081  0.001490  1.795729e-03\n",
      "2   1.5300  0.334105  0.001282  1.623990e-03\n",
      "3   1.5325  0.369770  0.001444  1.996089e-03\n",
      "4   1.5350  0.417315  0.001999  3.634116e-03\n",
      "5   1.5375  0.478206  0.002728  1.177032e-02\n",
      "6   1.5400  0.545807  0.003619  4.206769e-02\n",
      "7   1.5425  0.645566  0.005418  2.027403e-02\n",
      "8   1.5450  0.753076  0.012537  1.662217e-02\n",
      "9   1.5475  0.858329  0.013565  3.596371e-02\n",
      "10  1.5500  0.947207  0.007951  4.717223e-02\n",
      "11  1.5525  1.035638  0.009383  1.425992e-02\n",
      "12  1.5550  0.981476  0.039715  1.605307e-02\n",
      "13  1.5575  0.938121  0.002418  5.003545e-03\n",
      "14  1.5600  0.790970  0.000272  2.867008e-04\n",
      "15  1.5625  0.632660  0.000056  4.374967e-05\n",
      "16  1.5650  0.492222  0.000016  1.067989e-05\n",
      "17  1.5675  0.377301  0.000006  3.440465e-06\n",
      "18  1.5700  0.286430  0.000003  1.312012e-06\n",
      "19  1.5725  0.215189  0.000002  5.399587e-07\n",
      "20  1.5750  0.159029  0.000002  2.009906e-07\n",
      "21  1.5775  0.114237  0.000002  4.520259e-08\n",
      "22  1.5800  0.078122  0.000002  4.338622e-08\n",
      "23  1.5825  0.050120  0.000003  7.267866e-08\n",
      "24  1.5850  0.026873  0.000003  1.029577e-07\n",
      "25  1.5875  0.010089  0.000004  1.191867e-07\n",
      "26  1.5900  0.001381  0.000005  1.264474e-07\n",
      "27  1.5925  0.004232  0.000007  1.133919e-07\n",
      "28  1.5950  0.024663  0.000011  4.305505e-08\n",
      "29  1.5975  0.070906  0.000023  1.041764e-07\n",
      "30  1.6000  0.152485  0.000088  6.961811e-07\n",
      "31  1.6025  0.276925  0.000783  5.479279e-06\n",
      "32  1.6050  0.439360  0.000505  1.410392e-05\n",
      "33  1.6075  0.606305  0.000225  4.841430e-06\n",
      "34  1.6100  0.749445  0.000144  1.865210e-07\n",
      "35  1.6125  0.855375  0.000017  2.176731e-08\n",
      "36  1.6150  0.921200  0.000010  1.161037e-07\n",
      "37  1.6175  0.956710  0.000007  1.541489e-07\n",
      "38  1.6200  0.972260  0.000005  1.800037e-07\n",
      "39  1.6225  0.975305  0.000004  1.985664e-07\n",
      "40  1.6250  0.970082  0.000004  2.117713e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.05423742137404733"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_map():\n",
    "    maze=[]\n",
    "    for i in range(30):\n",
    "        if (i==0 or i==5 or i==6 or i==11 or i==12 or i==17 or i==18 or i == 23 or i== 24 or i==29 or i == 30):\n",
    "              maze.append([0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0])\n",
    "        elif (i==2 or i==8 or i==14 or i == 20 or i==26 ) :\n",
    "              maze.append([1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1])\n",
    "        else : \n",
    "              maze.append([1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1])\n",
    "    maze = np.asarray(maze)\n",
    "    return maze\n",
    "# for i in range(35):\n",
    "#     if (i==0 or i==4 or i==5 or i==9 or i==10 or i==14 or i==15 or i==19 or i==20 or i==24 or i==25 or i==29 or i==30 or i==34 or i==35):\n",
    "#         maze.append([0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0])\n",
    "#     elif (i==2 or i==7 or i==12 or i==17 or i==22 or i==27 or i==32) :\n",
    "#       maze.append([1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1])\n",
    "#     else : \n",
    "#         maze.append([1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1])\n",
    "maze = get_map()\n",
    "maze = maze.astype(float)\n",
    "maze.shape\n",
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "target_mark = 0.6\n",
    "LEFT1 = 0\n",
    "LEFT2 = 1\n",
    "LEFT3 = 2\n",
    "LEFT4 = 3\n",
    "UP1 = 4\n",
    "UP2 = 5\n",
    "UP3 = 6\n",
    "UP4 = 7\n",
    "RIGHT1 = 8\n",
    "RIGHT2 = 9\n",
    "RIGHT3 = 10\n",
    "RIGHT4 = 11\n",
    "DOWN1 = 12\n",
    "DOWN2 = 13\n",
    "DOWN3 = 14\n",
    "DOWN4 = 15\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT1: 'left1',\n",
    "    LEFT2: 'left2',\n",
    "    LEFT3: 'left3',\n",
    "    LEFT4: 'left4',\n",
    "    UP1:'up1',\n",
    "    UP2:'up2',\n",
    "    UP3:'up3',\n",
    "    UP4:'up4',\n",
    "    RIGHT1: 'right1',\n",
    "    RIGHT2: 'right2',\n",
    "    RIGHT3: 'right3',\n",
    "    RIGHT4: 'right4',\n",
    "    DOWN1: 'down1',\n",
    "    DOWN2: 'down2',\n",
    "    DOWN3: 'down3',\n",
    "    DOWN4: 'down4'\n",
    "}\n",
    "state_centroid_dict = {}\n",
    "num_actions = len(actions_dict)\n",
    "MODEL_NAME = \"model\"\n",
    "# Exploration factor\n",
    "AGGREGATE_STATS_EVERY = 5\n",
    "os.chdir(\"E:/RLAChips - Random/data\")\n",
    "source_data = glob.glob(\"*.csv\")\n",
    "os.getcwd()\n",
    "import sys\n",
    "df = {}\n",
    "for file in source_data :\n",
    "  data = pd.read_excel(file,header=0)\n",
    "  print(data)\n",
    "  row = data.loc[data['Lamda']==1.5500].values[0][1:]\n",
    "  file_name = file.split('.')[0]\n",
    "  output_loss = row[0]\n",
    "  df[file_name] = np.log(output_loss)\n",
    "df[\"K1_To_O1\"]=df[\"O4_To_K4\"]=df[\"J4_To_I4\"]=df[\"I1_To_J1\"]\n",
    "df[\"K1_To_O2\"]=df[\"O4_To_K3\"]=df[\"J4_To_I3\"]=df[\"I1_To_J2\"]\n",
    "df[\"K1_To_O3\"]=df[\"O4_To_K2\"]=df[\"J4_To_I2\"]=df[\"I1_To_J3\"]\n",
    "df[\"K1_To_O4\"]=df[\"O4_To_K1\"]=df[\"J4_To_I1\"]=df[\"I1_To_J4\"]\n",
    "df[\"K1_To_J4\"]=df[\"O4_To_I1\"]=df[\"J4_To_O4\"]=df[\"I1_To_K1\"]\n",
    "df[\"K1_To_J3\"]=df[\"O4_To_I2\"]=df[\"J4_To_O3\"]=df[\"I1_To_K2\"]\n",
    "df[\"K1_To_J2\"]=df[\"O4_To_I3\"]=df[\"J4_To_O2\"]=df[\"I1_To_K3\"]\n",
    "df[\"K1_To_J1\"]=df[\"O4_To_I4\"]=df[\"J4_To_O1\"]=df[\"I1_To_K4\"]\n",
    "df[\"K1_To_I4\"]=df[\"O4_To_J1\"]=df[\"J4_To_K4\"]=df[\"I1_To_O1\"]\n",
    "df[\"K1_To_I3\"]=df[\"O4_To_J2\"]=df[\"J4_To_K3\"]=df[\"I1_To_O2\"]\n",
    "df[\"K1_To_I2\"]=df[\"O4_To_J3\"]=df[\"J4_To_K2\"]=df[\"I1_To_O3\"]\n",
    "df[\"K1_To_I1\"]=df[\"O4_To_J4\"]=df[\"J4_To_K1\"]=df[\"I1_To_O4\"]\n",
    "###########################################################\n",
    "df[\"K2_To_O1\"]=df[\"O3_To_K4\"]=df[\"J3_To_I4\"]=df[\"I2_To_J1\"]\n",
    "df[\"K2_To_O2\"]=df[\"O3_To_K3\"]=df[\"J3_To_I3\"]=df[\"I2_To_J2\"]\n",
    "df[\"K2_To_O3\"]=df[\"O3_To_K2\"]=df[\"J3_To_I2\"]=df[\"I2_To_J3\"]\n",
    "df[\"K2_To_O4\"]=df[\"O3_To_K1\"]=df[\"J3_To_I1\"]=df[\"I2_To_J4\"]\n",
    "df[\"K2_To_J4\"]=df[\"O3_To_I1\"]=df[\"J3_To_O4\"]=df[\"I2_To_K1\"]\n",
    "df[\"K2_To_J3\"]=df[\"O3_To_I2\"]=df[\"J3_To_O3\"]=df[\"I2_To_K2\"]\n",
    "df[\"K2_To_J2\"]=df[\"O3_To_I3\"]=df[\"J3_To_O2\"]=df[\"I2_To_K3\"]\n",
    "df[\"K2_To_J1\"]=df[\"O3_To_I4\"]=df[\"J3_To_O1\"]=df[\"I2_To_K4\"]\n",
    "df[\"K2_To_I4\"]=df[\"O3_To_J1\"]=df[\"J3_To_K4\"]=df[\"I2_To_O1\"]\n",
    "df[\"K2_To_I3\"]=df[\"O3_To_J2\"]=df[\"J3_To_K3\"]=df[\"I2_To_O2\"]\n",
    "df[\"K2_To_I2\"]=df[\"O3_To_J3\"]=df[\"J3_To_K2\"]=df[\"I2_To_O3\"]\n",
    "df[\"K2_To_I1\"]=df[\"O3_To_J4\"]=df[\"J3_To_K1\"]=df[\"I2_To_O4\"]\n",
    "###########################################################\n",
    "df[\"K3_To_O1\"]=df[\"O2_To_K4\"]=df[\"J2_To_I4\"]=df[\"I3_To_J1\"]\n",
    "df[\"K3_To_O2\"]=df[\"O2_To_K3\"]=df[\"J2_To_I3\"]=df[\"I3_To_J2\"]\n",
    "df[\"K3_To_O3\"]=df[\"O2_To_K2\"]=df[\"J2_To_I2\"]=df[\"I3_To_J3\"]\n",
    "df[\"K3_To_O4\"]=df[\"O2_To_K1\"]=df[\"J2_To_I1\"]=df[\"I3_To_J4\"]\n",
    "df[\"K3_To_J1\"]=df[\"O2_To_I1\"]=df[\"J2_To_O4\"]=df[\"I3_To_K1\"]\n",
    "df[\"K3_To_J2\"]=df[\"O2_To_I2\"]=df[\"J2_To_O3\"]=df[\"I3_To_K2\"]\n",
    "df[\"K3_To_J3\"]=df[\"O2_To_I3\"]=df[\"J2_To_O2\"]=df[\"I3_To_K3\"]\n",
    "df[\"K3_To_J4\"]=df[\"O2_To_I4\"]=df[\"J2_To_O1\"]=df[\"I3_To_K4\"]\n",
    "df[\"K3_To_I1\"]=df[\"O2_To_J1\"]=df[\"J2_To_K4\"]=df[\"I3_To_O1\"]\n",
    "df[\"K3_To_I2\"]=df[\"O2_To_J2\"]=df[\"J2_To_K3\"]=df[\"I3_To_O2\"]\n",
    "df[\"K3_To_I3\"]=df[\"O2_To_J3\"]=df[\"J2_To_K2\"]=df[\"I3_To_O3\"]\n",
    "df[\"K3_To_I4\"]=df[\"O2_To_J4\"]=df[\"J2_To_K1\"]=df[\"I3_To_O4\"]\n",
    "###########################################################\n",
    "df[\"K4_To_O1\"]=df[\"O1_To_K4\"]=df[\"J1_To_I4\"]=df[\"I4_To_J1\"]\n",
    "df[\"K4_To_O2\"]=df[\"O1_To_K3\"]=df[\"J1_To_I3\"]=df[\"I4_To_J2\"]\n",
    "df[\"K4_To_O3\"]=df[\"O1_To_K2\"]=df[\"J1_To_I2\"]=df[\"I4_To_J3\"]\n",
    "df[\"K4_To_O4\"]=df[\"O1_To_K1\"]=df[\"J1_To_I1\"]=df[\"I4_To_J4\"]\n",
    "df[\"K4_To_J1\"]=df[\"O1_To_I1\"]=df[\"J1_To_O4\"]=df[\"I4_To_K1\"]\n",
    "df[\"K4_To_J2\"]=df[\"O1_To_I2\"]=df[\"J1_To_O3\"]=df[\"I4_To_K2\"]\n",
    "df[\"K4_To_J3\"]=df[\"O1_To_I3\"]=df[\"J1_To_O2\"]=df[\"I4_To_K3\"]\n",
    "df[\"K4_To_J4\"]=df[\"O1_To_I4\"]=df[\"J1_To_O1\"]=df[\"I4_To_K4\"]\n",
    "df[\"K4_To_I1\"]=df[\"O1_To_J1\"]=df[\"J1_To_K4\"]=df[\"I4_To_O1\"]\n",
    "df[\"K4_To_I2\"]=df[\"O1_To_J2\"]=df[\"J1_To_K3\"]=df[\"I4_To_O2\"]\n",
    "df[\"K4_To_I3\"]=df[\"O1_To_J3\"]=df[\"J1_To_K2\"]=df[\"I4_To_O3\"]\n",
    "df[\"K4_To_I4\"]=df[\"O1_To_J4\"]=df[\"J1_To_K1\"]=df[\"I4_To_O4\"]\n",
    "action_node_dict = {}\n",
    "action_node_dict[LEFT1]='K1'\n",
    "action_node_dict[LEFT2]='K2'\n",
    "action_node_dict[LEFT3]='K3'\n",
    "action_node_dict[LEFT4]='K4'\n",
    "action_node_dict[UP1]='J1'\n",
    "action_node_dict[UP2]='J2'\n",
    "action_node_dict[UP3]='J3'\n",
    "action_node_dict[UP4]='J4'\n",
    "action_node_dict[RIGHT1]='O1'\n",
    "action_node_dict[RIGHT2]='O2'\n",
    "action_node_dict[RIGHT3]='O3'\n",
    "action_node_dict[RIGHT4]='O4'\n",
    "action_node_dict[DOWN1]='I1'\n",
    "action_node_dict[DOWN2]='I2'\n",
    "action_node_dict[DOWN3]='I3'\n",
    "action_node_dict[DOWN4]='I4'\n",
    "df[\"{}_To_{}\".format(action_node_dict[DOWN1],action_node_dict[LEFT4])]\n",
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qmaze(object):\n",
    "    def __init__(self,  maze, rat=(0,1)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1,ncols-2) # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        self.current_distance = None\n",
    "        self.centroid = []\n",
    "        self.supervised = 0\n",
    "        self.old_action = 1\n",
    "        self.current_action = None\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        random.choice(self.free_cells)\n",
    "        self.non_available_cell = random.choices(self.free_cells,k=20)\n",
    "        self.reset(rat,self.non_available_cell)\n",
    "    def get_list_centroid(self):\n",
    "      dx=[0,3,0,-2,1,3,1,-2,-1,3,-1,-2,2,3,2,-2]\n",
    "      dy=[3,0,-2,0,3,1,-2,1,3,-1,-2,-1,3,2,-2,2]\n",
    "      for i in range (self._maze.shape[0]):\n",
    "        for j in range (self._maze.shape[1]):\n",
    "          if (self._maze[i][j]==0.95) :\n",
    "            self.centroid.append((i,j))\n",
    "            for temp in range(16):\n",
    "              tx = i + dx[temp]\n",
    "              ty = j + dy[temp]\n",
    "              state_centroid_dict[(tx,ty)] = (i,j)\n",
    "    def available_cell_for_agent(self):\n",
    "        list_available_cell_agent = []\n",
    "        nrows, ncols = self.maze.shape\n",
    "        for row in range(nrows) :\n",
    "            for col in range(ncols) :\n",
    "              if row == 1 or row == 2 or row == 3 or row ==6 or row == 7 or row ==8 or row == 11 or row == 12 or row == 13 :\n",
    "                if col == 0 or col == 14 :\n",
    "                      list_available_cell_agent.append((row,col))\n",
    "              if col== 1 or col == 2 or col == 3 or col ==6 or col == 7 or col ==8 or col == 11 or col == 12 or col == 13 :\n",
    "                if row == 0 or row == 14 :\n",
    "                      list_available_cell_agent.append((row,col))\n",
    "        return list_available_cell_agent\n",
    "    def reset(self, rat, non_available_cell):\n",
    "        self.rat = rat\n",
    "        self._maze = get_map()\n",
    "        nrows, ncols = self._maze.shape\n",
    "        row, col = rat\n",
    "        self._maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'valid')\n",
    "        self.min_reward = -256\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "        target_row,target_col = self.target\n",
    "        self.old_distance = -999\n",
    "        self.list_previous_state=[]\n",
    "        self.non_available_cell = non_available_cell\n",
    "        return self.observe().reshape(1,maze.size)\n",
    "    def check_state_centroid(self):\n",
    "      nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self._maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "        if self._maze[rat_row, rat_col] > 0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "        x_centroid,y_centroid = state_centroid_dict[(nrow, ncol)]\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            dx=[0,3,0,-2,1,3,1,-2,-1,3,-1,-2,2,3,2,-2]\n",
    "            dy=[3,0,-2,0,3,1,-2,1,3,-1,-2,-1,3,2,-2,2]\n",
    "            #r2,d2,l2,u2,r3,d3,l3,u3,r1,d1,l1,u1,r4,d4,l4,u4\n",
    "            if action == LEFT1:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[10]\n",
    "                y_centroid +=(dy[10]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[10]\n",
    "                y_centroid +=dy[10]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == LEFT2:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[2]\n",
    "                y_centroid +=(dy[2]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[2]\n",
    "                y_centroid +=dy[2]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == LEFT3:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[6]\n",
    "                y_centroid +=(dy[6]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[6]\n",
    "                y_centroid +=dy[6]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == LEFT4:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[14]\n",
    "                y_centroid +=(dy[14]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[14]\n",
    "                y_centroid +=dy[14]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT1:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[8]\n",
    "                y_centroid +=(dy[8]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[8]\n",
    "                y_centroid +=dy[8]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT2:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[0]\n",
    "                y_centroid +=(dy[0]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[0]\n",
    "                y_centroid +=dy[0]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT3:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[4]\n",
    "                y_centroid +=(dy[4]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[4]\n",
    "                y_centroid +=dy[4]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT4:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[12]\n",
    "                y_centroid +=(dy[12]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[12]\n",
    "                y_centroid +=dy[12]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN1:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[9]+1)\n",
    "                y_centroid +=dy[9]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[9]\n",
    "                y_centroid +=dy[9]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN2:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[1]+1)\n",
    "                y_centroid +=dy[1]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[1]\n",
    "                y_centroid +=dy[1]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN3:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[5]+1)\n",
    "                y_centroid +=dy[5]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[5]\n",
    "                y_centroid +=dy[5]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN4:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[13]+1)\n",
    "                y_centroid +=dy[13]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[13]\n",
    "                y_centroid +=dy[13]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP1:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[11]-1)\n",
    "                y_centroid +=dy[11]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[11]\n",
    "                y_centroid +=dy[11]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP2:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[3]-1)\n",
    "                y_centroid +=dy[3]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[3]\n",
    "                y_centroid +=dy[3]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP3:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[7]-1)\n",
    "                y_centroid +=dy[7]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[7]\n",
    "                y_centroid +=dy[7]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP4:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[15]-1)\n",
    "                y_centroid +=dy[15]\n",
    "                nmode = 'reward_unchanged'\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[15]\n",
    "                y_centroid +=dy[15]\n",
    "                if self._maze[x_centroid,y_centroid]==0.0 :\n",
    "                    nmode = 'blocked'\n",
    "                else :\n",
    "                    self.state = (x_centroid,y_centroid, nmode)\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            #nmode = 'invalid'\n",
    "            pass\n",
    "\n",
    "\n",
    "        # new state\n",
    "    \n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        target_row,target_col = self.target\n",
    "        nrows, ncols = self._maze.shape\n",
    "        reward = None\n",
    "        status = None\n",
    "        if mode == 'reward_unchanged'and not (rat_row, rat_col) in self.visited:\n",
    "          #mode = \"valid\"\n",
    "          return 0\n",
    "#         if self.current_action == self.old_action :\n",
    "#           return -1.2\n",
    "        if mode == 'blocked':\n",
    "            reward  =self.min_reward - 0.5\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -1.5\n",
    "        if mode == 'invalid':\n",
    "            reward  = -2\n",
    "        # if mode == 'valid':\n",
    "        #     reward  = -0.04\n",
    "        if mode == 'valid':\n",
    "            # self.current_distance = self.dist(rat_row,rat_col,target_row,target_col)\n",
    "            # if self.current_distance <= self.old_distance :\n",
    "            #       reward = -self.current_distance*0.004\n",
    "                  \n",
    "            # else : reward = -self.current_distance*0.005\n",
    "            # self.old_distance = self.current_distance\n",
    "            try : \n",
    "              reward = df[\"{}_To_{}\".format(action_node_dict[self.old_action],action_node_dict[self.current_action])]\n",
    "            except :\n",
    "              reward = -1\n",
    "        if rat_row == nrows-1 and rat_col == ncols-2:\n",
    "            status = 'win'\n",
    "            return 200\n",
    "        # if mode == 'valid' and status !='win':\n",
    "        #       self.current_distance = self.dist(rat_row,rat_col,target_row,target_col)\n",
    "        #       if (rat_row, rat_col) in self.visited :\n",
    "        #           reward = -0.25\n",
    "        #       elif self.current_distance >= self.old_distance :\n",
    "        #       #reward = -current_distance*0.001\n",
    "        #           reward = -0.1             \n",
    "        #       else : reward = -0.04\n",
    "        #       self.old_distance = self.current_distance\n",
    "        return reward\n",
    "    def dist(self,x1,y1,x2,y2):\n",
    "        return ((x2-x1)**2 + (y2-y1)**2)**0.5\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "    def act_to_predict(self, action):\n",
    "        self.update_state_to_predict(action)\n",
    "        status = self.game_status_to_predict()\n",
    "        envstate = self.observe()\n",
    "        return envstate, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env(self.non_available_cell)\n",
    "        envstate = np.reshape(canvas,newshape=(img_height,img_width,1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self, non_available_cell):\n",
    "        canvas = get_map()\n",
    "        nrows, ncols = self._maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "                if (r,c) in  non_available_cell:\n",
    "                    canvas[r,c] = 0.0\n",
    "                    self._maze[r,c]= 0.0\n",
    "        # draw the rat\n",
    "        \n",
    "        row, col, valid = self.state\n",
    "        row_target,col_target =  self.target\n",
    "        canvas[row, col] = rat_mark\n",
    "        canvas[row_target,col_target] = target_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self._maze.shape\n",
    "        target_row,target_col = self.target\n",
    "        if rat_row == target_row and rat_col == target_col:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "    def set_supervised(self,supervised):\n",
    "        self.supervised = supervised \n",
    "    def game_status_to_predict(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        target_row,target_col = self.target\n",
    "        if rat_row == target_row and rat_col == target_col:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [LEFT1, LEFT2, LEFT3,LEFT4, RIGHT1, RIGHT2, RIGHT3,RIGHT4, UP1, UP2, UP3,UP4, DOWN1, DOWN2, DOWN3,DOWN4]\n",
    "        nrows, ncols = self._maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(UP1)\n",
    "            actions.remove(UP2)\n",
    "            actions.remove(UP3)\n",
    "            actions.remove(UP4)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(DOWN1)\n",
    "            actions.remove(DOWN2)\n",
    "            actions.remove(DOWN3)\n",
    "            actions.remove(DOWN4)\n",
    "        if col == 0:\n",
    "            actions.remove(LEFT1)\n",
    "            actions.remove(LEFT2)\n",
    "            actions.remove(LEFT3)\n",
    "            actions.remove(LEFT4)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(RIGHT1)\n",
    "            actions.remove(RIGHT2)\n",
    "            actions.remove(RIGHT3)\n",
    "            actions.remove(RIGHT4)\n",
    "        # if row>0 and self.maze[row-1,col] == 0.0:\n",
    "        #     actions.remove(UP1)\n",
    "        #     actions.remove(UP2)\n",
    "        #     actions.remove(UP3)\n",
    "        #     actions.remove(UP4)\n",
    "        # if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "        #     actions.remove(DOWN1)\n",
    "        #     actions.remove(DOWN2)\n",
    "        #     actions.remove(DOWN3)\n",
    "        #     actions.remove(DOWN4)\n",
    "        # if col>0 and self.maze[row,col-1] == 0.0:\n",
    "        #     actions.remove(LEFT1)\n",
    "        #     actions.remove(LEFT2)\n",
    "        #     actions.remove(LEFT3)\n",
    "        #     actions.remove(LEFT4)\n",
    "        # if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "        #     actions.remove(RIGHT1)\n",
    "        #     actions.remove(RIGHT2)\n",
    "        #     actions.remove(RIGHT3)\n",
    "        #     actions.remove(RIGHT4)\n",
    "        dx=[0,3,0,-2,1,3,1,-2,-1,3,-1,-2,2,3,2,-2]\n",
    "        dy=[3,0,-2,0,3,1,-2,1,3,-1,-2,-1,3,2,-2,2]\n",
    "        tx_target,ty_target = self.target\n",
    "        if self.supervised == 1:\n",
    "          best_choice = None\n",
    "          best_distance = 999999\n",
    "          for action in actions :\n",
    "                x_centroid,y_centroid = state_centroid_dict[(row, col)]\n",
    "                if action == LEFT1:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[10]\n",
    "                    y_centroid +=(dy[10]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[10]\n",
    "                    y_centroid +=dy[10]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == LEFT2:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[2]\n",
    "                    y_centroid +=(dy[2]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target) \n",
    "                  else :\n",
    "                    x_centroid +=dx[2]\n",
    "                    y_centroid +=dy[2]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == LEFT3:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[6]\n",
    "                    y_centroid +=(dy[6]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[6]\n",
    "                    y_centroid +=dy[6]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == LEFT4:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[14]\n",
    "                    y_centroid +=(dy[14]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[14]\n",
    "                    y_centroid +=dy[14]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT1:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[8]\n",
    "                    y_centroid +=(dy[8]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[8]\n",
    "                    y_centroid +=dy[8]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT2:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[0]\n",
    "                    y_centroid +=(dy[0]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[0]\n",
    "                    y_centroid +=dy[0]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT3:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[4]\n",
    "                    y_centroid +=(dy[4]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[4]\n",
    "                    y_centroid +=dy[4]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT4:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[12]\n",
    "                    y_centroid +=(dy[12]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[12]\n",
    "                    y_centroid +=dy[12]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN1:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[9]+1)\n",
    "                    y_centroid +=dy[9]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[9]\n",
    "                    y_centroid +=dy[9]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN2:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[1]+1)\n",
    "                    y_centroid +=dy[1]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[1]\n",
    "                    y_centroid +=dy[1]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN3:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[5]+1)\n",
    "                    y_centroid +=dy[5]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[5]\n",
    "                    y_centroid +=dy[5]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN4:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[13]+1)\n",
    "                    y_centroid +=dy[13]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[13]\n",
    "                    y_centroid +=dy[13]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP1:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[11]-1)\n",
    "                    y_centroid +=dy[11]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[11]\n",
    "                    y_centroid +=dy[11]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP2:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[3]-1)\n",
    "                    y_centroid +=dy[3]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[3]\n",
    "                    y_centroid +=dy[3]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP3:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[7]-1)\n",
    "                    y_centroid +=dy[7]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[7]\n",
    "                    y_centroid +=dy[7]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP4:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[15]-1)\n",
    "                    y_centroid +=dy[15]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[15]\n",
    "                    y_centroid +=dy[15]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                if (cur_distance<best_distance):\n",
    "                    best_distance = cur_distance\n",
    "                    best_choice = action\n",
    "                #print(\"Best distance : \",best_distance, \" Best choice : \",best_choice)\n",
    "          actions.clear()\n",
    "          actions.append(best_choice)\n",
    "\n",
    "        return actions\n",
    "    def check_valid_with_previous_state(self,cell):\n",
    "        row_cell, col_cell = cell\n",
    "        for temp in self.visited:\n",
    "            row, col = temp\n",
    "            if row_cell == row and col_cell == col :\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fa8b2f4a08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze._maze.shape\n",
    "    row_target,col_target = qmaze.target\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows+20, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols+20, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze._maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[row_target,col_target] = 0.7 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    plt.show()\n",
    "    return img\n",
    "img_width, img_height = maze.shape\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_height, img_width)\n",
    "else:\n",
    "    input_shape = (img_height,img_width, 1)\n",
    "qmaze = Qmaze(maze)\n",
    "qmaze.get_list_centroid()\n",
    "# canvas, reward, game_over = qmaze.act(DOWN)\n",
    "# print(\"reward=\", reward)\n",
    "show(qmaze)\n",
    "qmaze.act(LEFT1) \n",
    "#qmaze.act(RIGHT2)\n",
    "qmaze.act(DOWN4)  # move down\n",
    "qmaze.act(DOWN4)  # move right\n",
    "qmaze.act(RIGHT2)  # move right\n",
    "qmaze.act(DOWN3)  # move right\n",
    "qmaze.act(UP1)  # move up\n",
    "qmaze.act(DOWN4)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x1,y1,x2,y2):\n",
    "        return ((x2-x1)**2 + (y2-y1)**2)**0.5\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "    def _write_logs(self, logs, index):\n",
    "        self.writer.reopen()\n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            if isinstance(value, np.ndarray):\n",
    "                summary_value.simple_value = value.item()\n",
    "            else:\n",
    "                summary_value.simple_value = value\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()\n",
    "class Memory:\n",
    "    \"\"\"class Memory: Stores experience trajectories.\n",
    "    Idea is to fill the memory with trajectories (s,a,r,gae_r,s_,d) tuples and get an arbitary number of\n",
    "    random batches from the memory for training. After that, clear and fill next set (on policy training).\n",
    "    The memory gets filled from the Agent object.\n",
    "    A special case is the batch_gae which gets calculated externally once when training on memory starts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.batch_s = []\n",
    "        self.batch_a = []\n",
    "        self.batch_r = []\n",
    "        self.batch_gae_r = [] #this gets set in agent make_gae which is called once on first training on memory\n",
    "        self.batch_s_ = []\n",
    "        self.batch_done = []\n",
    "        self.GAE_CALCULATED_Q = False #make sure make_gae can only be called once\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size):\n",
    "        \"\"\"simply retuns a randomized batch from the data in memory\n",
    "        r not really needed for training (gae_r is used). but might be interesting for logging.\n",
    "        \"\"\"\n",
    "        for _ in range(batch_size):\n",
    "            s,a,r,gae_r,s_,d = [],[],[],[],[],[]\n",
    "            pos = np.random.randint(len(self.batch_s)) #random position\n",
    "            s.append(self.batch_s[pos])\n",
    "            a.append(self.batch_a[pos])\n",
    "            r.append(self.batch_r[pos])\n",
    "            gae_r.append(self.batch_gae_r[pos])\n",
    "            s_.append(self.batch_s_[pos])\n",
    "            d.append(self.batch_done[pos])\n",
    "        return s,a,r,gae_r,s_,d #return randomized batches\n",
    "\n",
    "\n",
    "    def store(self, s, a, s_, r, done):\n",
    "        \"\"\"push s,a,r,s_,done into memory (=according lists)\n",
    "        \"\"\"\n",
    "        self.batch_s.append(s)\n",
    "        self.batch_a.append(a)\n",
    "        self.batch_r.append(r)\n",
    "        self.batch_s_.append(s_)\n",
    "        self.batch_done.append(done)\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"clear all lists (=memory)\n",
    "        \"\"\"\n",
    "        self.batch_s.clear()\n",
    "        self.batch_a.clear()\n",
    "        self.batch_r.clear()\n",
    "        self.batch_s_.clear()\n",
    "        self.batch_done.clear()\n",
    "        self.GAE_CALCULATED_Q = False\n",
    "\n",
    "\n",
    "    @property\n",
    "    def cnt_samples(self):\n",
    "        return len(self.batch_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,action_n, state_dim, training_batch_size):\n",
    "        \"\"\"This is the agent object.\n",
    "        Main interaction is the choose_action, store transition and train_network. \n",
    "        The agent only requires the state and action spaces to fuction, other than that it is pretty general\n",
    "        and should be easy to adapt for other deterministic envs.\n",
    "        To understand what is happening, I recommend to look at the ppo_loss method and the build_actor method first.\n",
    "        The training method itself is more or less only data preperation for calling the fit functions\n",
    "        for actor and critic. But critic has a trivial loss, so all the PPO magic is in the ppo_loss function.\n",
    "        \"\"\"\n",
    "        self.action_n = action_n\n",
    "        self.state_dim = state_dim        \n",
    "        #CONSTANTS\n",
    "        self.TRAINING_BATCH_SIZE = training_batch_size\n",
    "        self.GAMMA = 0.99\n",
    "        self.GAE_LAMBDA = 0.95\n",
    "        self.CLIPPING_LOSS_RATIO = 0.1\n",
    "        self.ENTROPY_LOSS_RATIO = 0.01\n",
    "        self.TARGET_UPDATE_ALPHA = 0.9\n",
    "        self.LR = 5e-9\n",
    "        #create actor and critic neural networks\n",
    "        self.critic_network = self._build_critic_network()\n",
    "        self.actor_network = self._build_actor_network()\n",
    "        try : \n",
    "            print(\"Loading model...\")\n",
    "            self.load_model(True)\n",
    "        except :\n",
    "            print(\"Load model failed\")\n",
    "        #for the loss function, additionally \"old\" predicitons are required from before the last update.\n",
    "        #therefore create another networtk. Set weights to be identical for now.\n",
    "        \n",
    "        self.actor_old_network = self._build_actor_network()\n",
    "        self.actor_old_network.set_weights(self.actor_network.get_weights()) \n",
    "        #for getting an action (predict), the model requires it's ususal input, but advantage and old_prediction is only used for loss(training). So create dummys for prediction only\n",
    "        self.dummy_advantage = np.zeros((1, 1))\n",
    "        self.dummy_old_prediciton = np.zeros((1, self.action_n))\n",
    "        #our transition memory buffer        \n",
    "        self.memory = Memory()\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.actor_network.save_weights(\"actor.h5\")\n",
    "        self.critic_network.save_weights(\"critic.h5\")\n",
    "        \n",
    "    def load_model(self,mode):\n",
    "        if mode == True :\n",
    "            self.actor_network.load_weights(\"actor.h5\")\n",
    "            self.critic_network.load_weights(\"critic.h5\")\n",
    "        \n",
    "    def _build_actor_network(self):\n",
    "        \"\"\"builds and returns a compiled keras.model for the actor.\n",
    "        There are 3 inputs. Only the state is for the pass though the neural net. \n",
    "        The other two inputs are exclusivly used for the custom loss function (ppo_loss).\n",
    "        \"\"\"\n",
    "        #define inputs. Advantage and old_prediction are required to pass to the ppo_loss funktion\n",
    "        state = keras.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        advantage = keras.layers.Input(shape=(1,),name='advantage_input')\n",
    "        old_prediction = keras.layers.Input(shape=(self.action_n,),name='old_prediction_input')\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense1')(state)\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense2')(dense)\n",
    "        #connect layers, output action using softmax activation\n",
    "        policy = keras.layers.Dense(self.action_n, activation=\"softmax\", name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        actor_network = keras.Model(inputs = [state,advantage,old_prediction], outputs = policy)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        actor_network.compile(\n",
    "            optimizer=Adam(lr=self.LR),\n",
    "            loss = self.ppo_loss(advantage=advantage,old_prediction=old_prediction)\n",
    "            )\n",
    "        #summary and return       \n",
    "        actor_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return actor_network\n",
    "\n",
    "\n",
    "    def _build_critic_network(self):\n",
    "        \"\"\"builds and returns a compiled keras.model for the critic.\n",
    "        The critic is a simple scalar prediction on the state value(output) given an state(input)\n",
    "        Loss is simply mse\n",
    "        \"\"\"\n",
    "        #define input layer\n",
    "        state = keras.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        #define hidden layers\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense1')(state)\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense2')(dense)\n",
    "        #connect the layers to a 1-dim output: scalar value of the state (= Q value or V(s))\n",
    "        V = keras.layers.Dense(1,activation='tanh', name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        critic_network = keras.Model(inputs=state, outputs=V)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        critic_network.compile(optimizer='Adam',loss = 'mean_squared_error')\n",
    "        #summary and return           \n",
    "        critic_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return critic_network\n",
    "    \n",
    "\n",
    "    def ppo_loss(self, advantage, old_prediction):\n",
    "        \"\"\"The PPO custom loss.\n",
    "        For explanation see for example:\n",
    "        https://youtu.be/WxQfQW48A4A\n",
    "        https://youtu.be/5P7I-xPq8u8\n",
    "        params:\n",
    "            :advantage: advantage, needed to process algorithm\n",
    "            :old_predictioN: prediction from \"old\" network, needed to process algorithm\n",
    "        returns:\n",
    "            :loss: keras type loss fuction (not a value but a fuction with two parameters y_true, y_pred)\n",
    "        \"\"\"\n",
    "        #refer to Keras custom loss function intro to understand why we define a funciton inside a function.\n",
    "        def loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred #y_true is taken action one_hot(in deterministic case) and pred is a softmax vector. prob is the probability of the taken aciton.\n",
    "            old_prob = y_true * old_prediction\n",
    "            ratio = prob / (old_prob + 1e-10)\n",
    "            clip_ratio = keras.backend.clip(ratio, min_value=1 - self.CLIPPING_LOSS_RATIO, max_value=1 + self.CLIPPING_LOSS_RATIO)\n",
    "            surrogate1 = ratio * advantage\n",
    "            surrogate2 = clip_ratio * advantage\n",
    "            entropy_loss = (prob * keras.backend.log(prob + 1e-10)) #optionally add the entropy loss to avoid getting stuck on local minima\n",
    "            ppo_loss = -keras.backend.mean(keras.backend.minimum(surrogate1,surrogate2) + self.ENTROPY_LOSS_RATIO * entropy_loss)\n",
    "            return ppo_loss\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def make_gae(self):\n",
    "        \"\"\"Generates GAE type rewards and pushes them into memory object\n",
    "        #GAE algorithm: \n",
    "            #delta = r + gamma * V(s') * mask - V(s)  |aka advantage\n",
    "            #gae = delta + gamma * lambda * mask * gae |moving average smoothing\n",
    "            #return(s,a) = gae + V(s)  |add value of state back to it.\n",
    "        \"\"\"\n",
    "        gae = 0\n",
    "        mask = 0\n",
    "        for i in reversed(range(self.memory.cnt_samples)):\n",
    "            mask = 0 if self.memory.batch_done[i] else 1\n",
    "            v = self.get_v(self.memory.batch_s[i])\n",
    "            delta = self.memory.batch_r[i] + self.GAMMA * self.get_v(self.memory.batch_s_[i]) * mask - v\n",
    "            gae = delta + self.GAMMA *  self.GAE_LAMBDA * mask * gae\n",
    "            self.memory.batch_gae_r.append(gae+v)\n",
    "        self.memory.batch_gae_r.reverse()\n",
    "        self.memory.GAE_CALCULATED_Q = True\n",
    "\n",
    "\n",
    "    def update_tartget_network(self):\n",
    "        \"\"\"Softupdate of the target network.\n",
    "        In ppo, the updates of the \n",
    "        \"\"\"\n",
    "        alpha = self.TARGET_UPDATE_ALPHA\n",
    "        actor_weights = np.array(self.actor_network.get_weights())\n",
    "        actor_tartget_weights = np.array(self.actor_old_network.get_weights())\n",
    "        new_weights = alpha*actor_weights + (1-alpha)*actor_tartget_weights\n",
    "        self.actor_old_network.set_weights(new_weights)\n",
    "\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "        \"\"\"chooses an action within the action space given a state.\n",
    "        The action is chosen by random with the weightings accoring to the probability\n",
    "        params:\n",
    "            :state: np.array of the states with state_dim length\n",
    "        \"\"\"\n",
    "        assert isinstance(state,np.ndarray)\n",
    "        #reshape for predict_on_batch which requires 2d-arrays\n",
    "        state = np.reshape(state,[-1,self.state_dim])\n",
    "        #the probability list for each action is the output of the actor network given a state\n",
    "        prob = self.actor_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton]).flatten()\n",
    "        #action is chosen by random with the weightings accoring to the probability\n",
    "        action = np.random.choice(self.action_n,p=prob)\n",
    "        return action,prob\n",
    "    \n",
    "\n",
    "    def train_network(self):\n",
    "        \"\"\"Train the actor and critic networks using GAE Algorithm.\n",
    "        1. Get GAE rewards\n",
    "        2. reshape batches s,a,gae_r baches\n",
    "        3. get value of state\n",
    "        4. calc advantage\n",
    "        5. get \"old\" precition (of target network)\n",
    "        6. fit actor and critic network\n",
    "        7. soft update target \"old\" network\n",
    "        \"\"\"\n",
    "        #important: make gae type rewards BEFORE getting random batches if not done yet\n",
    "        if not self.memory.GAE_CALCULATED_Q:\n",
    "            self.make_gae()\n",
    "        #get randomized mini batches\n",
    "        states,actions,rewards,gae_r,next_states,dones = self.memory.get_batch(self.TRAINING_BATCH_SIZE)\n",
    "       \n",
    "        #create np array batches for training\n",
    "        batch_s = np.vstack(states)\n",
    "        batch_a = np.vstack(actions)\n",
    "        batch_gae_r = np.vstack(gae_r)\n",
    "        #get values of states in batch\n",
    "        batch_v = self.get_v(batch_s)\n",
    "        #calc advantages. required for actor loss. \n",
    "        batch_advantage = batch_gae_r - batch_v\n",
    "        batch_advantage = keras.utils.normalize(batch_advantage) #\n",
    "        #calc old_prediction. Required for actor loss.\n",
    "        batch_old_prediction = self.get_old_prediction(batch_s)\n",
    "        #one-hot the actions. Actions will be the target for actor.\n",
    "        batch_a_final = np.zeros(shape=(len(batch_a), self.action_n))\n",
    "        batch_a_final[:, batch_a.flatten()] = 1\n",
    "\n",
    "        #commit training\n",
    "        self.actor_network.fit(x=[batch_s, batch_advantage, batch_old_prediction], y=batch_a_final, verbose=0)\n",
    "        self.critic_network.fit(x=batch_s, y=batch_gae_r, epochs=1, verbose=0)\n",
    "        #soft update the target network(aka actor_old). \n",
    "        self.update_tartget_network()\n",
    "\n",
    "\n",
    "    def store_transition(self, s, a, s_, r, done):\n",
    "        \"\"\"Store the experiences transtions into memory object.\n",
    "        \"\"\"\n",
    "        self.memory.store(s, a, s_, r, done)\n",
    "\n",
    "\n",
    "    def get_v(self,state):\n",
    "        \"\"\"Returns the value of the state.\n",
    "        Basically, just a forward pass though the critic networtk\n",
    "        \"\"\"\n",
    "        s = np.reshape(state,(-1, self.state_dim))\n",
    "        v = self.critic_network.predict_on_batch(s)\n",
    "        return v\n",
    "    \n",
    "\n",
    "    def get_old_prediction(self, state):\n",
    "        \"\"\"Makes an prediction (an action) given a state on the actor_old_network.\n",
    "        This is for the train_network --> ppo_loss\n",
    "        \"\"\"\n",
    "        state = np.reshape(state, (-1, self.state_dim))\n",
    "        return self.actor_old_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  45  Probability :  [8.3043236e-23 3.4074629e-12 6.9975220e-18 0.0000000e+00 2.1874382e-15\n",
      " 8.8114545e-16 4.0076083e-14 3.2922382e-37 1.4863194e-10 2.1453820e-16\n",
      " 1.0651054e-14 1.5720569e-32 4.2754013e-15 1.4477780e-01 8.5522223e-01\n",
      " 2.1766408e-24] \n",
      " Action :  14  Reward :  -1.5  Loss :  0  Win count :  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ce79d800a877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcnt_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_EPISODE_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;31m#get action from agent given state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pyLib-Gpu1.x\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mclear_output\u001b[1;34m(wait)\u001b[0m\n\u001b[0;32m   1462\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteractiveshell\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1464\u001b[1;33m         \u001b[0mInteractiveShell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1465\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\033[2K\\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pyLib-Gpu1.x\\lib\\site-packages\\ipykernel\\zmqshell.py\u001b[0m in \u001b[0;36mclear_output\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m    156\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush_streams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         self.session.send(\n\u001b[0;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_socket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'clear_output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pyLib-Gpu1.x\\lib\\site-packages\\ipykernel\\zmqshell.py\u001b[0m in \u001b[0;36m_flush_streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_flush_streams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;34m\"\"\"flush IO Streams prior to display\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pyLib-Gpu1.x\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pyLib-Gpu1.x\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\pyLib-Gpu1.x\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRAIN_ITERATIONS = 5000\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "TRAJECTORY_BUFFER_SIZE = 512\n",
    "BATCH_SIZE = 128\n",
    "RENDER_EVERY = 100\n",
    "AGGREGATE_STATS_EVERY = 5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"E:/RLAChips - Random\")\n",
    "    env = Qmaze(maze)\n",
    "    agent = Agent(num_actions,maze.size,BATCH_SIZE)\n",
    "    samples_filled = 0\n",
    "    tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "    ep_rewards=[]\n",
    "    ep_loss = []\n",
    "    win_count = 0\n",
    "    for cnt_episode in range(TRAIN_ITERATIONS):\n",
    "        rat_cell = random.choice(env.free_cells)\n",
    "        num_non_cell = np.random.randint(low = 30, high = 32)\n",
    "        non_available_cell = random.choices(env.free_cells,k=num_non_cell)\n",
    "        s = env.reset(rat_cell,non_available_cell)\n",
    "        r_sum = 0\n",
    "        loss_sum =0 \n",
    "        for cnt_step in range(MAX_EPISODE_LENGTH):\n",
    "            show(env)\n",
    "            clear_output(wait=True)\n",
    "            #get action from agent given state\n",
    "            a,prob = agent.choose_action(s)\n",
    "            #get s_,r,done\n",
    "            env.current_action = a\n",
    "            s_, r, done= env.act(a)\n",
    "            r_sum += r\n",
    "            if r==200 :\n",
    "                loss = 0\n",
    "                win_count+=1\n",
    "            elif r==0 :\n",
    "                loss = 0\n",
    "            elif r == -1 :\n",
    "                loss = 0\n",
    "            elif r == -1.5 :\n",
    "                try :\n",
    "                    loss = df[\"{}_To_{}\".format(action_node_dict[env.old_action],action_node_dict[env.current_action])]\n",
    "                except :\n",
    "                    loss = 0\n",
    "            else :\n",
    "                loss = r\n",
    "            loss_sum+=loss*10\n",
    "            if done == 'not_over':\n",
    "                done = False\n",
    "            else :\n",
    "                done = True\n",
    "            env.old_action = a\n",
    "            print(\"Episode : \",cnt_episode, \" Probability : \",prob,\"\\n Action : \",a,\" Reward : \",r,\" Loss : \",loss*10,\" Win count : \",win_count)\n",
    "            #store transitions to agent.memory\n",
    "        \n",
    "            agent.store_transition(s.reshape((-1,maze.size)), a, s_.reshape((-1,maze.size)), r, done)\n",
    "            samples_filled += 1\n",
    "            #train in batches one buffer is filled with samples.\n",
    "            if samples_filled % TRAJECTORY_BUFFER_SIZE == 0 and samples_filled != 0:\n",
    "                #To be sample efficient, sample as often as statistically necearry to \n",
    "                # use all availible samples in memory. Imortant to sample randomly \n",
    "                # to keep the training data independant and identically distributed IID\n",
    "                for _ in range(TRAJECTORY_BUFFER_SIZE // BATCH_SIZE):\n",
    "                    agent.train_network()\n",
    "                agent.memory.clear()\n",
    "                samples_filled = 0\n",
    "            #set state to next_state\n",
    "            s = s_\n",
    "            if done:\n",
    "                break\n",
    "        ep_rewards.append(r_sum)\n",
    "        ep_loss.append(loss_sum)\n",
    "        if  cnt_episode%AGGREGATE_STATS_EVERY==0 or cnt_episode == 1 :\n",
    "            print(\"updating stats...\")\n",
    "            average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            average_loss = sum(ep_loss[-AGGREGATE_STATS_EVERY:]) / len(ep_loss[-AGGREGATE_STATS_EVERY:])\n",
    "            tensorboard.step = cnt_episode\n",
    "            tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward,\n",
    "                                           reward_max=max_reward,tranmission_loss = average_loss,\n",
    "                                           epsilon=1)\n",
    "            agent.save_model()\n",
    "        if cnt_episode % 10 == 0:\n",
    "            print(f\"Episode:{cnt_episode}, step:{cnt_step}, r_sum:{r_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOslRe9iSuuIZbBBqNQE42x",
   "collapsed_sections": [],
   "name": "Bn sao ca RLAChip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
