{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87198,
     "status": "ok",
     "timestamp": 1593147077598,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "CwfA47wzHi1X",
    "outputId": "492c3f3f-03dd-4a77-ac38-a6ef388eeb0e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.impute import SimpleImputer\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tensorflow import nn ## de goi cac active function\n",
    "from sklearn import tree\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,Activation, Dropout, Flatten, Dense\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from collections import namedtuple\n",
    "import matplotlib\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from IPython.display import clear_output\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import np_utils ## dung de categorical cac label\n",
    "from sklearn.model_selection import train_test_split  ## dung de tach bo test ra\n",
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "import datetime\n",
    "from math import pow\n",
    "import random\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from collections import deque\n",
    "import csv\n",
    "from keras.layers import PReLU\n",
    "import math\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import h5py\n",
    "import simplejson as json\n",
    "from tqdm import tqdm\n",
    "from keras_radam import RAdam\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import clear_output\n",
    "from keras.utils import  to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import math\n",
    "from keras.callbacks import History \n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87188,
     "status": "ok",
     "timestamp": 1593147077599,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "_Mf8NWHmEvrd",
    "outputId": "7f06962f-ab94-47d0-c052-26a67208675c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze=[]\n",
    "for i in range(30):\n",
    "    if (i==0 or i==5 or i==6 or i==11 or i==12 or i==17 or i==18 or i == 23 or i== 24 or i==29 or i == 30):\n",
    "        maze.append([0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0,0,1,1,1,1,0])\n",
    "    elif (i==2 or i==8 or i==14 or i == 20 or i==26 or i == 32) :\n",
    "      maze.append([1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1,1,0.9,0.95,0.9,0.9,1])\n",
    "    else : \n",
    "        maze.append([1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1,1,0.9,0.9,0.9,0.9,1])\n",
    "# for i in range(35):\n",
    "#     if (i==0 or i==4 or i==5 or i==9 or i==10 or i==14 or i==15 or i==19 or i==20 or i==24 or i==25 or i==29 or i==30 or i==34 or i==35):\n",
    "#         maze.append([0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0,0,1,1,1,0])\n",
    "#     elif (i==2 or i==7 or i==12 or i==17 or i==22 or i==27 or i==32) :\n",
    "#       maze.append([1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1,1,0.9,0.95,0.9,1])\n",
    "#     else : \n",
    "#         maze.append([1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1,1,0.9,0.9,0.9,1])\n",
    "maze = np.asarray(maze)\n",
    "maze = maze.astype(float)\n",
    "maze.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87186,
     "status": "ok",
     "timestamp": 1593147077600,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "7vAa9PvDEv0O"
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "target_mark = 0.6\n",
    "LEFT1 = 0\n",
    "LEFT2 = 1\n",
    "LEFT3 = 2\n",
    "LEFT4 = 3\n",
    "UP1 = 4\n",
    "UP2 = 5\n",
    "UP3 = 6\n",
    "UP4 = 7\n",
    "RIGHT1 = 8\n",
    "RIGHT2 = 9\n",
    "RIGHT3 = 10\n",
    "RIGHT4 = 11\n",
    "DOWN1 = 12\n",
    "DOWN2 = 13\n",
    "DOWN3 = 14\n",
    "DOWN4 = 15\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT1: 'left1',\n",
    "    LEFT2: 'left2',\n",
    "    LEFT3: 'left3',\n",
    "    LEFT4: 'left4',\n",
    "    UP1:'up1',\n",
    "    UP2:'up2',\n",
    "    UP3:'up3',\n",
    "    UP4:'up4',\n",
    "    RIGHT1: 'right1',\n",
    "    RIGHT2: 'right2',\n",
    "    RIGHT3: 'right3',\n",
    "    RIGHT4: 'right4',\n",
    "    DOWN1: 'down1',\n",
    "    DOWN2: 'down2',\n",
    "    DOWN3: 'down3',\n",
    "    DOWN4: 'down4'\n",
    "}\n",
    "state_centroid_dict = {}\n",
    "num_actions = len(actions_dict)\n",
    "MODEL_NAME = \"model\"\n",
    "# Exploration factor\n",
    "AGGREGATE_STATS_EVERY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 87184,
     "status": "ok",
     "timestamp": 1593147077601,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "4CA8FigBY-0o"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"E:/RLAChips - Random/data\")\n",
    "source_data = glob.glob(\"*.csv\")\n",
    "os.getcwd()\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 114515,
     "status": "ok",
     "timestamp": 1593147104943,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "EHCZqC2Rcrdb",
    "outputId": "7dabe51e-425b-4052-90ab-6344ec6868fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3  \\\n",
      "0   1.5250  6.267768e-04  7.008009e-05  7.357284e-03  6.423477e-04   \n",
      "1   1.5275  2.194808e-03  8.639115e-05  9.189811e-03  2.403074e-04   \n",
      "2   1.5300  1.671665e-02  2.634570e-04  1.545621e-02  7.445869e-05   \n",
      "3   1.5325  3.662386e-01  4.872822e-03  3.790922e-02  9.149724e-03   \n",
      "4   1.5350  6.132423e-01  8.438783e-03  1.631000e-02  1.562700e-02   \n",
      "5   1.5375  6.612227e-01  8.108768e-03  4.204107e-02  1.048023e-02   \n",
      "6   1.5400  4.991382e-01  4.970275e-03  5.351833e-02  5.230132e-03   \n",
      "7   1.5425  4.043050e-01  3.501639e-03  4.201422e-02  1.452211e-02   \n",
      "8   1.5450  6.379861e-01  6.492284e-03  2.736030e-02  3.639567e-02   \n",
      "9   1.5475  5.319590e-01  8.605414e-03  1.105764e-02  1.193523e-02   \n",
      "10  1.5500  5.689370e-01  1.504422e-02  4.147845e-02  3.621517e-03   \n",
      "11  1.5525  5.712135e-01  2.167229e-02  3.957852e-02  1.048726e-02   \n",
      "12  1.5550  4.536188e-01  2.354945e-02  3.139316e-03  3.126116e-03   \n",
      "13  1.5575  1.564698e-01  1.129499e-02  5.973522e-02  8.397369e-03   \n",
      "14  1.5600  5.181086e-04  3.851388e-05  7.266459e-04  2.377631e-04   \n",
      "15  1.5625  3.196805e-04  1.955239e-05  1.055047e-04  4.376495e-05   \n",
      "16  1.5650  2.018542e-04  1.175204e-05  4.070887e-05  1.368702e-05   \n",
      "17  1.5675  1.369221e-04  7.380850e-06  2.688887e-05  5.952853e-06   \n",
      "18  1.5700  1.010686e-04  4.843824e-06  2.182827e-05  3.265751e-06   \n",
      "19  1.5725  8.079313e-05  3.275742e-06  1.928429e-05  2.096339e-06   \n",
      "20  1.5750  6.984510e-05  2.268738e-06  1.794198e-05  1.480648e-06   \n",
      "21  1.5775  6.565906e-05  1.548661e-06  1.789426e-05  1.133546e-06   \n",
      "22  1.5800  6.865344e-05  1.056877e-06  1.941261e-05  9.361029e-07   \n",
      "23  1.5825  8.418182e-05  7.722900e-07  2.474754e-05  4.591283e-07   \n",
      "24  1.5850  1.404230e-04  1.305274e-06  4.355419e-05  1.226920e-06   \n",
      "25  1.5875  8.325868e-04  1.971355e-05  2.637415e-04  1.874987e-05   \n",
      "26  1.5900  5.833713e-04  5.002408e-05  4.763878e-04  6.293087e-05   \n",
      "27  1.5925  4.797351e-05  7.790284e-06  4.845242e-06  6.921793e-06   \n",
      "28  1.5950  8.670987e-06  3.267251e-06  4.751514e-07  3.573952e-06   \n",
      "29  1.5975  2.320437e-06  2.038487e-06  2.121258e-08  2.671734e-06   \n",
      "30  1.6000  9.018211e-07  1.460907e-06  1.640141e-08  2.827580e-06   \n",
      "31  1.6025  7.123654e-07  1.101834e-06  8.552133e-08  2.566945e-05   \n",
      "32  1.6050  1.304982e-06  7.304065e-07  9.891818e-07  3.667371e-06   \n",
      "33  1.6075  2.708918e-06  1.704552e-06  9.448553e-05  8.512909e-06   \n",
      "34  1.6100  4.893705e-07  1.086110e-06  1.090949e-06  1.125744e-06   \n",
      "35  1.6125  3.855736e-07  8.342046e-07  6.413904e-07  1.035044e-06   \n",
      "36  1.6150  4.342195e-07  7.247505e-07  5.881899e-07  9.609059e-07   \n",
      "37  1.6175  4.789411e-07  6.419506e-07  6.432722e-07  9.072635e-07   \n",
      "38  1.6200  5.099075e-07  5.890021e-07  7.972293e-07  8.664508e-07   \n",
      "39  1.6225  5.293102e-07  5.483521e-07  1.341902e-06  8.384885e-07   \n",
      "40  1.6250  4.711664e-07  5.245234e-07  8.839217e-06  8.902332e-07   \n",
      "\n",
      "           Loss4  \n",
      "0   1.905137e-05  \n",
      "1   3.727799e-05  \n",
      "2   1.899631e-04  \n",
      "3   3.558606e-03  \n",
      "4   6.862756e-03  \n",
      "5   1.181117e-02  \n",
      "6   1.716079e-02  \n",
      "7   2.058878e-02  \n",
      "8   3.306899e-02  \n",
      "9   2.946688e-02  \n",
      "10  4.324636e-02  \n",
      "11  3.234890e-02  \n",
      "12  6.780882e-03  \n",
      "13  5.574741e-04  \n",
      "14  4.383446e-06  \n",
      "15  2.709510e-06  \n",
      "16  1.636726e-06  \n",
      "17  1.102634e-06  \n",
      "18  8.090117e-07  \n",
      "19  6.315467e-07  \n",
      "20  5.108062e-07  \n",
      "21  4.327158e-07  \n",
      "22  3.728905e-07  \n",
      "23  3.496818e-07  \n",
      "24  3.110605e-07  \n",
      "25  2.159018e-07  \n",
      "26  8.670110e-08  \n",
      "27  2.263835e-07  \n",
      "28  2.211941e-07  \n",
      "29  2.411727e-07  \n",
      "30  2.186782e-07  \n",
      "31  2.224140e-07  \n",
      "32  1.814627e-07  \n",
      "33  2.510174e-07  \n",
      "34  1.837723e-07  \n",
      "35  1.804133e-07  \n",
      "36  1.777462e-07  \n",
      "37  1.766439e-07  \n",
      "38  1.762250e-07  \n",
      "39  1.759063e-07  \n",
      "40  1.762138e-07  \n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.294852  0.002311  2.635875e-03\n",
      "1   1.5275  0.309081  0.001490  1.795729e-03\n",
      "2   1.5300  0.334105  0.001282  1.623990e-03\n",
      "3   1.5325  0.369770  0.001444  1.996089e-03\n",
      "4   1.5350  0.417315  0.001999  3.634116e-03\n",
      "5   1.5375  0.478206  0.002728  1.177032e-02\n",
      "6   1.5400  0.545807  0.003619  4.206769e-02\n",
      "7   1.5425  0.645566  0.005418  2.027403e-02\n",
      "8   1.5450  0.753076  0.012537  1.662217e-02\n",
      "9   1.5475  0.858329  0.013565  3.596371e-02\n",
      "10  1.5500  0.947207  0.007951  4.717223e-02\n",
      "11  1.5525  1.035638  0.009383  1.425992e-02\n",
      "12  1.5550  0.981476  0.039715  1.605307e-02\n",
      "13  1.5575  0.938121  0.002418  5.003545e-03\n",
      "14  1.5600  0.790970  0.000272  2.867008e-04\n",
      "15  1.5625  0.632660  0.000056  4.374967e-05\n",
      "16  1.5650  0.492222  0.000016  1.067989e-05\n",
      "17  1.5675  0.377301  0.000006  3.440465e-06\n",
      "18  1.5700  0.286430  0.000003  1.312012e-06\n",
      "19  1.5725  0.215189  0.000002  5.399587e-07\n",
      "20  1.5750  0.159029  0.000002  2.009906e-07\n",
      "21  1.5775  0.114237  0.000002  4.520259e-08\n",
      "22  1.5800  0.078122  0.000002  4.338622e-08\n",
      "23  1.5825  0.050120  0.000003  7.267866e-08\n",
      "24  1.5850  0.026873  0.000003  1.029577e-07\n",
      "25  1.5875  0.010089  0.000004  1.191867e-07\n",
      "26  1.5900  0.001381  0.000005  1.264474e-07\n",
      "27  1.5925  0.004232  0.000007  1.133919e-07\n",
      "28  1.5950  0.024663  0.000011  4.305505e-08\n",
      "29  1.5975  0.070906  0.000023  1.041764e-07\n",
      "30  1.6000  0.152485  0.000088  6.961811e-07\n",
      "31  1.6025  0.276925  0.000783  5.479279e-06\n",
      "32  1.6050  0.439360  0.000505  1.410392e-05\n",
      "33  1.6075  0.606305  0.000225  4.841430e-06\n",
      "34  1.6100  0.749445  0.000144  1.865210e-07\n",
      "35  1.6125  0.855375  0.000017  2.176731e-08\n",
      "36  1.6150  0.921200  0.000010  1.161037e-07\n",
      "37  1.6175  0.956710  0.000007  1.541489e-07\n",
      "38  1.6200  0.972260  0.000005  1.800037e-07\n",
      "39  1.6225  0.975305  0.000004  1.985664e-07\n",
      "40  1.6250  0.970082  0.000004  2.117713e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.001691  5.408815e-04  3.213630e-03\n",
      "1   1.5275  0.003403  2.252752e-04  4.400668e-03\n",
      "2   1.5300  0.012617  7.455876e-06  9.245909e-03\n",
      "3   1.5325  0.154955  4.608904e-03  4.642866e-02\n",
      "4   1.5350  0.664325  3.988920e-02  3.896226e-02\n",
      "5   1.5375  0.842653  3.642013e-02  1.449024e-02\n",
      "6   1.5400  0.614232  7.385617e-03  3.864665e-02\n",
      "7   1.5425  0.617709  7.272574e-03  4.674179e-02\n",
      "8   1.5450  0.870990  2.447510e-02  3.771547e-02\n",
      "9   1.5475  0.959850  2.372102e-02  1.785169e-02\n",
      "10  1.5500  0.662230  8.030389e-03  2.120213e-02\n",
      "11  1.5525  0.921199  3.092431e-02  4.620466e-02\n",
      "12  1.5550  0.828882  4.400454e-02  2.500839e-02\n",
      "13  1.5575  0.455419  1.881693e-02  1.720995e-02\n",
      "14  1.5600  0.001033  1.593887e-03  6.961007e-04\n",
      "15  1.5625  0.000265  2.042644e-04  5.915334e-05\n",
      "16  1.5650  0.000168  5.718142e-05  1.418678e-05\n",
      "17  1.5675  0.000112  2.358230e-05  5.510658e-06\n",
      "18  1.5700  0.000080  1.248622e-05  2.760080e-06\n",
      "19  1.5725  0.000060  7.899860e-06  1.579207e-06\n",
      "20  1.5750  0.000048  5.732213e-06  9.488055e-07\n",
      "21  1.5775  0.000039  4.679222e-06  5.625864e-07\n",
      "22  1.5800  0.000032  4.317070e-06  2.820897e-07\n",
      "23  1.5825  0.000028  4.872743e-06  7.650059e-08\n",
      "24  1.5850  0.000024  6.976767e-06  3.269183e-09\n",
      "25  1.5875  0.000021  2.864441e-05  4.032823e-06\n",
      "26  1.5900  0.000018  2.708856e-05  2.608703e-05\n",
      "27  1.5925  0.000018  1.121585e-06  2.333378e-06\n",
      "28  1.5950  0.000017  4.177431e-07  1.190624e-06\n",
      "29  1.5975  0.000017  4.030539e-07  7.372328e-07\n",
      "30  1.6000  0.000018  5.331796e-07  4.338941e-07\n",
      "31  1.6025  0.000024  1.017740e-06  9.119234e-08\n",
      "32  1.6050  0.000104  2.344605e-05  1.666409e-05\n",
      "33  1.6075  0.000051  9.154693e-07  1.186269e-05\n",
      "34  1.6100  0.000028  1.948099e-06  1.219098e-05\n",
      "35  1.6125  0.000009  3.114765e-07  1.148367e-06\n",
      "36  1.6150  0.000007  2.286389e-07  6.143595e-07\n",
      "37  1.6175  0.000006  2.179007e-07  4.518979e-07\n",
      "38  1.6200  0.000006  2.275329e-07  3.670865e-07\n",
      "39  1.6225  0.000005  2.461583e-07  3.095064e-07\n",
      "40  1.6250  0.000005  3.871730e-07  2.811765e-07\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  3.249131e-03  1.157054e-03\n",
      "1   1.5275  0.118812  2.613183e-03  6.583992e-04\n",
      "2   1.5300  0.183227  2.781244e-03  4.358437e-04\n",
      "3   1.5325  0.311414  3.799868e-03  5.476985e-04\n",
      "4   1.5350  0.536881  6.214607e-03  1.940065e-03\n",
      "5   1.5375  0.785143  1.063555e-02  6.469603e-03\n",
      "6   1.5400  0.886749  1.868861e-02  1.096699e-02\n",
      "7   1.5425  0.882151  3.886312e-02  1.137066e-02\n",
      "8   1.5450  0.888211  4.058553e-02  1.062819e-02\n",
      "9   1.5475  0.898351  1.882241e-02  1.231554e-02\n",
      "10  1.5500  0.884318  1.285530e-02  2.202517e-02\n",
      "11  1.5525  0.891510  1.641046e-02  3.164553e-02\n",
      "12  1.5550  0.750956  2.978085e-02  8.950454e-03\n",
      "13  1.5575  0.311554  4.397421e-03  1.516899e-03\n",
      "14  1.5600  0.093184  2.851123e-04  2.572778e-04\n",
      "15  1.5625  0.033225  3.847958e-05  6.024352e-05\n",
      "16  1.5650  0.014730  9.810420e-06  1.877581e-05\n",
      "17  1.5675  0.007910  4.079919e-06  7.290227e-06\n",
      "18  1.5700  0.005029  2.359308e-06  3.380012e-06\n",
      "19  1.5725  0.003729  1.656281e-06  1.829884e-06\n",
      "20  1.5750  0.003201  1.292808e-06  1.161332e-06\n",
      "21  1.5775  0.003186  1.061193e-06  8.638020e-07\n",
      "22  1.5800  0.003736  8.840757e-07  8.006064e-07\n",
      "23  1.5825  0.004868  6.678824e-07  8.602698e-07\n",
      "24  1.5850  0.009585  4.383928e-07  1.716265e-06\n",
      "25  1.5875  0.045032  1.942413e-07  9.298206e-06\n",
      "26  1.5900  0.190316  1.750018e-05  7.138411e-05\n",
      "27  1.5925  0.009399  2.096493e-06  4.743000e-06\n",
      "28  1.5950  0.002505  1.572777e-06  1.970329e-06\n",
      "29  1.5975  0.001030  1.383101e-06  1.623976e-06\n",
      "30  1.6000  0.000513  1.639650e-06  2.830783e-06\n",
      "31  1.6025  0.000302  4.824041e-06  3.687513e-05\n",
      "32  1.6050  0.000174  1.069142e-06  5.351556e-08\n",
      "33  1.6075  0.000114  6.800334e-06  1.233166e-06\n",
      "34  1.6100  0.000081  3.778238e-07  5.756362e-08\n",
      "35  1.6125  0.000068  3.178216e-07  1.064093e-08\n",
      "36  1.6150  0.000070  3.811052e-07  1.364581e-08\n",
      "37  1.6175  0.000089  4.076630e-07  1.865470e-08\n",
      "38  1.6200  0.000143  4.278335e-07  2.225208e-08\n",
      "39  1.6225  0.000324  4.344700e-07  2.490008e-08\n",
      "40  1.6250  0.002578  4.196965e-07  4.331406e-08\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  9.275143e-06  3.249131e-03\n",
      "1   1.5275  0.118812  1.240541e-05  2.613183e-03\n",
      "2   1.5300  0.183227  2.183270e-05  2.781244e-03\n",
      "3   1.5325  0.311414  4.718525e-05  3.799868e-03\n",
      "4   1.5350  0.536881  9.373105e-05  6.214607e-03\n",
      "5   1.5375  0.785143  1.233090e-04  1.063555e-02\n",
      "6   1.5400  0.886749  1.488600e-04  1.868861e-02\n",
      "7   1.5425  0.882151  1.721407e-04  3.886312e-02\n",
      "8   1.5450  0.888211  1.664701e-04  4.058553e-02\n",
      "9   1.5475  0.898351  1.511181e-04  1.882241e-02\n",
      "10  1.5500  0.884318  1.197090e-04  1.285530e-02\n",
      "11  1.5525  0.891510  5.678842e-05  1.641046e-02\n",
      "12  1.5550  0.750956  2.636283e-05  2.978085e-02\n",
      "13  1.5575  0.311554  8.209738e-06  4.397421e-03\n",
      "14  1.5600  0.093184  5.872857e-07  2.851123e-04\n",
      "15  1.5625  0.033225  1.921785e-07  3.847958e-05\n",
      "16  1.5650  0.014730  1.224903e-07  9.810420e-06\n",
      "17  1.5675  0.007910  9.910043e-08  4.079919e-06\n",
      "18  1.5700  0.005029  8.774090e-08  2.359308e-06\n",
      "19  1.5725  0.003729  8.086510e-08  1.656281e-06\n",
      "20  1.5750  0.003201  7.532593e-08  1.292808e-06\n",
      "21  1.5775  0.003186  7.206798e-08  1.061193e-06\n",
      "22  1.5800  0.003736  6.910069e-08  8.840757e-07\n",
      "23  1.5825  0.004868  6.383778e-08  6.678824e-07\n",
      "24  1.5850  0.009585  6.207522e-08  4.383928e-07\n",
      "25  1.5875  0.045032  6.199881e-08  1.942413e-07\n",
      "26  1.5900  0.190316  1.598424e-07  1.750018e-05\n",
      "27  1.5925  0.009399  3.989894e-08  2.096493e-06\n",
      "28  1.5950  0.002505  4.372294e-08  1.572777e-06\n",
      "29  1.5975  0.001030  4.314594e-08  1.383101e-06\n",
      "30  1.6000  0.000513  4.067785e-08  1.639650e-06\n",
      "31  1.6025  0.000302  7.136202e-08  4.824041e-06\n",
      "32  1.6050  0.000174  3.886280e-08  1.069142e-06\n",
      "33  1.6075  0.000114  1.053652e-07  6.800334e-06\n",
      "34  1.6100  0.000081  6.681929e-09  3.778238e-07\n",
      "35  1.6125  0.000068  2.825183e-08  3.178216e-07\n",
      "36  1.6150  0.000070  2.910617e-08  3.811052e-07\n",
      "37  1.6175  0.000089  2.790654e-08  4.076630e-07\n",
      "38  1.6200  0.000143  2.688429e-08  4.278335e-07\n",
      "39  1.6225  0.000324  2.600963e-08  4.344700e-07\n",
      "40  1.6250  0.002578  2.723457e-08  4.196965e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  8.678195e-03  4.354697e-03  7.979320e-04\n",
      "1   1.5275  1.464066e-02  4.603720e-03  3.862844e-04\n",
      "2   1.5300  3.787371e-02  6.737941e-03  8.956786e-05\n",
      "3   1.5325  1.789523e-01  1.542104e-02  1.004522e-03\n",
      "4   1.5350  8.346425e-01  2.707670e-02  1.730301e-02\n",
      "5   1.5375  7.881981e-01  7.361873e-03  2.512415e-02\n",
      "6   1.5400  9.421682e-01  7.993247e-03  2.622900e-02\n",
      "7   1.5425  7.225043e-01  1.494140e-02  9.523406e-03\n",
      "8   1.5450  6.045495e-01  2.154035e-02  4.897305e-03\n",
      "9   1.5475  8.158972e-01  3.391100e-02  1.155260e-02\n",
      "10  1.5500  9.152217e-01  2.872994e-02  1.409190e-02\n",
      "11  1.5525  1.031628e+00  1.594717e-02  9.329139e-03\n",
      "12  1.5550  7.263572e-01  1.031064e-02  2.668847e-03\n",
      "13  1.5575  4.447288e-01  1.393132e-02  5.157502e-03\n",
      "14  1.5600  6.175406e-03  3.591040e-04  3.403970e-04\n",
      "15  1.5625  3.572644e-04  2.709877e-05  6.092533e-05\n",
      "16  1.5650  7.057531e-05  3.265422e-06  1.778475e-05\n",
      "17  1.5675  3.676279e-05  5.636786e-07  6.868278e-06\n",
      "18  1.5700  2.596438e-05  2.223149e-07  3.247247e-06\n",
      "19  1.5725  1.936498e-05  1.984777e-07  1.809216e-06\n",
      "20  1.5750  1.448678e-05  2.304673e-07  1.172802e-06\n",
      "21  1.5775  1.054124e-05  2.870431e-07  8.914730e-07\n",
      "22  1.5800  7.097104e-06  4.136866e-07  8.517066e-07\n",
      "23  1.5825  2.872110e-06  6.842304e-07  9.936282e-07\n",
      "24  1.5850  6.000600e-07  1.808587e-06  2.678692e-06\n",
      "25  1.5875  5.398022e-05  1.556182e-05  2.662113e-05\n",
      "26  1.5900  3.096854e-04  4.860001e-05  5.544758e-05\n",
      "27  1.5925  5.218712e-05  1.745059e-06  5.215496e-06\n",
      "28  1.5950  3.553839e-05  5.937621e-07  2.377305e-06\n",
      "29  1.5975  3.225042e-05  3.267548e-07  1.717714e-06\n",
      "30  1.6000  3.636656e-05  6.286471e-07  2.129434e-06\n",
      "31  1.6025  5.353359e-05  1.724389e-05  2.574305e-05\n",
      "32  1.6050  1.455642e-04  8.067656e-06  4.426458e-06\n",
      "33  1.6075  7.479076e-04  2.178997e-05  7.372365e-06\n",
      "34  1.6100  4.359472e-05  7.170879e-07  5.648023e-07\n",
      "35  1.6125  6.184811e-06  5.420901e-07  4.704096e-07\n",
      "36  1.6150  1.240907e-06  4.512076e-07  4.072215e-07\n",
      "37  1.6175  2.004300e-07  3.991376e-07  3.627241e-07\n",
      "38  1.6200  3.276010e-08  3.611923e-07  3.290254e-07\n",
      "39  1.6225  4.536845e-08  3.499531e-07  3.040243e-07\n",
      "40  1.6250  5.310729e-08  4.279843e-07  3.075118e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  8.678195e-03  4.354697e-03  7.979320e-04\n",
      "1   1.5275  1.464066e-02  4.603720e-03  3.862844e-04\n",
      "2   1.5300  3.787371e-02  6.737941e-03  8.956786e-05\n",
      "3   1.5325  1.789523e-01  1.542104e-02  1.004522e-03\n",
      "4   1.5350  8.346425e-01  2.707670e-02  1.730301e-02\n",
      "5   1.5375  7.881981e-01  7.361873e-03  2.512415e-02\n",
      "6   1.5400  9.421682e-01  7.993247e-03  2.622900e-02\n",
      "7   1.5425  7.225043e-01  1.494140e-02  9.523406e-03\n",
      "8   1.5450  6.045495e-01  2.154035e-02  4.897305e-03\n",
      "9   1.5475  8.158972e-01  3.391100e-02  1.155260e-02\n",
      "10  1.5500  9.152217e-01  2.872994e-02  1.409190e-02\n",
      "11  1.5525  1.031628e+00  1.594717e-02  9.329139e-03\n",
      "12  1.5550  7.263572e-01  1.031064e-02  2.668847e-03\n",
      "13  1.5575  4.447288e-01  1.393132e-02  5.157502e-03\n",
      "14  1.5600  6.175406e-03  3.591040e-04  3.403970e-04\n",
      "15  1.5625  3.572644e-04  2.709877e-05  6.092533e-05\n",
      "16  1.5650  7.057531e-05  3.265422e-06  1.778475e-05\n",
      "17  1.5675  3.676279e-05  5.636786e-07  6.868278e-06\n",
      "18  1.5700  2.596438e-05  2.223149e-07  3.247247e-06\n",
      "19  1.5725  1.936498e-05  1.984777e-07  1.809216e-06\n",
      "20  1.5750  1.448678e-05  2.304673e-07  1.172802e-06\n",
      "21  1.5775  1.054124e-05  2.870431e-07  8.914730e-07\n",
      "22  1.5800  7.097104e-06  4.136866e-07  8.517066e-07\n",
      "23  1.5825  2.872110e-06  6.842304e-07  9.936282e-07\n",
      "24  1.5850  6.000600e-07  1.808587e-06  2.678692e-06\n",
      "25  1.5875  5.398022e-05  1.556182e-05  2.662113e-05\n",
      "26  1.5900  3.096854e-04  4.860001e-05  5.544758e-05\n",
      "27  1.5925  5.218712e-05  1.745059e-06  5.215496e-06\n",
      "28  1.5950  3.553839e-05  5.937621e-07  2.377305e-06\n",
      "29  1.5975  3.225042e-05  3.267548e-07  1.717714e-06\n",
      "30  1.6000  3.636656e-05  6.286471e-07  2.129434e-06\n",
      "31  1.6025  5.353359e-05  1.724389e-05  2.574305e-05\n",
      "32  1.6050  1.455642e-04  8.067656e-06  4.426458e-06\n",
      "33  1.6075  7.479076e-04  2.178997e-05  7.372365e-06\n",
      "34  1.6100  4.359472e-05  7.170879e-07  5.648023e-07\n",
      "35  1.6125  6.184811e-06  5.420901e-07  4.704096e-07\n",
      "36  1.6150  1.240907e-06  4.512076e-07  4.072215e-07\n",
      "37  1.6175  2.004300e-07  3.991376e-07  3.627241e-07\n",
      "38  1.6200  3.276010e-08  3.611923e-07  3.290254e-07\n",
      "39  1.6225  4.536845e-08  3.499531e-07  3.040243e-07\n",
      "40  1.6250  5.310729e-08  4.279843e-07  3.075118e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  3.249131e-03  1.157054e-03\n",
      "1   1.5275  0.118812  2.613183e-03  6.583992e-04\n",
      "2   1.5300  0.183227  2.781244e-03  4.358437e-04\n",
      "3   1.5325  0.311414  3.799868e-03  5.476985e-04\n",
      "4   1.5350  0.536881  6.214607e-03  1.940065e-03\n",
      "5   1.5375  0.785143  1.063555e-02  6.469603e-03\n",
      "6   1.5400  0.886749  1.868861e-02  1.096699e-02\n",
      "7   1.5425  0.882151  3.886312e-02  1.137066e-02\n",
      "8   1.5450  0.888211  4.058553e-02  1.062819e-02\n",
      "9   1.5475  0.898351  1.882241e-02  1.231554e-02\n",
      "10  1.5500  0.884318  1.285530e-02  2.202517e-02\n",
      "11  1.5525  0.891510  1.641046e-02  3.164553e-02\n",
      "12  1.5550  0.750956  2.978085e-02  8.950454e-03\n",
      "13  1.5575  0.311554  4.397421e-03  1.516899e-03\n",
      "14  1.5600  0.093184  2.851123e-04  2.572778e-04\n",
      "15  1.5625  0.033225  3.847958e-05  6.024352e-05\n",
      "16  1.5650  0.014730  9.810420e-06  1.877581e-05\n",
      "17  1.5675  0.007910  4.079919e-06  7.290227e-06\n",
      "18  1.5700  0.005029  2.359308e-06  3.380012e-06\n",
      "19  1.5725  0.003729  1.656281e-06  1.829884e-06\n",
      "20  1.5750  0.003201  1.292808e-06  1.161332e-06\n",
      "21  1.5775  0.003186  1.061193e-06  8.638020e-07\n",
      "22  1.5800  0.003736  8.840757e-07  8.006064e-07\n",
      "23  1.5825  0.004868  6.678824e-07  8.602698e-07\n",
      "24  1.5850  0.009585  4.383928e-07  1.716265e-06\n",
      "25  1.5875  0.045032  1.942413e-07  9.298206e-06\n",
      "26  1.5900  0.190316  1.750018e-05  7.138411e-05\n",
      "27  1.5925  0.009399  2.096493e-06  4.743000e-06\n",
      "28  1.5950  0.002505  1.572777e-06  1.970329e-06\n",
      "29  1.5975  0.001030  1.383101e-06  1.623976e-06\n",
      "30  1.6000  0.000513  1.639650e-06  2.830783e-06\n",
      "31  1.6025  0.000302  4.824041e-06  3.687513e-05\n",
      "32  1.6050  0.000174  1.069142e-06  5.351556e-08\n",
      "33  1.6075  0.000114  6.800334e-06  1.233166e-06\n",
      "34  1.6100  0.000081  3.778238e-07  5.756362e-08\n",
      "35  1.6125  0.000068  3.178216e-07  1.064093e-08\n",
      "36  1.6150  0.000070  3.811052e-07  1.364581e-08\n",
      "37  1.6175  0.000089  4.076630e-07  1.865470e-08\n",
      "38  1.6200  0.000143  4.278335e-07  2.225208e-08\n",
      "39  1.6225  0.000324  4.344700e-07  2.490008e-08\n",
      "40  1.6250  0.002578  4.196965e-07  4.331406e-08\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.086760  9.275143e-06  3.249131e-03\n",
      "1   1.5275  0.118812  1.240541e-05  2.613183e-03\n",
      "2   1.5300  0.183227  2.183270e-05  2.781244e-03\n",
      "3   1.5325  0.311414  4.718525e-05  3.799868e-03\n",
      "4   1.5350  0.536881  9.373105e-05  6.214607e-03\n",
      "5   1.5375  0.785143  1.233090e-04  1.063555e-02\n",
      "6   1.5400  0.886749  1.488600e-04  1.868861e-02\n",
      "7   1.5425  0.882151  1.721407e-04  3.886312e-02\n",
      "8   1.5450  0.888211  1.664701e-04  4.058553e-02\n",
      "9   1.5475  0.898351  1.511181e-04  1.882241e-02\n",
      "10  1.5500  0.884318  1.197090e-04  1.285530e-02\n",
      "11  1.5525  0.891510  5.678842e-05  1.641046e-02\n",
      "12  1.5550  0.750956  2.636283e-05  2.978085e-02\n",
      "13  1.5575  0.311554  8.209738e-06  4.397421e-03\n",
      "14  1.5600  0.093184  5.872857e-07  2.851123e-04\n",
      "15  1.5625  0.033225  1.921785e-07  3.847958e-05\n",
      "16  1.5650  0.014730  1.224903e-07  9.810420e-06\n",
      "17  1.5675  0.007910  9.910043e-08  4.079919e-06\n",
      "18  1.5700  0.005029  8.774090e-08  2.359308e-06\n",
      "19  1.5725  0.003729  8.086510e-08  1.656281e-06\n",
      "20  1.5750  0.003201  7.532593e-08  1.292808e-06\n",
      "21  1.5775  0.003186  7.206798e-08  1.061193e-06\n",
      "22  1.5800  0.003736  6.910069e-08  8.840757e-07\n",
      "23  1.5825  0.004868  6.383778e-08  6.678824e-07\n",
      "24  1.5850  0.009585  6.207522e-08  4.383928e-07\n",
      "25  1.5875  0.045032  6.199881e-08  1.942413e-07\n",
      "26  1.5900  0.190316  1.598424e-07  1.750018e-05\n",
      "27  1.5925  0.009399  3.989894e-08  2.096493e-06\n",
      "28  1.5950  0.002505  4.372294e-08  1.572777e-06\n",
      "29  1.5975  0.001030  4.314594e-08  1.383101e-06\n",
      "30  1.6000  0.000513  4.067785e-08  1.639650e-06\n",
      "31  1.6025  0.000302  7.136202e-08  4.824041e-06\n",
      "32  1.6050  0.000174  3.886280e-08  1.069142e-06\n",
      "33  1.6075  0.000114  1.053652e-07  6.800334e-06\n",
      "34  1.6100  0.000081  6.681929e-09  3.778238e-07\n",
      "35  1.6125  0.000068  2.825183e-08  3.178216e-07\n",
      "36  1.6150  0.000070  2.910617e-08  3.811052e-07\n",
      "37  1.6175  0.000089  2.790654e-08  4.076630e-07\n",
      "38  1.6200  0.000143  2.688429e-08  4.278335e-07\n",
      "39  1.6225  0.000324  2.600963e-08  4.344700e-07\n",
      "40  1.6250  0.002578  2.723457e-08  4.196965e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda        Output         Loss1         Loss2         Loss3  \\\n",
      "0   1.5250  6.267768e-04  7.008009e-05  7.357284e-03  6.423477e-04   \n",
      "1   1.5275  2.194808e-03  8.639115e-05  9.189811e-03  2.403074e-04   \n",
      "2   1.5300  1.671665e-02  2.634570e-04  1.545621e-02  7.445869e-05   \n",
      "3   1.5325  3.662386e-01  4.872822e-03  3.790922e-02  9.149724e-03   \n",
      "4   1.5350  6.132423e-01  8.438783e-03  1.631000e-02  1.562700e-02   \n",
      "5   1.5375  6.612227e-01  8.108768e-03  4.204107e-02  1.048023e-02   \n",
      "6   1.5400  4.991382e-01  4.970275e-03  5.351833e-02  5.230132e-03   \n",
      "7   1.5425  4.043050e-01  3.501639e-03  4.201422e-02  1.452211e-02   \n",
      "8   1.5450  6.379861e-01  6.492284e-03  2.736030e-02  3.639567e-02   \n",
      "9   1.5475  5.319590e-01  8.605414e-03  1.105764e-02  1.193523e-02   \n",
      "10  1.5500  5.689370e-01  1.504422e-02  4.147845e-02  3.621517e-03   \n",
      "11  1.5525  5.712135e-01  2.167229e-02  3.957852e-02  1.048726e-02   \n",
      "12  1.5550  4.536188e-01  2.354945e-02  3.139316e-03  3.126116e-03   \n",
      "13  1.5575  1.564698e-01  1.129499e-02  5.973522e-02  8.397369e-03   \n",
      "14  1.5600  5.181086e-04  3.851388e-05  7.266459e-04  2.377631e-04   \n",
      "15  1.5625  3.196805e-04  1.955239e-05  1.055047e-04  4.376495e-05   \n",
      "16  1.5650  2.018542e-04  1.175204e-05  4.070887e-05  1.368702e-05   \n",
      "17  1.5675  1.369221e-04  7.380850e-06  2.688887e-05  5.952853e-06   \n",
      "18  1.5700  1.010686e-04  4.843824e-06  2.182827e-05  3.265751e-06   \n",
      "19  1.5725  8.079313e-05  3.275742e-06  1.928429e-05  2.096339e-06   \n",
      "20  1.5750  6.984510e-05  2.268738e-06  1.794198e-05  1.480648e-06   \n",
      "21  1.5775  6.565906e-05  1.548661e-06  1.789426e-05  1.133546e-06   \n",
      "22  1.5800  6.865344e-05  1.056877e-06  1.941261e-05  9.361029e-07   \n",
      "23  1.5825  8.418182e-05  7.722900e-07  2.474754e-05  4.591283e-07   \n",
      "24  1.5850  1.404230e-04  1.305274e-06  4.355419e-05  1.226920e-06   \n",
      "25  1.5875  8.325868e-04  1.971355e-05  2.637415e-04  1.874987e-05   \n",
      "26  1.5900  5.833713e-04  5.002408e-05  4.763878e-04  6.293087e-05   \n",
      "27  1.5925  4.797351e-05  7.790284e-06  4.845242e-06  6.921793e-06   \n",
      "28  1.5950  8.670987e-06  3.267251e-06  4.751514e-07  3.573952e-06   \n",
      "29  1.5975  2.320437e-06  2.038487e-06  2.121258e-08  2.671734e-06   \n",
      "30  1.6000  9.018211e-07  1.460907e-06  1.640141e-08  2.827580e-06   \n",
      "31  1.6025  7.123654e-07  1.101834e-06  8.552133e-08  2.566945e-05   \n",
      "32  1.6050  1.304982e-06  7.304065e-07  9.891818e-07  3.667371e-06   \n",
      "33  1.6075  2.708918e-06  1.704552e-06  9.448553e-05  8.512909e-06   \n",
      "34  1.6100  4.893705e-07  1.086110e-06  1.090949e-06  1.125744e-06   \n",
      "35  1.6125  3.855736e-07  8.342046e-07  6.413904e-07  1.035044e-06   \n",
      "36  1.6150  4.342195e-07  7.247505e-07  5.881899e-07  9.609059e-07   \n",
      "37  1.6175  4.789411e-07  6.419506e-07  6.432722e-07  9.072635e-07   \n",
      "38  1.6200  5.099075e-07  5.890021e-07  7.972293e-07  8.664508e-07   \n",
      "39  1.6225  5.293102e-07  5.483521e-07  1.341902e-06  8.384885e-07   \n",
      "40  1.6250  4.711664e-07  5.245234e-07  8.839217e-06  8.902332e-07   \n",
      "\n",
      "           Loss4  \n",
      "0   1.905137e-05  \n",
      "1   3.727799e-05  \n",
      "2   1.899631e-04  \n",
      "3   3.558606e-03  \n",
      "4   6.862756e-03  \n",
      "5   1.181117e-02  \n",
      "6   1.716079e-02  \n",
      "7   2.058878e-02  \n",
      "8   3.306899e-02  \n",
      "9   2.946688e-02  \n",
      "10  4.324636e-02  \n",
      "11  3.234890e-02  \n",
      "12  6.780882e-03  \n",
      "13  5.574741e-04  \n",
      "14  4.383446e-06  \n",
      "15  2.709510e-06  \n",
      "16  1.636726e-06  \n",
      "17  1.102634e-06  \n",
      "18  8.090117e-07  \n",
      "19  6.315467e-07  \n",
      "20  5.108062e-07  \n",
      "21  4.327158e-07  \n",
      "22  3.728905e-07  \n",
      "23  3.496818e-07  \n",
      "24  3.110605e-07  \n",
      "25  2.159018e-07  \n",
      "26  8.670110e-08  \n",
      "27  2.263835e-07  \n",
      "28  2.211941e-07  \n",
      "29  2.411727e-07  \n",
      "30  2.186782e-07  \n",
      "31  2.224140e-07  \n",
      "32  1.814627e-07  \n",
      "33  2.510174e-07  \n",
      "34  1.837723e-07  \n",
      "35  1.804133e-07  \n",
      "36  1.777462e-07  \n",
      "37  1.766439e-07  \n",
      "38  1.762250e-07  \n",
      "39  1.759063e-07  \n",
      "40  1.762138e-07  \n",
      "     Lamda    Output         Loss1         Loss2         Loss3         Loss4\n",
      "0   1.5250  0.001605  6.794929e-05  6.368372e-03  5.098424e-04  4.478890e-05\n",
      "1   1.5275  0.006084  5.462846e-05  8.061127e-03  1.299085e-04  9.571646e-05\n",
      "2   1.5300  0.044850  1.907763e-04  1.405522e-02  2.919328e-04  4.948124e-04\n",
      "3   1.5325  0.505877  2.740684e-03  2.868444e-02  1.088974e-02  4.966965e-03\n",
      "4   1.5350  0.613312  4.064763e-03  1.613898e-02  1.580778e-02  7.062651e-03\n",
      "5   1.5375  0.648924  4.562870e-03  2.305432e-02  1.668321e-02  1.187267e-02\n",
      "6   1.5400  0.617783  4.448995e-03  3.888015e-02  1.190667e-02  2.133715e-02\n",
      "7   1.5425  0.378178  3.213661e-03  4.013772e-02  3.392436e-03  1.891579e-02\n",
      "8   1.5450  0.336012  4.819206e-03  4.600918e-02  5.772468e-03  1.712439e-02\n",
      "9   1.5475  0.498954  1.754610e-02  6.002495e-02  1.844188e-02  2.901292e-02\n",
      "10  1.5500  0.537853  4.502345e-02  3.361700e-02  1.778942e-02  4.658052e-02\n",
      "11  1.5525  0.666605  6.556754e-02  1.837077e-02  6.895402e-03  3.986703e-02\n",
      "12  1.5550  0.479291  3.394913e-02  5.636612e-02  6.626445e-03  5.910250e-03\n",
      "13  1.5575  0.103909  5.411499e-03  3.418050e-02  7.363765e-03  5.656349e-04\n",
      "14  1.5600  0.004178  1.901677e-04  2.055064e-03  5.687519e-04  2.610994e-05\n",
      "15  1.5625  0.000804  3.558676e-05  4.214374e-04  1.354228e-04  8.048342e-06\n",
      "16  1.5650  0.000291  1.277334e-05  1.393321e-04  5.071411e-05  4.258499e-06\n",
      "17  1.5675  0.000146  6.234654e-06  6.018397e-05  2.489244e-05  2.764013e-06\n",
      "18  1.5700  0.000088  3.590559e-06  3.078419e-05  1.473364e-05  1.990794e-06\n",
      "19  1.5725  0.000060  2.289635e-06  1.754601e-05  1.002254e-05  1.530587e-06\n",
      "20  1.5750  0.000044  1.577991e-06  1.066219e-05  7.618875e-06  1.215162e-06\n",
      "21  1.5775  0.000035  1.155576e-06  6.500516e-06  6.373403e-06  1.013268e-06\n",
      "22  1.5800  0.000031  9.218465e-07  3.602002e-06  5.891970e-06  8.553816e-07\n",
      "23  1.5825  0.000026  8.267707e-07  1.359174e-06  4.783188e-06  8.031140e-07\n",
      "24  1.5850  0.000047  1.199406e-06  1.310297e-06  6.298549e-06  7.055716e-07\n",
      "25  1.5875  0.002441  3.943934e-06  2.690915e-04  4.169637e-06  8.887361e-07\n",
      "26  1.5900  0.001037  3.368069e-06  4.678601e-05  1.288157e-04  9.837543e-07\n",
      "27  1.5925  0.000082  4.720564e-07  9.996821e-06  1.598907e-06  4.277291e-07\n",
      "28  1.5950  0.000026  1.507645e-07  3.636439e-06  1.100691e-06  3.295420e-07\n",
      "29  1.5975  0.000015  1.116723e-07  2.700047e-06  1.289562e-06  1.162530e-06\n",
      "30  1.6000  0.000011  9.523902e-08  2.120451e-06  2.210901e-06  1.002700e-06\n",
      "31  1.6025  0.000009  6.708302e-08  1.928210e-06  3.022873e-05  4.312668e-07\n",
      "32  1.6050  0.000007  1.012091e-07  1.402169e-06  2.467240e-06  4.475883e-07\n",
      "33  1.6075  0.000010  1.270081e-07  8.677083e-05  7.378579e-06  4.350741e-08\n",
      "34  1.6100  0.000006  8.396486e-08  2.262579e-06  8.328661e-07  4.737232e-07\n",
      "35  1.6125  0.000005  8.339516e-08  1.283660e-06  6.932043e-07  4.066839e-07\n",
      "36  1.6150  0.000005  8.764951e-08  8.799727e-07  6.386578e-07  3.803799e-07\n",
      "37  1.6175  0.000004  8.714050e-08  5.869244e-07  6.171809e-07  3.690567e-07\n",
      "38  1.6200  0.000004  9.127323e-08  3.222169e-07  6.051038e-07  3.575896e-07\n",
      "39  1.6225  0.000004  9.501011e-08  8.923770e-08  5.969434e-07  3.485429e-07\n",
      "40  1.6250  0.000004  9.839872e-08  3.449686e-06  6.518943e-07  3.421691e-07\n",
      "     Lamda        Output         Loss1         Loss2         Loss3     Loss4\n",
      "0   1.5250  7.759203e-03  2.810659e-04  4.741191e-03  2.052793e-04  0.000123\n",
      "1   1.5275  2.826181e-02  5.622016e-04  6.316141e-03  8.528190e-05  0.000105\n",
      "2   1.5300  1.660524e-01  2.159336e-03  1.197298e-02  2.323445e-03  0.000306\n",
      "3   1.5325  6.066207e-01  6.490719e-03  2.023057e-02  1.337747e-02  0.001364\n",
      "4   1.5350  6.566044e-01  7.807817e-03  1.909708e-02  1.683771e-02  0.002142\n",
      "5   1.5375  6.674774e-01  1.228861e-02  1.788222e-02  2.106594e-02  0.003288\n",
      "6   1.5400  6.939650e-01  2.415034e-02  1.562640e-02  2.841817e-02  0.006132\n",
      "7   1.5425  6.652879e-01  3.317946e-02  1.452066e-02  2.866401e-02  0.014200\n",
      "8   1.5450  5.036620e-01  2.253379e-02  1.521266e-02  1.375535e-02  0.015210\n",
      "9   1.5475  4.677292e-01  1.928910e-02  2.286189e-02  4.289599e-03  0.010845\n",
      "10  1.5500  5.569447e-01  2.886299e-02  4.042195e-02  3.193308e-03  0.014366\n",
      "11  1.5525  4.426872e-01  3.628320e-02  4.103359e-02  6.500068e-03  0.019238\n",
      "12  1.5550  5.685309e-01  2.037885e-02  5.841348e-02  1.630479e-02  0.069059\n",
      "13  1.5575  3.886485e-02  2.838426e-04  3.858155e-03  1.774559e-03  0.003526\n",
      "14  1.5600  3.399200e-03  8.477106e-06  2.589679e-04  2.165442e-04  0.000247\n",
      "15  1.5625  6.772637e-04  1.285634e-06  2.763442e-05  5.359824e-05  0.000070\n",
      "16  1.5650  2.288264e-04  4.819295e-07  1.918311e-06  1.976521e-05  0.000035\n",
      "17  1.5675  1.200526e-04  2.883161e-07  1.396611e-08  9.576153e-06  0.000022\n",
      "18  1.5700  9.366061e-05  2.412643e-07  2.454251e-06  5.657781e-06  0.000016\n",
      "19  1.5725  1.083816e-04  2.495996e-07  9.016904e-06  3.849876e-06  0.000013\n",
      "20  1.5750  2.166342e-04  2.929587e-07  3.594809e-05  2.852474e-06  0.000011\n",
      "21  1.5775  2.432595e-03  4.535662e-07  6.615210e-04  1.661263e-06  0.000013\n",
      "22  1.5800  5.415632e-04  2.611948e-07  2.119313e-04  4.610360e-06  0.000005\n",
      "23  1.5825  8.764130e-05  3.319275e-07  4.645623e-05  3.477777e-06  0.000005\n",
      "24  1.5850  3.786245e-05  4.029893e-07  2.224306e-05  4.332101e-06  0.000005\n",
      "25  1.5875  4.247689e-05  4.992652e-07  1.970771e-05  9.836742e-06  0.000005\n",
      "26  1.5900  5.241851e-05  4.054175e-07  1.467379e-04  1.482265e-04  0.000004\n",
      "27  1.5925  7.294957e-07  6.216111e-07  5.413896e-06  3.756552e-07  0.000004\n",
      "28  1.5950  1.452955e-08  7.878897e-07  4.117044e-06  7.668651e-08  0.000004\n",
      "29  1.5975  3.626202e-08  1.090040e-06  3.652292e-06  3.463830e-07  0.000004\n",
      "30  1.6000  5.373036e-08  1.608136e-06  3.485319e-06  1.007881e-06  0.000003\n",
      "31  1.6025  1.694230e-07  3.258354e-07  3.735383e-06  1.527787e-05  0.000005\n",
      "32  1.6050  9.605282e-07  4.392399e-06  5.556213e-06  4.362484e-07  0.000004\n",
      "33  1.6075  1.442985e-04  5.479760e-05  4.144045e-05  8.124414e-07  0.000002\n",
      "34  1.6100  2.231525e-06  1.456125e-05  3.567000e-07  5.542550e-08  0.000002\n",
      "35  1.6125  6.481971e-07  6.804862e-06  9.298168e-07  8.591558e-08  0.000002\n",
      "36  1.6150  9.614008e-05  5.431123e-06  1.049711e-05  1.530204e-06  0.000002\n",
      "37  1.6175  1.530454e-06  5.475666e-06  9.062261e-07  1.190492e-07  0.000002\n",
      "38  1.6200  1.030312e-06  5.360188e-06  1.048185e-06  1.445719e-07  0.000002\n",
      "39  1.6225  9.163802e-07  5.125624e-06  1.172317e-06  1.613283e-07  0.000002\n",
      "40  1.6250  1.010178e-06  4.682065e-06  1.428391e-06  1.761124e-07  0.000002\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.079030  4.089329e-06  4.467725e-05\n",
      "1   1.5275  0.104548  4.344036e-06  3.840563e-05\n",
      "2   1.5300  0.176480  6.723286e-06  3.788310e-05\n",
      "3   1.5325  0.364203  1.555693e-05  3.547363e-05\n",
      "4   1.5350  0.708654  4.476767e-05  1.250842e-05\n",
      "5   1.5375  0.915456  1.111844e-04  4.351438e-05\n",
      "6   1.5400  0.972803  1.735169e-04  2.238396e-04\n",
      "7   1.5425  0.993720  1.523336e-04  1.792418e-04\n",
      "8   1.5450  0.919701  1.341870e-04  6.666326e-05\n",
      "9   1.5475  0.921006  1.437431e-04  9.770000e-05\n",
      "10  1.5500  0.990718  1.421179e-04  2.336624e-04\n",
      "11  1.5525  0.728928  9.114173e-05  1.195117e-04\n",
      "12  1.5550  0.735387  9.578282e-05  5.831193e-05\n",
      "13  1.5575  0.421791  6.099936e-05  6.946262e-05\n",
      "14  1.5600  0.021972  1.302311e-07  1.162852e-06\n",
      "15  1.5625  0.003215  2.679791e-07  4.373285e-07\n",
      "16  1.5650  0.000808  2.027307e-07  1.134421e-07\n",
      "17  1.5675  0.000282  1.522274e-07  4.986034e-08\n",
      "18  1.5700  0.000124  1.201055e-07  4.011112e-08\n",
      "19  1.5725  0.000064  9.850094e-08  4.139308e-08\n",
      "20  1.5750  0.000037  8.441355e-08  4.331631e-08\n",
      "21  1.5775  0.000023  7.262082e-08  4.665907e-08\n",
      "22  1.5800  0.000016  6.266331e-08  4.768795e-08\n",
      "23  1.5825  0.000010  6.666931e-08  5.650010e-08\n",
      "24  1.5850  0.000008  6.169729e-08  5.818004e-08\n",
      "25  1.5875  0.000007  5.801841e-08  5.975966e-08\n",
      "26  1.5900  0.000009  5.375549e-08  5.764623e-08\n",
      "27  1.5925  0.000013  5.003153e-08  5.465299e-08\n",
      "28  1.5950  0.000025  4.652886e-08  4.985713e-08\n",
      "29  1.5975  0.000058  4.275466e-08  4.133296e-08\n",
      "30  1.6000  0.000187  3.910653e-08  2.519369e-08\n",
      "31  1.6025  0.001455  3.367379e-08  1.209107e-08\n",
      "32  1.6050  0.041388  1.416194e-07  6.580959e-09\n",
      "33  1.6075  0.070424  1.341015e-06  2.523723e-06\n",
      "34  1.6100  0.014606  7.304490e-07  2.171325e-07\n",
      "35  1.6125  0.000139  1.790203e-07  2.057392e-07\n",
      "36  1.6150  0.000060  1.306662e-07  1.923684e-07\n",
      "37  1.6175  0.000038  1.165594e-07  1.846515e-07\n",
      "38  1.6200  0.000029  1.102054e-07  1.767763e-07\n",
      "39  1.6225  0.000023  1.081766e-07  1.734211e-07\n",
      "40  1.6250  0.000019  1.088454e-07  1.738726e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.001691  5.408815e-04  3.213630e-03\n",
      "1   1.5275  0.003403  2.252752e-04  4.400668e-03\n",
      "2   1.5300  0.012617  7.455876e-06  9.245909e-03\n",
      "3   1.5325  0.154955  4.608904e-03  4.642866e-02\n",
      "4   1.5350  0.664325  3.988920e-02  3.896226e-02\n",
      "5   1.5375  0.842653  3.642013e-02  1.449024e-02\n",
      "6   1.5400  0.614232  7.385617e-03  3.864665e-02\n",
      "7   1.5425  0.617709  7.272574e-03  4.674179e-02\n",
      "8   1.5450  0.870990  2.447510e-02  3.771547e-02\n",
      "9   1.5475  0.959850  2.372102e-02  1.785169e-02\n",
      "10  1.5500  0.662230  8.030389e-03  2.120213e-02\n",
      "11  1.5525  0.921199  3.092431e-02  4.620466e-02\n",
      "12  1.5550  0.828882  4.400454e-02  2.500839e-02\n",
      "13  1.5575  0.455419  1.881693e-02  1.720995e-02\n",
      "14  1.5600  0.001033  1.593887e-03  6.961007e-04\n",
      "15  1.5625  0.000265  2.042644e-04  5.915334e-05\n",
      "16  1.5650  0.000168  5.718142e-05  1.418678e-05\n",
      "17  1.5675  0.000112  2.358230e-05  5.510658e-06\n",
      "18  1.5700  0.000080  1.248622e-05  2.760080e-06\n",
      "19  1.5725  0.000060  7.899860e-06  1.579207e-06\n",
      "20  1.5750  0.000048  5.732213e-06  9.488055e-07\n",
      "21  1.5775  0.000039  4.679222e-06  5.625864e-07\n",
      "22  1.5800  0.000032  4.317070e-06  2.820897e-07\n",
      "23  1.5825  0.000028  4.872743e-06  7.650059e-08\n",
      "24  1.5850  0.000024  6.976767e-06  3.269183e-09\n",
      "25  1.5875  0.000021  2.864441e-05  4.032823e-06\n",
      "26  1.5900  0.000018  2.708856e-05  2.608703e-05\n",
      "27  1.5925  0.000018  1.121585e-06  2.333378e-06\n",
      "28  1.5950  0.000017  4.177431e-07  1.190624e-06\n",
      "29  1.5975  0.000017  4.030539e-07  7.372328e-07\n",
      "30  1.6000  0.000018  5.331796e-07  4.338941e-07\n",
      "31  1.6025  0.000024  1.017740e-06  9.119234e-08\n",
      "32  1.6050  0.000104  2.344605e-05  1.666409e-05\n",
      "33  1.6075  0.000051  9.154693e-07  1.186269e-05\n",
      "34  1.6100  0.000028  1.948099e-06  1.219098e-05\n",
      "35  1.6125  0.000009  3.114765e-07  1.148367e-06\n",
      "36  1.6150  0.000007  2.286389e-07  6.143595e-07\n",
      "37  1.6175  0.000006  2.179007e-07  4.518979e-07\n",
      "38  1.6200  0.000006  2.275329e-07  3.670865e-07\n",
      "39  1.6225  0.000005  2.461583e-07  3.095064e-07\n",
      "40  1.6250  0.000005  3.871730e-07  2.811765e-07\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  3.373498e-03  1.016009e-03  7.437555e-03\n",
      "1   1.5275  5.924307e-03  4.689646e-04  9.366337e-03\n",
      "2   1.5300  1.815484e-02  5.396734e-05  1.570342e-02\n",
      "3   1.5325  1.323762e-01  2.580244e-03  4.202082e-02\n",
      "4   1.5350  8.028176e-01  4.012462e-02  4.335778e-02\n",
      "5   1.5375  7.264296e-01  3.178640e-02  1.213833e-02\n",
      "6   1.5400  7.496079e-01  1.126967e-02  4.568269e-02\n",
      "7   1.5425  5.561754e-01  8.651943e-03  4.109767e-02\n",
      "8   1.5450  8.310452e-01  2.651149e-02  3.502112e-02\n",
      "9   1.5475  9.607707e-01  1.863326e-02  1.693259e-02\n",
      "10  1.5500  9.463861e-01  5.880462e-03  2.881455e-02\n",
      "11  1.5525  7.339911e-01  8.608346e-03  3.761001e-02\n",
      "12  1.5550  8.387948e-01  1.189356e-02  3.043280e-02\n",
      "13  1.5575  3.889726e-01  3.155324e-03  1.748332e-02\n",
      "14  1.5600  5.020889e-03  4.734354e-04  1.325290e-03\n",
      "15  1.5625  4.966457e-04  8.728307e-05  1.293749e-04\n",
      "16  1.5650  1.825423e-04  2.908470e-05  2.110695e-05\n",
      "17  1.5675  9.733369e-05  1.337339e-05  3.442565e-06\n",
      "18  1.5700  6.111671e-05  7.724542e-06  2.454886e-07\n",
      "19  1.5725  4.232105e-05  5.307075e-06  5.866995e-08\n",
      "20  1.5750  3.135801e-05  4.133000e-06  6.587004e-07\n",
      "21  1.5775  2.454843e-05  3.669067e-06  1.737786e-06\n",
      "22  1.5800  2.016573e-05  3.639735e-06  3.669749e-06\n",
      "23  1.5825  1.781327e-05  3.777472e-06  8.004644e-06\n",
      "24  1.5850  1.715825e-05  6.329396e-06  2.374814e-05\n",
      "25  1.5875  2.675144e-05  3.775248e-05  2.199823e-04\n",
      "26  1.5900  4.481771e-07  5.015036e-05  6.902332e-04\n",
      "27  1.5925  5.268898e-06  6.449106e-06  2.739557e-05\n",
      "28  1.5950  5.689952e-06  3.527853e-06  1.036772e-05\n",
      "29  1.5975  5.885442e-06  3.135665e-06  5.725966e-06\n",
      "30  1.6000  6.446160e-06  4.267662e-06  3.937720e-06\n",
      "31  1.6025  9.167246e-06  3.822162e-05  3.414487e-06\n",
      "32  1.6050  9.762819e-05  8.210216e-07  4.579604e-06\n",
      "33  1.6075  1.808804e-04  7.949422e-06  6.918022e-05\n",
      "34  1.6100  3.158706e-05  1.016258e-06  1.135590e-07\n",
      "35  1.6125  4.405490e-06  8.282094e-07  3.028827e-07\n",
      "36  1.6150  3.052579e-06  8.036153e-07  4.154345e-07\n",
      "37  1.6175  2.592828e-06  7.985939e-07  5.333450e-07\n",
      "38  1.6200  2.332041e-06  8.128505e-07  7.562765e-07\n",
      "39  1.6225  2.150367e-06  8.356496e-07  1.490463e-06\n",
      "40  1.6250  1.993656e-06  1.027213e-06  1.010748e-05\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.008431  0.001278  1.122851e-03\n",
      "1   1.5275  0.013380  0.000515  1.233349e-03\n",
      "2   1.5300  0.031680  0.000069  1.537538e-03\n",
      "3   1.5325  0.129820  0.001616  2.213287e-03\n",
      "4   1.5350  0.730009  0.026842  1.964121e-03\n",
      "5   1.5375  0.748307  0.026763  6.139760e-04\n",
      "6   1.5400  0.695714  0.007528  2.057337e-03\n",
      "7   1.5425  0.701404  0.006128  2.434310e-03\n",
      "8   1.5450  0.816212  0.015283  1.566192e-03\n",
      "9   1.5475  0.983766  0.018402  7.456848e-04\n",
      "10  1.5500  0.694692  0.010808  9.473715e-04\n",
      "11  1.5525  0.804977  0.040500  1.807564e-03\n",
      "12  1.5550  0.637358  0.012979  8.836702e-04\n",
      "13  1.5575  0.536553  0.002228  8.264159e-04\n",
      "14  1.5600  0.009374  0.000461  1.559087e-04\n",
      "15  1.5625  0.000374  0.000105  2.656041e-05\n",
      "16  1.5650  0.000069  0.000038  5.161793e-06\n",
      "17  1.5675  0.000039  0.000019  2.151986e-08\n",
      "18  1.5700  0.000029  0.000011  1.086560e-06\n",
      "19  1.5725  0.000023  0.000008  9.371378e-07\n",
      "20  1.5750  0.000017  0.000006  2.767798e-07\n",
      "21  1.5775  0.000014  0.000005  8.007039e-07\n",
      "22  1.5800  0.000011  0.000004  2.602308e-06\n",
      "23  1.5825  0.000012  0.000004  7.789268e-06\n",
      "24  1.5850  0.000014  0.000004  2.044729e-05\n",
      "25  1.5875  0.000065  0.000017  1.302440e-04\n",
      "26  1.5900  0.000148  0.000045  1.997349e-04\n",
      "27  1.5925  0.000022  0.000009  2.648492e-05\n",
      "28  1.5950  0.000011  0.000005  6.828762e-06\n",
      "29  1.5975  0.000007  0.000004  2.568388e-06\n",
      "30  1.6000  0.000004  0.000006  9.916770e-07\n",
      "31  1.6025  0.000002  0.000039  4.005337e-07\n",
      "32  1.6050  0.000037  0.000002  1.992431e-07\n",
      "33  1.6075  0.000033  0.000002  4.053157e-07\n",
      "34  1.6100  0.000021  0.000002  6.809498e-07\n",
      "35  1.6125  0.000004  0.000002  9.363440e-07\n",
      "36  1.6150  0.000003  0.000002  1.161989e-06\n",
      "37  1.6175  0.000003  0.000002  1.287456e-06\n",
      "38  1.6200  0.000003  0.000002  9.134474e-07\n",
      "39  1.6225  0.000003  0.000002  2.417512e-06\n",
      "40  1.6250  0.000003  0.000002  7.783026e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda        Output         Loss1         Loss2\n",
      "0   1.5250  2.286867e-02  1.114650e-03  1.955629e-04\n",
      "1   1.5275  3.226604e-02  9.879813e-04  1.726094e-04\n",
      "2   1.5300  6.221049e-02  1.159140e-03  2.744983e-04\n",
      "3   1.5325  1.803953e-01  2.031399e-03  8.626380e-04\n",
      "4   1.5350  7.071202e-01  5.221940e-03  5.155206e-03\n",
      "5   1.5375  7.895442e-01  4.974320e-03  1.471883e-02\n",
      "6   1.5400  5.807383e-01  4.632635e-03  4.780651e-02\n",
      "7   1.5425  6.854076e-01  6.209957e-03  2.967437e-02\n",
      "8   1.5450  8.493646e-01  1.000676e-02  1.807196e-02\n",
      "9   1.5475  7.819405e-01  1.226403e-02  1.913873e-02\n",
      "10  1.5500  8.133177e-01  1.628424e-02  4.102091e-02\n",
      "11  1.5525  1.049766e+00  2.469697e-02  2.414540e-02\n",
      "12  1.5550  1.034718e+00  2.936977e-02  1.563460e-02\n",
      "13  1.5575  5.102365e-01  1.681070e-02  9.566493e-03\n",
      "14  1.5600  3.932440e-02  1.467974e-03  1.458365e-05\n",
      "15  1.5625  4.982770e-03  2.062254e-04  2.075656e-05\n",
      "16  1.5650  1.080251e-03  4.769123e-05  1.211588e-05\n",
      "17  1.5675  3.376336e-04  1.519872e-05  7.646851e-06\n",
      "18  1.5700  1.401553e-04  6.109212e-06  5.290414e-06\n",
      "19  1.5725  7.379755e-05  2.956756e-06  3.921335e-06\n",
      "20  1.5750  4.736805e-05  1.691170e-06  3.056196e-06\n",
      "21  1.5775  3.602384e-05  1.116716e-06  2.478770e-06\n",
      "22  1.5800  3.116414e-05  8.656713e-07  2.071127e-06\n",
      "23  1.5825  2.382782e-05  4.215019e-07  1.949279e-06\n",
      "24  1.5850  2.510362e-05  4.663490e-07  1.720712e-06\n",
      "25  1.5875  2.970695e-05  9.988507e-07  1.552748e-06\n",
      "26  1.5900  1.455822e-05  1.040514e-04  1.421558e-06\n",
      "27  1.5925  2.878681e-05  3.186492e-07  1.309450e-06\n",
      "28  1.5950  3.974528e-05  7.802933e-08  1.243662e-06\n",
      "29  1.5975  5.476537e-05  5.130529e-08  1.216397e-06\n",
      "30  1.6000  8.148102e-05  8.143136e-08  1.262272e-06\n",
      "31  1.6025  1.603850e-04  2.487003e-07  1.288195e-06\n",
      "32  1.6050  1.546897e-02  1.808012e-05  4.622986e-06\n",
      "33  1.6075  8.417055e-03  2.050644e-06  1.126086e-06\n",
      "34  1.6100  1.023432e-02  6.326835e-06  3.843505e-07\n",
      "35  1.6125  5.257267e-04  6.855871e-07  6.327450e-07\n",
      "36  1.6150  1.225469e-04  2.708281e-07  6.262556e-07\n",
      "37  1.6175  5.161394e-05  1.386781e-07  6.421424e-07\n",
      "38  1.6200  3.950068e-05  8.756575e-08  6.419658e-07\n",
      "39  1.6225  3.553145e-05  7.564508e-08  6.444975e-07\n",
      "40  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "41  1.6250  6.510834e-07  7.172027e-08  2.988830e-05\n",
      "42  1.6250  2.988830e-05  7.172027e-08  6.510834e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.057202  1.834650e-03  5.367757e-04\n",
      "1   1.5275  0.076104  1.493815e-03  4.393677e-04\n",
      "2   1.5300  0.122175  1.510919e-03  5.679304e-04\n",
      "3   1.5325  0.240158  1.968313e-03  1.195144e-03\n",
      "4   1.5350  0.535216  3.334484e-03  4.050699e-03\n",
      "5   1.5375  0.843439  5.434988e-03  1.615736e-02\n",
      "6   1.5400  0.721371  7.015227e-03  5.930535e-02\n",
      "7   1.5425  0.645359  7.545665e-03  2.691568e-02\n",
      "8   1.5450  0.712574  9.374232e-03  1.470843e-02\n",
      "9   1.5475  0.900660  1.128575e-02  2.244013e-02\n",
      "10  1.5500  0.978799  1.033380e-02  5.164029e-02\n",
      "11  1.5525  0.935382  7.698892e-03  2.104217e-02\n",
      "12  1.5550  0.925658  6.343058e-03  1.248969e-02\n",
      "13  1.5575  0.467722  2.722452e-03  9.823797e-03\n",
      "14  1.5600  0.104297  5.334548e-04  1.715612e-05\n",
      "15  1.5625  0.026151  1.238945e-04  3.519627e-06\n",
      "16  1.5650  0.008739  3.980877e-05  4.605827e-06\n",
      "17  1.5675  0.003639  1.655128e-05  3.647462e-06\n",
      "18  1.5700  0.001793  8.434052e-06  2.776089e-06\n",
      "19  1.5725  0.001011  5.073891e-06  2.153612e-06\n",
      "20  1.5750  0.000640  3.530141e-06  1.720077e-06\n",
      "21  1.5775  0.000448  2.765925e-06  1.400793e-06\n",
      "22  1.5800  0.000343  2.457175e-06  1.166160e-06\n",
      "23  1.5825  0.000276  1.677882e-06  9.560747e-07\n",
      "24  1.5850  0.000247  1.911641e-06  8.100803e-07\n",
      "25  1.5875  0.000254  3.464333e-06  6.858765e-07\n",
      "26  1.5900  0.001507  1.721520e-04  7.340374e-07\n",
      "27  1.5925  0.000194  4.406502e-07  5.148542e-07\n",
      "28  1.5950  0.000224  4.731719e-07  4.227821e-07\n",
      "29  1.5975  0.000272  8.195675e-07  3.274759e-07\n",
      "30  1.6000  0.000363  1.935017e-06  1.955832e-07\n",
      "31  1.6025  0.000594  2.905784e-05  4.168477e-07\n",
      "32  1.6050  0.001583  1.500931e-06  6.794728e-07\n",
      "33  1.6075  0.058319  7.197378e-06  8.969204e-06\n",
      "34  1.6100  0.000374  9.663898e-08  1.409848e-06\n",
      "35  1.6125  0.000218  2.997168e-08  7.620756e-07\n",
      "36  1.6150  0.000289  2.846708e-08  6.263328e-07\n",
      "37  1.6175  0.000344  2.826030e-08  5.478388e-07\n",
      "38  1.6200  0.000303  2.501696e-08  5.013889e-07\n",
      "39  1.6225  0.000199  2.256476e-08  4.655271e-07\n",
      "40  1.6250  0.000107  2.379448e-08  4.376882e-07\n",
      "     Lamda    Output         Loss1         Loss2\n",
      "0   1.5250  0.142469  2.301422e-03  1.362186e-03\n",
      "1   1.5275  0.172150  1.647296e-03  1.021327e-03\n",
      "2   1.5300  0.225608  1.442682e-03  1.077888e-03\n",
      "3   1.5325  0.316388  1.545829e-03  1.588820e-03\n",
      "4   1.5350  0.460132  2.071984e-03  3.402076e-03\n",
      "5   1.5375  0.642856  3.381322e-03  1.164004e-02\n",
      "6   1.5400  0.755083  5.321155e-03  5.751546e-02\n",
      "7   1.5425  0.791614  5.353910e-03  3.051205e-02\n",
      "8   1.5450  0.766149  5.996810e-03  1.472381e-02\n",
      "9   1.5475  0.756065  8.993469e-03  1.760796e-02\n",
      "10  1.5500  0.789694  2.005260e-02  3.845266e-02\n",
      "11  1.5525  0.893329  3.030534e-02  1.784819e-02\n",
      "12  1.5550  0.937813  9.047394e-03  1.055586e-02\n",
      "13  1.5575  0.592988  1.850023e-03  1.086546e-02\n",
      "14  1.5600  0.287633  3.960383e-04  1.343680e-04\n",
      "15  1.5625  0.135081  1.023397e-04  3.347257e-06\n",
      "16  1.5650  0.069936  3.368472e-05  2.440251e-07\n",
      "17  1.5675  0.039965  1.368135e-05  1.659897e-07\n",
      "18  1.5700  0.024843  6.609768e-06  1.265229e-07\n",
      "19  1.5725  0.016578  3.683033e-06  8.915509e-08\n",
      "20  1.5750  0.011755  2.323734e-06  6.222894e-08\n",
      "21  1.5775  0.008806  1.611364e-06  5.186687e-08\n",
      "22  1.5800  0.006946  1.230864e-06  4.821318e-08\n",
      "23  1.5825  0.005933  6.028718e-07  7.111140e-08\n",
      "24  1.5850  0.005354  5.676107e-07  7.571512e-08\n",
      "25  1.5875  0.005727  1.298362e-06  9.163177e-08\n",
      "26  1.5900  0.024542  1.276873e-04  8.989275e-08\n",
      "27  1.5925  0.001497  3.681371e-06  4.799868e-08\n",
      "28  1.5950  0.002102  1.633422e-06  7.928544e-08\n",
      "29  1.5975  0.002352  1.379305e-06  1.009703e-07\n",
      "30  1.6000  0.002592  2.390487e-06  1.726209e-07\n",
      "31  1.6025  0.002982  3.523851e-05  2.347466e-07\n",
      "32  1.6050  0.003484  4.646649e-07  6.536153e-07\n",
      "33  1.6075  0.004365  1.479398e-06  7.241156e-07\n",
      "34  1.6100  0.005749  1.025618e-06  1.731486e-07\n",
      "35  1.6125  0.007834  5.954243e-07  7.712207e-09\n",
      "36  1.6150  0.010611  4.797081e-07  3.552143e-08\n",
      "37  1.6175  0.012906  3.834395e-07  4.389222e-08\n",
      "38  1.6200  0.012690  3.128823e-07  5.129908e-08\n",
      "39  1.6225  0.010207  2.597109e-07  5.873255e-08\n",
      "40  1.6250  0.007530  2.312329e-07  6.487464e-08\n",
      "     Lamda    Output     Loss1         Loss2\n",
      "0   1.5250  0.294852  0.002311  2.635875e-03\n",
      "1   1.5275  0.309081  0.001490  1.795729e-03\n",
      "2   1.5300  0.334105  0.001282  1.623990e-03\n",
      "3   1.5325  0.369770  0.001444  1.996089e-03\n",
      "4   1.5350  0.417315  0.001999  3.634116e-03\n",
      "5   1.5375  0.478206  0.002728  1.177032e-02\n",
      "6   1.5400  0.545807  0.003619  4.206769e-02\n",
      "7   1.5425  0.645566  0.005418  2.027403e-02\n",
      "8   1.5450  0.753076  0.012537  1.662217e-02\n",
      "9   1.5475  0.858329  0.013565  3.596371e-02\n",
      "10  1.5500  0.947207  0.007951  4.717223e-02\n",
      "11  1.5525  1.035638  0.009383  1.425992e-02\n",
      "12  1.5550  0.981476  0.039715  1.605307e-02\n",
      "13  1.5575  0.938121  0.002418  5.003545e-03\n",
      "14  1.5600  0.790970  0.000272  2.867008e-04\n",
      "15  1.5625  0.632660  0.000056  4.374967e-05\n",
      "16  1.5650  0.492222  0.000016  1.067989e-05\n",
      "17  1.5675  0.377301  0.000006  3.440465e-06\n",
      "18  1.5700  0.286430  0.000003  1.312012e-06\n",
      "19  1.5725  0.215189  0.000002  5.399587e-07\n",
      "20  1.5750  0.159029  0.000002  2.009906e-07\n",
      "21  1.5775  0.114237  0.000002  4.520259e-08\n",
      "22  1.5800  0.078122  0.000002  4.338622e-08\n",
      "23  1.5825  0.050120  0.000003  7.267866e-08\n",
      "24  1.5850  0.026873  0.000003  1.029577e-07\n",
      "25  1.5875  0.010089  0.000004  1.191867e-07\n",
      "26  1.5900  0.001381  0.000005  1.264474e-07\n",
      "27  1.5925  0.004232  0.000007  1.133919e-07\n",
      "28  1.5950  0.024663  0.000011  4.305505e-08\n",
      "29  1.5975  0.070906  0.000023  1.041764e-07\n",
      "30  1.6000  0.152485  0.000088  6.961811e-07\n",
      "31  1.6025  0.276925  0.000783  5.479279e-06\n",
      "32  1.6050  0.439360  0.000505  1.410392e-05\n",
      "33  1.6075  0.606305  0.000225  4.841430e-06\n",
      "34  1.6100  0.749445  0.000144  1.865210e-07\n",
      "35  1.6125  0.855375  0.000017  2.176731e-08\n",
      "36  1.6150  0.921200  0.000010  1.161037e-07\n",
      "37  1.6175  0.956710  0.000007  1.541489e-07\n",
      "38  1.6200  0.972260  0.000005  1.800037e-07\n",
      "39  1.6225  0.975305  0.000004  1.985664e-07\n",
      "40  1.6250  0.970082  0.000004  2.117713e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'I1_To_J1': -0.009325105654083286,\n",
       " 'I1_To_J2': -0.5852892645520461,\n",
       " 'I1_To_J3': -0.6201703160216055,\n",
       " 'I1_To_J4': -0.5639856406243621,\n",
       " 'I1_To_K1': -0.20663343349373478,\n",
       " 'I1_To_K2': -0.02142939581066442,\n",
       " 'I1_To_K3': -0.23610927006756355,\n",
       " 'I1_To_K4': -0.05423742137404733,\n",
       " 'I1_To_O1': -0.4121422363863366,\n",
       " 'I1_To_O2': -0.05510460521420364,\n",
       " 'I1_To_O3': -0.36428730523760855,\n",
       " 'I1_To_O4': -0.20663343349373478,\n",
       " 'I2_To_J1': -0.5852892645520461,\n",
       " 'I2_To_J2': -0.009325105654083286,\n",
       " 'I2_To_J3': -0.5852892645520461,\n",
       " 'I2_To_J4': -0.6201703160216055,\n",
       " 'I2_To_K1': -0.36428730523760855,\n",
       " 'I2_To_K2': -0.12293905729779019,\n",
       " 'I2_To_K3': -0.12293905729779019,\n",
       " 'I2_To_K4': -0.23610927006756355,\n",
       " 'I2_To_O1': -0.05510460521420364,\n",
       " 'I2_To_O2': -0.08858894988546631,\n",
       " 'I2_To_O3': -0.36428730523760855,\n",
       " 'I2_To_O4': -0.02142939581066442,\n",
       " 'I3_To_J1': -0.6201703160216055,\n",
       " 'I3_To_J2': -0.5852892645520461,\n",
       " 'I3_To_J3': -0.009325105654083286,\n",
       " 'I3_To_J4': -0.5852892645520461,\n",
       " 'I3_To_K1': -0.05510460521420364,\n",
       " 'I3_To_K2': -0.08858894988546631,\n",
       " 'I3_To_K3': -0.36428730523760855,\n",
       " 'I3_To_K4': -0.02142939581066442,\n",
       " 'I3_To_O1': -0.36428730523760855,\n",
       " 'I3_To_O2': -0.12293905729779019,\n",
       " 'I3_To_O3': -0.12293905729779019,\n",
       " 'I3_To_O4': -0.23610927006756355,\n",
       " 'I4_To_J1': -0.5639856406243621,\n",
       " 'I4_To_J2': -0.6201703160216055,\n",
       " 'I4_To_J3': -0.5852892645520461,\n",
       " 'I4_To_J4': -0.009325105654083286,\n",
       " 'I4_To_K1': -0.4121422363863366,\n",
       " 'I4_To_K2': -0.05510460521420364,\n",
       " 'I4_To_K3': -0.36428730523760855,\n",
       " 'I4_To_K4': -0.20663343349373478,\n",
       " 'I4_To_O1': -0.20663343349373478,\n",
       " 'I4_To_O2': -0.02142939581066442,\n",
       " 'I4_To_O3': -0.23610927006756355,\n",
       " 'I4_To_O4': -0.05423742137404733}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = {}\n",
    "for file in source_data :\n",
    "  data = pd.read_excel(file,header=0)\n",
    "  print(data)\n",
    "  row = data.loc[data['Lamda']==1.5500].values[0][1:]\n",
    "  file_name = file.split('.')[0]\n",
    "  output_loss = row[0]\n",
    "  df[file_name] = np.log(output_loss)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 114505,
     "status": "ok",
     "timestamp": 1593147104944,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "5iYmR0Anxm0w",
    "outputId": "a0266052-6466-405a-c2b7-c3d5468c0ba4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"K1_To_O1\"]=df[\"O4_To_K4\"]=df[\"J4_To_I4\"]=df[\"I1_To_J1\"]\n",
    "df[\"K1_To_O2\"]=df[\"O4_To_K3\"]=df[\"J4_To_I3\"]=df[\"I1_To_J2\"]\n",
    "df[\"K1_To_O3\"]=df[\"O4_To_K2\"]=df[\"J4_To_I2\"]=df[\"I1_To_J3\"]\n",
    "df[\"K1_To_O4\"]=df[\"O4_To_K1\"]=df[\"J4_To_I1\"]=df[\"I1_To_J4\"]\n",
    "df[\"K1_To_J4\"]=df[\"O4_To_I1\"]=df[\"J4_To_O4\"]=df[\"I1_To_K1\"]\n",
    "df[\"K1_To_J3\"]=df[\"O4_To_I2\"]=df[\"J4_To_O3\"]=df[\"I1_To_K2\"]\n",
    "df[\"K1_To_J2\"]=df[\"O4_To_I3\"]=df[\"J4_To_O2\"]=df[\"I1_To_K3\"]\n",
    "df[\"K1_To_J1\"]=df[\"O4_To_I4\"]=df[\"J4_To_O1\"]=df[\"I1_To_K4\"]\n",
    "df[\"K1_To_I4\"]=df[\"O4_To_J1\"]=df[\"J4_To_K4\"]=df[\"I1_To_O1\"]\n",
    "df[\"K1_To_I3\"]=df[\"O4_To_J2\"]=df[\"J4_To_K3\"]=df[\"I1_To_O2\"]\n",
    "df[\"K1_To_I2\"]=df[\"O4_To_J3\"]=df[\"J4_To_K2\"]=df[\"I1_To_O3\"]\n",
    "df[\"K1_To_I1\"]=df[\"O4_To_J4\"]=df[\"J4_To_K1\"]=df[\"I1_To_O4\"]\n",
    "###########################################################\n",
    "df[\"K2_To_O1\"]=df[\"O3_To_K4\"]=df[\"J3_To_I4\"]=df[\"I2_To_J1\"]\n",
    "df[\"K2_To_O2\"]=df[\"O3_To_K3\"]=df[\"J3_To_I3\"]=df[\"I2_To_J2\"]\n",
    "df[\"K2_To_O3\"]=df[\"O3_To_K2\"]=df[\"J3_To_I2\"]=df[\"I2_To_J3\"]\n",
    "df[\"K2_To_O4\"]=df[\"O3_To_K1\"]=df[\"J3_To_I1\"]=df[\"I2_To_J4\"]\n",
    "df[\"K2_To_J4\"]=df[\"O3_To_I1\"]=df[\"J3_To_O4\"]=df[\"I2_To_K1\"]\n",
    "df[\"K2_To_J3\"]=df[\"O3_To_I2\"]=df[\"J3_To_O3\"]=df[\"I2_To_K2\"]\n",
    "df[\"K2_To_J2\"]=df[\"O3_To_I3\"]=df[\"J3_To_O2\"]=df[\"I2_To_K3\"]\n",
    "df[\"K2_To_J1\"]=df[\"O3_To_I4\"]=df[\"J3_To_O1\"]=df[\"I2_To_K4\"]\n",
    "df[\"K2_To_I4\"]=df[\"O3_To_J1\"]=df[\"J3_To_K4\"]=df[\"I2_To_O1\"]\n",
    "df[\"K2_To_I3\"]=df[\"O3_To_J2\"]=df[\"J3_To_K3\"]=df[\"I2_To_O2\"]\n",
    "df[\"K2_To_I2\"]=df[\"O3_To_J3\"]=df[\"J3_To_K2\"]=df[\"I2_To_O3\"]\n",
    "df[\"K2_To_I1\"]=df[\"O3_To_J4\"]=df[\"J3_To_K1\"]=df[\"I2_To_O4\"]\n",
    "###########################################################\n",
    "df[\"K3_To_O1\"]=df[\"O2_To_K4\"]=df[\"J2_To_I4\"]=df[\"I3_To_J1\"]\n",
    "df[\"K3_To_O2\"]=df[\"O2_To_K3\"]=df[\"J2_To_I3\"]=df[\"I3_To_J2\"]\n",
    "df[\"K3_To_O3\"]=df[\"O2_To_K2\"]=df[\"J2_To_I2\"]=df[\"I3_To_J3\"]\n",
    "df[\"K3_To_O4\"]=df[\"O2_To_K1\"]=df[\"J2_To_I1\"]=df[\"I3_To_J4\"]\n",
    "df[\"K3_To_J1\"]=df[\"O2_To_I1\"]=df[\"J2_To_O4\"]=df[\"I3_To_K1\"]\n",
    "df[\"K3_To_J2\"]=df[\"O2_To_I2\"]=df[\"J2_To_O3\"]=df[\"I3_To_K2\"]\n",
    "df[\"K3_To_J3\"]=df[\"O2_To_I3\"]=df[\"J2_To_O2\"]=df[\"I3_To_K3\"]\n",
    "df[\"K3_To_J4\"]=df[\"O2_To_I4\"]=df[\"J2_To_O1\"]=df[\"I3_To_K4\"]\n",
    "df[\"K3_To_I1\"]=df[\"O2_To_J1\"]=df[\"J2_To_K4\"]=df[\"I3_To_O1\"]\n",
    "df[\"K3_To_I2\"]=df[\"O2_To_J2\"]=df[\"J2_To_K3\"]=df[\"I3_To_O2\"]\n",
    "df[\"K3_To_I3\"]=df[\"O2_To_J3\"]=df[\"J2_To_K2\"]=df[\"I3_To_O3\"]\n",
    "df[\"K3_To_I4\"]=df[\"O2_To_J4\"]=df[\"J2_To_K1\"]=df[\"I3_To_O4\"]\n",
    "###########################################################\n",
    "df[\"K4_To_O1\"]=df[\"O1_To_K4\"]=df[\"J1_To_I4\"]=df[\"I4_To_J1\"]\n",
    "df[\"K4_To_O2\"]=df[\"O1_To_K3\"]=df[\"J1_To_I3\"]=df[\"I4_To_J2\"]\n",
    "df[\"K4_To_O3\"]=df[\"O1_To_K2\"]=df[\"J1_To_I2\"]=df[\"I4_To_J3\"]\n",
    "df[\"K4_To_O4\"]=df[\"O1_To_K1\"]=df[\"J1_To_I1\"]=df[\"I4_To_J4\"]\n",
    "df[\"K4_To_J1\"]=df[\"O1_To_I1\"]=df[\"J1_To_O4\"]=df[\"I4_To_K1\"]\n",
    "df[\"K4_To_J2\"]=df[\"O1_To_I2\"]=df[\"J1_To_O3\"]=df[\"I4_To_K2\"]\n",
    "df[\"K4_To_J3\"]=df[\"O1_To_I3\"]=df[\"J1_To_O2\"]=df[\"I4_To_K3\"]\n",
    "df[\"K4_To_J4\"]=df[\"O1_To_I4\"]=df[\"J1_To_O1\"]=df[\"I4_To_K4\"]\n",
    "df[\"K4_To_I1\"]=df[\"O1_To_J1\"]=df[\"J1_To_K4\"]=df[\"I4_To_O1\"]\n",
    "df[\"K4_To_I2\"]=df[\"O1_To_J2\"]=df[\"J1_To_K3\"]=df[\"I4_To_O2\"]\n",
    "df[\"K4_To_I3\"]=df[\"O1_To_J3\"]=df[\"J1_To_K2\"]=df[\"I4_To_O3\"]\n",
    "df[\"K4_To_I4\"]=df[\"O1_To_J4\"]=df[\"J1_To_K1\"]=df[\"I4_To_O4\"]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 114494,
     "status": "ok",
     "timestamp": 1593147104944,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "S46aKVGw-1b8",
    "outputId": "375e1a25-7796-468b-fd9d-a33982d439b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05423742137404733"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_node_dict = {}\n",
    "action_node_dict[LEFT1]='K1'\n",
    "action_node_dict[LEFT2]='K2'\n",
    "action_node_dict[LEFT3]='K3'\n",
    "action_node_dict[LEFT4]='K4'\n",
    "action_node_dict[UP1]='J1'\n",
    "action_node_dict[UP2]='J2'\n",
    "action_node_dict[UP3]='J3'\n",
    "action_node_dict[UP4]='J4'\n",
    "action_node_dict[RIGHT1]='O1'\n",
    "action_node_dict[RIGHT2]='O2'\n",
    "action_node_dict[RIGHT3]='O3'\n",
    "action_node_dict[RIGHT4]='O4'\n",
    "action_node_dict[DOWN1]='I1'\n",
    "action_node_dict[DOWN2]='I2'\n",
    "action_node_dict[DOWN3]='I3'\n",
    "action_node_dict[DOWN4]='I4'\n",
    "df[\"{}_To_{}\".format(action_node_dict[DOWN1],action_node_dict[LEFT4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 117610,
     "status": "ok",
     "timestamp": 1593147108063,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "DQzW46P1Ewo7"
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,1)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1,ncols-2) # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        self.current_distance = None\n",
    "        self.centroid = []\n",
    "        self.supervised = 0\n",
    "        self.old_action = 1\n",
    "        self.current_action = None\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "    def get_list_centroid(self):\n",
    "      dx=[0,3,0,-2,1,3,1,-2,-1,3,-1,-2,2,3,2,-2]\n",
    "      dy=[3,0,-2,0,3,1,-2,1,3,-1,-2,-1,3,2,-2,2]\n",
    "      for i in range (self._maze.shape[0]):\n",
    "        for j in range (self._maze.shape[1]):\n",
    "          if (self._maze[i][j]==0.95) :\n",
    "            self.centroid.append((i,j))\n",
    "            for temp in range(16):\n",
    "              tx = i + dx[temp]\n",
    "              ty = j + dy[temp]\n",
    "              state_centroid_dict[(tx,ty)] = (i,j)\n",
    "    def available_cell_for_agent(self):\n",
    "        list_available_cell_agent = []\n",
    "        nrows, ncols = self.maze.shape\n",
    "        for row in range(nrows) :\n",
    "            for col in range(ncols) :\n",
    "              if row == 1 or row == 2 or row == 3 or row ==6 or row == 7 or row ==8 or row == 11 or row == 12 or row == 13 :\n",
    "                if col == 0 or col == 14 :\n",
    "                      list_available_cell_agent.append((row,col))\n",
    "              if col== 1 or col == 2 or col == 3 or col ==6 or col == 7 or col ==8 or col == 11 or col == 12 or col == 13 :\n",
    "                if row == 0 or row == 14 :\n",
    "                      list_available_cell_agent.append((row,col))\n",
    "        return list_available_cell_agent\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'valid')\n",
    "        self.min_reward = -256\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "        target_row,target_col = self.target\n",
    "        self.old_distance = -999\n",
    "        self.list_previous_state=[]\n",
    "        return self.observe().reshape(1,maze.size)\n",
    "    def check_state_centroid(self):\n",
    "      nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "        if self.maze[rat_row, rat_col] > 0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "        x_centroid,y_centroid = state_centroid_dict[(nrow, ncol)]\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            dx=[0,3,0,-2,1,3,1,-2,-1,3,-1,-2,2,3,2,-2]\n",
    "            dy=[3,0,-2,0,3,1,-2,1,3,-1,-2,-1,3,2,-2,2]\n",
    "            #r2,d2,l2,u2,r3,d3,l3,u3,r1,d1,l1,u1,r4,d4,l4,u4\n",
    "            if action == LEFT1:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[10]\n",
    "                y_centroid +=(dy[10]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[10]\n",
    "                y_centroid +=dy[10]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == LEFT2:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[2]\n",
    "                y_centroid +=(dy[2]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[2]\n",
    "                y_centroid +=dy[2]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == LEFT3:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[6]\n",
    "                y_centroid +=(dy[6]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[6]\n",
    "                y_centroid +=dy[6]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == LEFT4:\n",
    "              if self._maze[nrow][ncol-1]== 1 and self._maze[nrow][ncol+1]== 0.9:\n",
    "                x_centroid +=dx[14]\n",
    "                y_centroid +=(dy[14]-1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[14]\n",
    "                y_centroid +=dy[14]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT1:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[8]\n",
    "                y_centroid +=(dy[8]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[8]\n",
    "                y_centroid +=dy[8]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT2:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[0]\n",
    "                y_centroid +=(dy[0]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[0]\n",
    "                y_centroid +=dy[0]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT3:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[4]\n",
    "                y_centroid +=(dy[4]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[4]\n",
    "                y_centroid +=dy[4]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == RIGHT4:\n",
    "              if self._maze[nrow][ncol+1]== 1 and self._maze[nrow][ncol-1]== 0.9:\n",
    "                x_centroid +=dx[12]\n",
    "                y_centroid +=(dy[12]+1)\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[12]\n",
    "                y_centroid +=dy[12]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN1:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[9]+1)\n",
    "                y_centroid +=dy[9]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[9]\n",
    "                y_centroid +=dy[9]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN2:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[1]+1)\n",
    "                y_centroid +=dy[1]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[1]\n",
    "                y_centroid +=dy[1]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN3:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[5]+1)\n",
    "                y_centroid +=dy[5]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[5]\n",
    "                y_centroid +=dy[5]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == DOWN4:\n",
    "              if self._maze[nrow+1][ncol]== 1 and self._maze[nrow-1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[13]+1)\n",
    "                y_centroid +=dy[13]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[13]\n",
    "                y_centroid +=dy[13]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP1:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[11]-1)\n",
    "                y_centroid +=dy[11]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[11]\n",
    "                y_centroid +=dy[11]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP2:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[3]-1)\n",
    "                y_centroid +=dy[3]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[3]\n",
    "                y_centroid +=dy[3]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP3:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[7]-1)\n",
    "                y_centroid +=dy[7]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[7]\n",
    "                y_centroid +=dy[7]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "            elif action == UP4:\n",
    "              if self._maze[nrow-1][ncol]== 1 and self._maze[nrow+1][ncol]== 0.9:\n",
    "                x_centroid +=(dx[15]-1)\n",
    "                y_centroid +=dy[15]\n",
    "                nmode = 'reward_unchanged'\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "              else :\n",
    "                x_centroid +=dx[15]\n",
    "                y_centroid +=dy[15]\n",
    "                self.state = (x_centroid,y_centroid, nmode)\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            #nmode = 'invalid'\n",
    "            pass\n",
    "\n",
    "\n",
    "        # new state\n",
    "    \n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        target_row,target_col = self.target\n",
    "        nrows, ncols = self.maze.shape\n",
    "        reward = None\n",
    "        status = None\n",
    "        if mode == 'reward_unchanged'and not (rat_row, rat_col) in self.visited:\n",
    "          #mode = \"valid\"\n",
    "          return 0\n",
    "#         if self.current_action == self.old_action :\n",
    "#           return -1.2\n",
    "        if mode == 'blocked':\n",
    "            reward  =self.min_reward - 0.5\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -1.5\n",
    "        if mode == 'invalid':\n",
    "            reward  = -2\n",
    "        # if mode == 'valid':\n",
    "        #     reward  = -0.04\n",
    "        if mode == 'valid':\n",
    "            # self.current_distance = self.dist(rat_row,rat_col,target_row,target_col)\n",
    "            # if self.current_distance <= self.old_distance :\n",
    "            #       reward = -self.current_distance*0.004\n",
    "                  \n",
    "            # else : reward = -self.current_distance*0.005\n",
    "            # self.old_distance = self.current_distance\n",
    "            try : \n",
    "              reward = df[\"{}_To_{}\".format(action_node_dict[self.old_action],action_node_dict[self.current_action])]\n",
    "            except :\n",
    "              reward = -1\n",
    "        if rat_row == nrows-1 and rat_col == ncols-2:\n",
    "            status = 'win'\n",
    "            return 200\n",
    "        # if mode == 'valid' and status !='win':\n",
    "        #       self.current_distance = self.dist(rat_row,rat_col,target_row,target_col)\n",
    "        #       if (rat_row, rat_col) in self.visited :\n",
    "        #           reward = -0.25\n",
    "        #       elif self.current_distance >= self.old_distance :\n",
    "        #       #reward = -current_distance*0.001\n",
    "        #           reward = -0.1             \n",
    "        #       else : reward = -0.04\n",
    "        #       self.old_distance = self.current_distance\n",
    "        return reward\n",
    "    def dist(self,x1,y1,x2,y2):\n",
    "        return ((x2-x1)**2 + (y2-y1)**2)**0.5\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "    def act_to_predict(self, action):\n",
    "        self.update_state_to_predict(action)\n",
    "        status = self.game_status_to_predict()\n",
    "        envstate = self.observe()\n",
    "        return envstate, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = np.reshape(canvas,newshape=(img_height,img_width,1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        row_target,col_target =  self.target\n",
    "        canvas[row, col] = rat_mark\n",
    "        canvas[row_target,col_target] = target_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        target_row,target_col = self.target\n",
    "        if rat_row == target_row and rat_col == target_col:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "    def set_supervised(self,supervised):\n",
    "        self.supervised = supervised \n",
    "    def game_status_to_predict(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        target_row,target_col = self.target\n",
    "        if rat_row == target_row and rat_col == target_col:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [LEFT1, LEFT2, LEFT3,LEFT4, RIGHT1, RIGHT2, RIGHT3,RIGHT4, UP1, UP2, UP3,UP4, DOWN1, DOWN2, DOWN3,DOWN4]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(UP1)\n",
    "            actions.remove(UP2)\n",
    "            actions.remove(UP3)\n",
    "            actions.remove(UP4)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(DOWN1)\n",
    "            actions.remove(DOWN2)\n",
    "            actions.remove(DOWN3)\n",
    "            actions.remove(DOWN4)\n",
    "        if col == 0:\n",
    "            actions.remove(LEFT1)\n",
    "            actions.remove(LEFT2)\n",
    "            actions.remove(LEFT3)\n",
    "            actions.remove(LEFT4)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(RIGHT1)\n",
    "            actions.remove(RIGHT2)\n",
    "            actions.remove(RIGHT3)\n",
    "            actions.remove(RIGHT4)\n",
    "        # if row>0 and self.maze[row-1,col] == 0.0:\n",
    "        #     actions.remove(UP1)\n",
    "        #     actions.remove(UP2)\n",
    "        #     actions.remove(UP3)\n",
    "        #     actions.remove(UP4)\n",
    "        # if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "        #     actions.remove(DOWN1)\n",
    "        #     actions.remove(DOWN2)\n",
    "        #     actions.remove(DOWN3)\n",
    "        #     actions.remove(DOWN4)\n",
    "        # if col>0 and self.maze[row,col-1] == 0.0:\n",
    "        #     actions.remove(LEFT1)\n",
    "        #     actions.remove(LEFT2)\n",
    "        #     actions.remove(LEFT3)\n",
    "        #     actions.remove(LEFT4)\n",
    "        # if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "        #     actions.remove(RIGHT1)\n",
    "        #     actions.remove(RIGHT2)\n",
    "        #     actions.remove(RIGHT3)\n",
    "        #     actions.remove(RIGHT4)\n",
    "        dx=[0,3,0,-2,1,3,1,-2,-1,3,-1,-2,2,3,2,-2]\n",
    "        dy=[3,0,-2,0,3,1,-2,1,3,-1,-2,-1,3,2,-2,2]\n",
    "        tx_target,ty_target = self.target\n",
    "        if self.supervised == 1:\n",
    "          best_choice = None\n",
    "          best_distance = 999999\n",
    "          for action in actions :\n",
    "                x_centroid,y_centroid = state_centroid_dict[(row, col)]\n",
    "                if action == LEFT1:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[10]\n",
    "                    y_centroid +=(dy[10]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[10]\n",
    "                    y_centroid +=dy[10]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == LEFT2:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[2]\n",
    "                    y_centroid +=(dy[2]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target) \n",
    "                  else :\n",
    "                    x_centroid +=dx[2]\n",
    "                    y_centroid +=dy[2]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == LEFT3:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[6]\n",
    "                    y_centroid +=(dy[6]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[6]\n",
    "                    y_centroid +=dy[6]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == LEFT4:\n",
    "                  if self._maze[row][col-1]== 1 and self._maze[row][col+1]== 0.9:\n",
    "                    x_centroid +=dx[14]\n",
    "                    y_centroid +=(dy[14]-1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[14]\n",
    "                    y_centroid +=dy[14]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT1:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[8]\n",
    "                    y_centroid +=(dy[8]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[8]\n",
    "                    y_centroid +=dy[8]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT2:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[0]\n",
    "                    y_centroid +=(dy[0]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[0]\n",
    "                    y_centroid +=dy[0]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT3:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[4]\n",
    "                    y_centroid +=(dy[4]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[4]\n",
    "                    y_centroid +=dy[4]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == RIGHT4:\n",
    "                  if self._maze[row][col+1]== 1 and self._maze[row][col-1]== 0.9:\n",
    "                    x_centroid +=dx[12]\n",
    "                    y_centroid +=(dy[12]+1)\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[12]\n",
    "                    y_centroid +=dy[12]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN1:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[9]+1)\n",
    "                    y_centroid +=dy[9]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[9]\n",
    "                    y_centroid +=dy[9]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN2:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[1]+1)\n",
    "                    y_centroid +=dy[1]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[1]\n",
    "                    y_centroid +=dy[1]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN3:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[5]+1)\n",
    "                    y_centroid +=dy[5]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[5]\n",
    "                    y_centroid +=dy[5]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == DOWN4:\n",
    "                  if self._maze[row+1][col]== 1 and self._maze[row-1][col]== 0.9:\n",
    "                    x_centroid +=(dx[13]+1)\n",
    "                    y_centroid +=dy[13]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[13]\n",
    "                    y_centroid +=dy[13]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP1:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[11]-1)\n",
    "                    y_centroid +=dy[11]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[11]\n",
    "                    y_centroid +=dy[11]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP2:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[3]-1)\n",
    "                    y_centroid +=dy[3]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[3]\n",
    "                    y_centroid +=dy[3]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP3:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[7]-1)\n",
    "                    y_centroid +=dy[7]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[7]\n",
    "                    y_centroid +=dy[7]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                elif action == UP4:\n",
    "                  if self._maze[row-1][col]== 1 and self._maze[row+1][col]== 0.9:\n",
    "                    x_centroid +=(dx[15]-1)\n",
    "                    y_centroid +=dy[15]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                  else :\n",
    "                    x_centroid +=dx[15]\n",
    "                    y_centroid +=dy[15]\n",
    "                    cur_distance = self.dist(x_centroid,y_centroid,tx_target,ty_target)\n",
    "                if (cur_distance<best_distance):\n",
    "                    best_distance = cur_distance\n",
    "                    best_choice = action\n",
    "                #print(\"Best distance : \",best_distance, \" Best choice : \",best_choice)\n",
    "          actions.clear()\n",
    "          actions.append(best_choice)\n",
    "\n",
    "        return actions\n",
    "    def check_valid_with_previous_state(self,cell):\n",
    "        row_cell, col_cell = cell\n",
    "        for temp in self.visited:\n",
    "            row, col = temp\n",
    "            if row_cell == row and col_cell == col :\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 117608,
     "status": "ok",
     "timestamp": 1593147108064,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "HYi1PiaNEwwH"
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    row_target,col_target = qmaze.target\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows+20, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols+20, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[row_target,col_target] = 0.7 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    plt.show()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 117596,
     "status": "ok",
     "timestamp": 1593147108066,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "aHUTI80rEw3-",
    "outputId": "b77175da-9b8f-46d7-a72d-2c9ce1f9a708"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29139dae408>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_width, img_height = maze.shape\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_height, img_width)\n",
    "else:\n",
    "    input_shape = (img_height,img_width, 1)\n",
    "qmaze = Qmaze(maze)\n",
    "qmaze.get_list_centroid()\n",
    "# canvas, reward, game_over = qmaze.act(DOWN)\n",
    "# print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 118292,
     "status": "ok",
     "timestamp": 1593147108772,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "jK31MzS0Ew_s",
    "outputId": "35750b8b-36fd-48cd-eb4d-185ad6e1b885"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2913a204388>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmaze.act(LEFT1) \n",
    "#qmaze.act(RIGHT2)\n",
    "qmaze.act(DOWN4)  # move down\n",
    "qmaze.act(DOWN4)  # move right\n",
    "qmaze.act(RIGHT2)  # move right\n",
    "qmaze.act(DOWN3)  # move right\n",
    "qmaze.act(UP1)  # move up\n",
    "qmaze.act(DOWN4)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1593149619964,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "uYXPeyapI47z"
   },
   "outputs": [],
   "source": [
    "def dist(x1,y1,x2,y2):\n",
    "        return ((x2-x1)**2 + (y2-y1)**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1593149621588,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "OnzTJZUc4onl"
   },
   "outputs": [],
   "source": [
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "    def _write_logs(self, logs, index):\n",
    "        self.writer.reopen()\n",
    "        for name, value in logs.items():\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            if isinstance(value, np.ndarray):\n",
    "                summary_value.simple_value = value.item()\n",
    "            else:\n",
    "                summary_value.simple_value = value\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1593149622005,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "lZBOHx-JEz8H"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"class Memory: Stores experience trajectories.\n",
    "    Idea is to fill the memory with trajectories (s,a,r,gae_r,s_,d) tuples and get an arbitary number of\n",
    "    random batches from the memory for training. After that, clear and fill next set (on policy training).\n",
    "    The memory gets filled from the Agent object.\n",
    "    A special case is the batch_gae which gets calculated externally once when training on memory starts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.batch_s = []\n",
    "        self.batch_a = []\n",
    "        self.batch_r = []\n",
    "        self.batch_gae_r = [] #this gets set in agent make_gae which is called once on first training on memory\n",
    "        self.batch_s_ = []\n",
    "        self.batch_done = []\n",
    "        self.GAE_CALCULATED_Q = False #make sure make_gae can only be called once\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size):\n",
    "        \"\"\"simply retuns a randomized batch from the data in memory\n",
    "        r not really needed for training (gae_r is used). but might be interesting for logging.\n",
    "        \"\"\"\n",
    "        for _ in range(batch_size):\n",
    "            s,a,r,gae_r,s_,d = [],[],[],[],[],[]\n",
    "            pos = np.random.randint(len(self.batch_s)) #random position\n",
    "            s.append(self.batch_s[pos])\n",
    "            a.append(self.batch_a[pos])\n",
    "            r.append(self.batch_r[pos])\n",
    "            gae_r.append(self.batch_gae_r[pos])\n",
    "            s_.append(self.batch_s_[pos])\n",
    "            d.append(self.batch_done[pos])\n",
    "        return s,a,r,gae_r,s_,d #return randomized batches\n",
    "\n",
    "\n",
    "    def store(self, s, a, s_, r, done):\n",
    "        \"\"\"push s,a,r,s_,done into memory (=according lists)\n",
    "        \"\"\"\n",
    "        self.batch_s.append(s)\n",
    "        self.batch_a.append(a)\n",
    "        self.batch_r.append(r)\n",
    "        self.batch_s_.append(s_)\n",
    "        self.batch_done.append(done)\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"clear all lists (=memory)\n",
    "        \"\"\"\n",
    "        self.batch_s.clear()\n",
    "        self.batch_a.clear()\n",
    "        self.batch_r.clear()\n",
    "        self.batch_s_.clear()\n",
    "        self.batch_done.clear()\n",
    "        self.GAE_CALCULATED_Q = False\n",
    "\n",
    "\n",
    "    @property\n",
    "    def cnt_samples(self):\n",
    "        return len(self.batch_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1018,
     "status": "ok",
     "timestamp": 1593149622006,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "ra_ejSjaE0Hd"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,action_n, state_dim, training_batch_size):\n",
    "        \"\"\"This is the agent object.\n",
    "        Main interaction is the choose_action, store transition and train_network. \n",
    "        The agent only requires the state and action spaces to fuction, other than that it is pretty general\n",
    "        and should be easy to adapt for other deterministic envs.\n",
    "        To understand what is happening, I recommend to look at the ppo_loss method and the build_actor method first.\n",
    "        The training method itself is more or less only data preperation for calling the fit functions\n",
    "        for actor and critic. But critic has a trivial loss, so all the PPO magic is in the ppo_loss function.\n",
    "        \"\"\"\n",
    "        self.action_n = action_n\n",
    "        self.state_dim = state_dim        \n",
    "        #CONSTANTS\n",
    "        self.TRAINING_BATCH_SIZE = training_batch_size\n",
    "        self.TARGET_UPDATE_ALPHA = 0.95\n",
    "        self.GAMMA = 0.98\n",
    "        self.GAE_LAMBDA = 0.95\n",
    "        self.CLIPPING_LOSS_RATIO = 0.1\n",
    "        self.ENTROPY_LOSS_RATIO = 0.001\n",
    "        self.TARGET_UPDATE_ALPHA = 0.9\n",
    "        self.LR = 5e-6\n",
    "        #create actor and critic neural networks\n",
    "        self.critic_network = self._build_critic_network()\n",
    "        self.actor_network = self._build_actor_network()\n",
    "        try : \n",
    "            print(\"Loading model...\")\n",
    "            self.load_model(True)\n",
    "        except :\n",
    "            print(\"Load model failed\")\n",
    "        #for the loss function, additionally \"old\" predicitons are required from before the last update.\n",
    "        #therefore create another networtk. Set weights to be identical for now.\n",
    "        \n",
    "        self.actor_old_network = self._build_actor_network()\n",
    "        self.actor_old_network.set_weights(self.actor_network.get_weights()) \n",
    "        #for getting an action (predict), the model requires it's ususal input, but advantage and old_prediction is only used for loss(training). So create dummys for prediction only\n",
    "        self.dummy_advantage = np.zeros((1, 1))\n",
    "        self.dummy_old_prediciton = np.zeros((1, self.action_n))\n",
    "        #our transition memory buffer        \n",
    "        self.memory = Memory()\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.actor_network.save(\"actor.h5\")\n",
    "        self.critic_network.save(\"critic.h5\")\n",
    "        \n",
    "    def load_model(self,mode):\n",
    "        if mode == True :\n",
    "            self.actor_network = load_model(\"actor.h5\")\n",
    "            self.critic_network = load_model(\"critic.h5\")\n",
    "            self.actor_network.compile(optimizer=Adam(lr=self.LR),\n",
    "                                       loss = self.ppo_loss(advantage=advantage,old_prediction=old_prediction))\n",
    "            self.critic_network.compile(optimizer='Adam',loss = 'mean_squared_error')\n",
    "        \n",
    "    def _build_actor_network(self):\n",
    "        \"\"\"builds and returns a compiled keras.model for the actor.\n",
    "        There are 3 inputs. Only the state is for the pass though the neural net. \n",
    "        The other two inputs are exclusivly used for the custom loss function (ppo_loss).\n",
    "        \"\"\"\n",
    "        #define inputs. Advantage and old_prediction are required to pass to the ppo_loss funktion\n",
    "        state = keras.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        advantage = keras.layers.Input(shape=(1,),name='advantage_input')\n",
    "        old_prediction = keras.layers.Input(shape=(self.action_n,),name='old_prediction_input')\n",
    "        #define hidden layers\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense1')(state)\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense2')(dense)\n",
    "        #connect layers, output action using softmax activation\n",
    "        policy = keras.layers.Dense(self.action_n, activation=\"softmax\", name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        actor_network = keras.Model(inputs = [state,advantage,old_prediction], outputs = policy)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        actor_network.compile(\n",
    "            optimizer=Adam(lr=self.LR),\n",
    "            loss = self.ppo_loss(advantage=advantage,old_prediction=old_prediction)\n",
    "            )\n",
    "        #summary and return       \n",
    "        actor_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return actor_network\n",
    "\n",
    "\n",
    "    def _build_critic_network(self):\n",
    "        \"\"\"builds and returns a compiled keras.model for the critic.\n",
    "        The critic is a simple scalar prediction on the state value(output) given an state(input)\n",
    "        Loss is simply mse\n",
    "        \"\"\"\n",
    "        #define input layer\n",
    "        state = keras.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        #define hidden layers\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense1')(state)\n",
    "        dense = keras.layers.Dense(maze.size,activation='relu',name='dense2')(dense)\n",
    "        #connect the layers to a 1-dim output: scalar value of the state (= Q value or V(s))\n",
    "        V = keras.layers.Dense(1, name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        critic_network = keras.Model(inputs=state, outputs=V)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        critic_network.compile(optimizer='Adam',loss = 'mean_squared_error')\n",
    "        #summary and return           \n",
    "        critic_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return critic_network\n",
    "    \n",
    "\n",
    "    def ppo_loss(self, advantage, old_prediction):\n",
    "        \"\"\"The PPO custom loss.\n",
    "        For explanation see for example:\n",
    "        https://youtu.be/WxQfQW48A4A\n",
    "        https://youtu.be/5P7I-xPq8u8\n",
    "        params:\n",
    "            :advantage: advantage, needed to process algorithm\n",
    "            :old_predictioN: prediction from \"old\" network, needed to process algorithm\n",
    "        returns:\n",
    "            :loss: keras type loss fuction (not a value but a fuction with two parameters y_true, y_pred)\n",
    "        \"\"\"\n",
    "        #refer to Keras custom loss function intro to understand why we define a funciton inside a function.\n",
    "        def loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred #y_true is taken action one_hot(in deterministic case) and pred is a softmax vector. prob is the probability of the taken aciton.\n",
    "            old_prob = y_true * old_prediction\n",
    "            ratio = prob / (old_prob + 1e-10)\n",
    "            clip_ratio = keras.backend.clip(ratio, min_value=1 - self.CLIPPING_LOSS_RATIO, max_value=1 + self.CLIPPING_LOSS_RATIO)\n",
    "            surrogate1 = ratio * advantage\n",
    "            surrogate2 = clip_ratio * advantage\n",
    "            entropy_loss = (prob * keras.backend.log(prob + 1e-10)) #optionally add the entropy loss to avoid getting stuck on local minima\n",
    "            ppo_loss = -keras.backend.mean(keras.backend.minimum(surrogate1,surrogate2) + self.ENTROPY_LOSS_RATIO * entropy_loss)\n",
    "            return ppo_loss\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def make_gae(self):\n",
    "        \"\"\"Generates GAE type rewards and pushes them into memory object\n",
    "        #GAE algorithm: \n",
    "            #delta = r + gamma * V(s') * mask - V(s)  |aka advantage\n",
    "            #gae = delta + gamma * lambda * mask * gae |moving average smoothing\n",
    "            #return(s,a) = gae + V(s)  |add value of state back to it.\n",
    "        \"\"\"\n",
    "        gae = 0\n",
    "        mask = 0\n",
    "        for i in reversed(range(self.memory.cnt_samples)):\n",
    "            mask = 0 if self.memory.batch_done[i] else 1\n",
    "            v = self.get_v(self.memory.batch_s[i])\n",
    "            delta = self.memory.batch_r[i] + self.GAMMA * self.get_v(self.memory.batch_s_[i]) * mask - v\n",
    "            gae = delta + self.GAMMA *  self.GAE_LAMBDA * mask * gae\n",
    "            self.memory.batch_gae_r.append(gae+v)\n",
    "        self.memory.batch_gae_r.reverse()\n",
    "        self.memory.GAE_CALCULATED_Q = True\n",
    "\n",
    "\n",
    "    def update_tartget_network(self):\n",
    "        \"\"\"Softupdate of the target network.\n",
    "        In ppo, the updates of the \n",
    "        \"\"\"\n",
    "        alpha = self.TARGET_UPDATE_ALPHA\n",
    "        actor_weights = np.array(self.actor_network.get_weights())\n",
    "        actor_tartget_weights = np.array(self.actor_old_network.get_weights())\n",
    "        new_weights = alpha*actor_weights + (1-alpha)*actor_tartget_weights\n",
    "        self.actor_old_network.set_weights(new_weights)\n",
    "\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "        \"\"\"chooses an action within the action space given a state.\n",
    "        The action is chosen by random with the weightings accoring to the probability\n",
    "        params:\n",
    "            :state: np.array of the states with state_dim length\n",
    "        \"\"\"\n",
    "        assert isinstance(state,np.ndarray)\n",
    "        #reshape for predict_on_batch which requires 2d-arrays\n",
    "        state = np.reshape(state,[-1,self.state_dim])\n",
    "        #the probability list for each action is the output of the actor network given a state\n",
    "        prob = self.actor_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton]).flatten()\n",
    "        #action is chosen by random with the weightings accoring to the probability\n",
    "        action = np.random.choice(self.action_n,p=prob)\n",
    "        return action,prob\n",
    "    \n",
    "\n",
    "    def train_network(self):\n",
    "        \"\"\"Train the actor and critic networks using GAE Algorithm.\n",
    "        1. Get GAE rewards\n",
    "        2. reshape batches s,a,gae_r baches\n",
    "        3. get value of state\n",
    "        4. calc advantage\n",
    "        5. get \"old\" precition (of target network)\n",
    "        6. fit actor and critic network\n",
    "        7. soft update target \"old\" network\n",
    "        \"\"\"\n",
    "        #important: make gae type rewards BEFORE getting random batches if not done yet\n",
    "        if not self.memory.GAE_CALCULATED_Q:\n",
    "            self.make_gae()\n",
    "        #get randomized mini batches\n",
    "        states,actions,rewards,gae_r,next_states,dones = self.memory.get_batch(self.TRAINING_BATCH_SIZE)\n",
    "       \n",
    "        #create np array batches for training\n",
    "        batch_s = np.vstack(states)\n",
    "        batch_a = np.vstack(actions)\n",
    "        batch_gae_r = np.vstack(gae_r)\n",
    "        #get values of states in batch\n",
    "        batch_v = self.get_v(batch_s)\n",
    "        #calc advantages. required for actor loss. \n",
    "        batch_advantage = batch_gae_r - batch_v\n",
    "        batch_advantage = keras.utils.normalize(batch_advantage) #\n",
    "        #calc old_prediction. Required for actor loss.\n",
    "        batch_old_prediction = self.get_old_prediction(batch_s)\n",
    "        #one-hot the actions. Actions will be the target for actor.\n",
    "        batch_a_final = np.zeros(shape=(len(batch_a), self.action_n))\n",
    "        batch_a_final[:, batch_a.flatten()] = 1\n",
    "\n",
    "        #commit training\n",
    "        self.actor_network.fit(x=[batch_s, batch_advantage, batch_old_prediction], y=batch_a_final, verbose=0)\n",
    "        self.critic_network.fit(x=batch_s, y=batch_gae_r, epochs=1, verbose=0)\n",
    "        #soft update the target network(aka actor_old). \n",
    "        self.update_tartget_network()\n",
    "\n",
    "\n",
    "    def store_transition(self, s, a, s_, r, done):\n",
    "        \"\"\"Store the experiences transtions into memory object.\n",
    "        \"\"\"\n",
    "        self.memory.store(s, a, s_, r, done)\n",
    "\n",
    "\n",
    "    def get_v(self,state):\n",
    "        \"\"\"Returns the value of the state.\n",
    "        Basically, just a forward pass though the critic networtk\n",
    "        \"\"\"\n",
    "        s = np.reshape(state,(-1, self.state_dim))\n",
    "        v = self.critic_network.predict_on_batch(s)\n",
    "        return v\n",
    "    \n",
    "\n",
    "    def get_old_prediction(self, state):\n",
    "        \"\"\"Makes an prediction (an action) given a state on the actor_old_network.\n",
    "        This is for the train_network --> ppo_loss\n",
    "        \"\"\"\n",
    "        state = np.reshape(state, (-1, self.state_dim))\n",
    "        return self.actor_old_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 941,
     "status": "ok",
     "timestamp": 1593149622607,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "VjXoRo7gE0Q0",
    "outputId": "2ddac579-f89a-41c4-8ab4-a096ab7b3fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :  4999  Probability :  [5.9619127e-04 2.5457223e-03 5.3789857e-04 2.3747587e-03 7.3019997e-04\n",
      " 1.5170239e-03 3.6905645e-04 2.1906260e-03 1.8429855e-02 3.3294590e-04\n",
      " 1.2540024e-02 3.2199275e-01 1.8553450e-03 6.0049314e-03 2.3772884e-03\n",
      " 6.2560540e-01]  Action :  15  Reward :  200  Loss :  0  Win count :  4556\n"
     ]
    }
   ],
   "source": [
    "TRAIN_ITERATIONS = 5000\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "TRAJECTORY_BUFFER_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "RENDER_EVERY = 100\n",
    "AGGREGATE_STATS_EVERY = 5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"E:/RLAChips - Random\")\n",
    "    env = Qmaze(maze)\n",
    "    agent = Agent(num_actions,maze.size,BATCH_SIZE)\n",
    "    samples_filled = 0\n",
    "    tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "    ep_rewards=[]\n",
    "    ep_loss = []\n",
    "    win_count = 0\n",
    "    for cnt_episode in range(TRAIN_ITERATIONS):\n",
    "        rat_cell = random.choice(env.free_cells)\n",
    "        s = env.reset(rat_cell)\n",
    "        r_sum = 0\n",
    "        loss_sum =0 \n",
    "        for cnt_step in range(MAX_EPISODE_LENGTH):\n",
    "            show(env)\n",
    "            clear_output(wait=True)\n",
    "            #get action from agent given state\n",
    "            a,prob = agent.choose_action(s)\n",
    "            #get s_,r,done\n",
    "            env.current_action = a\n",
    "            s_, r, done= env.act(a)\n",
    "            r_sum += r\n",
    "            if r==200 :\n",
    "                loss = 0\n",
    "                win_count+=1\n",
    "            elif r==0 :\n",
    "                loss = 0\n",
    "            elif r == -1 :\n",
    "                loss = 0\n",
    "            elif r == -1.5 :\n",
    "                try :\n",
    "                    loss = df[\"{}_To_{}\".format(action_node_dict[env.old_action],action_node_dict[env.current_action])]\n",
    "                except :\n",
    "                    loss = 0\n",
    "            else :\n",
    "                loss = r\n",
    "            loss_sum+=loss*10\n",
    "            if done == 'not_over':\n",
    "                done = False\n",
    "            else :\n",
    "                done = True\n",
    "            env.old_action = a\n",
    "            print(\"Episode : \",cnt_episode, \" Probability : \",prob,\" Action : \",a,\" Reward : \",r,\" Loss : \",loss*10,\" Win count : \",win_count)\n",
    "            #store transitions to agent.memory\n",
    "        \n",
    "            agent.store_transition(s.reshape((-1,maze.size)), a, s_.reshape((-1,maze.size)), r, done)\n",
    "            samples_filled += 1\n",
    "            #train in batches one buffer is filled with samples.\n",
    "            if samples_filled % TRAJECTORY_BUFFER_SIZE == 0 and samples_filled != 0:\n",
    "                #To be sample efficient, sample as often as statistically necearry to \n",
    "                # use all availible samples in memory. Imortant to sample randomly \n",
    "                # to keep the training data independant and identically distributed IID\n",
    "                for _ in range(TRAJECTORY_BUFFER_SIZE // BATCH_SIZE):\n",
    "                    agent.train_network()\n",
    "                agent.memory.clear()\n",
    "                samples_filled = 0\n",
    "            #set state to next_state\n",
    "            s = s_\n",
    "            if done:\n",
    "                break\n",
    "        ep_rewards.append(r_sum)\n",
    "        ep_loss.append(loss_sum)\n",
    "        if  cnt_episode%AGGREGATE_STATS_EVERY==0 or cnt_episode == 1 :\n",
    "            print(\"updating stats...\")\n",
    "            average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "            average_loss = sum(ep_loss[-AGGREGATE_STATS_EVERY:]) / len(ep_loss[-AGGREGATE_STATS_EVERY:])\n",
    "            tensorboard.step = cnt_episode\n",
    "            tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward,\n",
    "                                           reward_max=max_reward,tranmission_loss = average_loss,\n",
    "                                           epsilon=1)\n",
    "            agent.save_model()\n",
    "        if cnt_episode % 10 == 0:\n",
    "            print(f\"Episode:{cnt_episode}, step:{cnt_step}, r_sum:{r_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 121119,
     "status": "aborted",
     "timestamp": 1593147111671,
     "user": {
      "displayName": "Nguyên Đỗ Hoàng Khôi",
      "photoUrl": "",
      "userId": "00234567357869733044"
     },
     "user_tz": -420
    },
    "id": "ybibrvVfcQiv"
   },
   "outputs": [],
   "source": [
    "result_ep_rewards = np.asarray(ep_rewards)\n",
    "result_ep_loss = np.asarray(ep_loss)\n",
    "rsl = pd.DataFrame(result_ep_loss)\n",
    "rsl.to_excel('rsl.xlsx', index=False, header=False)\n",
    "rsr = pd.DataFrame(result_ep_rewards)\n",
    "rsr.to_excel('rsr.xlsx', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-472.896922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-495.382389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-490.146183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-548.853009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-499.466767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>-9.954332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-3.151083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-12.615703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>-12.707494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>-2.711871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0    -472.896922\n",
       "1    -495.382389\n",
       "2    -490.146183\n",
       "3    -548.853009\n",
       "4    -499.466767\n",
       "...          ...\n",
       "4995   -9.954332\n",
       "4996   -3.151083\n",
       "4997  -12.615703\n",
       "4998  -12.707494\n",
       "4999   -2.711871\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyOslRe9iSuuIZbBBqNQE42x",
   "collapsed_sections": [],
   "name": "Bản sao của RLAChip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
